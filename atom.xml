<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>时间与精神的小屋</title>
  
  <subtitle>专注思考的时候，时间仿佛也静下来了</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://link3280.github.io/"/>
  <updated>2020-06-13T03:25:27.589Z</updated>
  <id>https://link3280.github.io/</id>
  
  <author>
    <name>Paul Lin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>详解 Flink 实时应用的确定性</title>
    <link href="https://link3280.github.io/2020/06/12/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84-Flink-%E5%BA%94%E7%94%A8/"/>
    <id>https://link3280.github.io/2020/06/12/如何构建确定性的-Flink-应用/</id>
    <published>2020-06-12T14:09:03.000Z</published>
    <updated>2020-06-13T03:25:27.589Z</updated>
    
    <content type="html"><![CDATA[<p>确定性（Determinism）是计算机科学中十分重要的特性，确定性的算法保证对于给定相同的输入总是产生相同的输出。在分布式实时计算领域，确定性是业界一直难以解决的课题，由此导致用离线计算修正实时计算结果的 Lambda 架构成为大数据领域过去近十年的主流架构。而在最近几年随着 Google The Dataflow Model 的提出，实时计算和离线计算的关系逐渐清晰，在实时计算中提供与离线计算一致的确定性成为可能。本文将基于流行实时计算引擎 Apache Flink，梳理构建一个确定性的实时应用要满足什么条件。</p><a id="more"></a><h1 id="确定性与准确性"><a href="#确定性与准确性" class="headerlink" title="确定性与准确性"></a>确定性与准确性</h1><p>比起确定性，准确性（Accuracy）可能是我们接触更多的近义词，大多数场景下两者可以混用，但其实它们稍有不同: 准确的东西一定是确定的，但确定性的东西未必百分百准确。在大数据领域，不少算法可以根据需求调整成本和准确性的平衡，比如 HyperLogLog 去重统计算法给出的结果是有一定误差的（因此不是准确的），但却同时是确定性的（重算可以得到相同结果）。</p><p>要分区确定性和准确性的缘故是，准确性与具体的业务逻辑紧密耦合难以评估，而确定性则是通用的需求（除去少数场景用户故意使用非确定性的算法）。当一个 Flink 实时应用提供确定性，意味着它在异常场景的自动重试或者手动重流数据的情况下，都能像离线作业一般产出相同的结果，这将很大程度上提高用户的信任度。</p><h1 id="影响-Flink-应用确定性的因素"><a href="#影响-Flink-应用确定性的因素" class="headerlink" title="影响 Flink 应用确定性的因素"></a>影响 Flink 应用确定性的因素</h1><h2 id="投递语义"><a href="#投递语义" class="headerlink" title="投递语义"></a>投递语义</h2><p>常见的投递语义有 <code>At-Most-Once</code>、<code>At-Least-Once</code> 和 <code>Exactly-Once</code> 三种。严格来说只有 <code>Exactly-Once</code> 满足确定性的要求，但如果整个业务逻辑是幂等的， 基于<code>At-Least-Once</code> 也可以达到结果的确定性。</p><p>实时计算的 <code>Exactly-Once</code> 通常指端到端的 <code>Exactly-Once</code>，保证输出到下游系统的数据和上游的数据是一致的，没有重复计算或者数据丢失。要达到这点，需要分别实现读取数据源（Source 端）的 <code>Exactly-Once</code>、计算的 <code>Exactly-Once</code> 和输出到下游系统（Sink 端）的 <code>Exactly-Once</code>。</p><p>其中前面两个都比较好保证，因为 Flink 应用出现异常会自动恢复至最近一个成功 checkpoint，Pull-Based 的 Source 的状态和 Flink 内部计算的状态都会自动回滚到快照时间点，而问题在于 Push-Based 的 Sink 端。Sink 端是否能顺利回滚依赖于外部系统的特性，通常来说需要外部系统支持事务，然而不少大数据组件对事务的支持并不是很好，即使是实时计算最常用的 Kafka 也直到 2017 年的 0.11 版本才支持事务，更多的组件需要依赖各种 trick 来达到某种场景下的 <code>Exactly-Once</code>。</p><p>总体来说这些 trick 可以分为两大类:</p><ul><li>依赖写操作的幂等性。比如 HBase 等 KV 存储虽然没有提供跨行事务，但可以通过幂等写操作配合基于主键的 Upsert 操作达到 <code>Exactly-Once</code>。不过由于 Upsert 不能表达 Delete 操作，这种模式不适合有 Delete 的业务场景。</li><li>预写日志（WAL，Write-Ahead-Log）。预写日志是广泛应用于事物机制的技术，包括 MySQL、PostgreSQL 等成熟关系型数据库的事物都基于预写日志。预写日志的基本原理先将变更写入缓存区，等事务提交的时候再一次全部应用。比如 HDFS/S3 等文件系统本身并不提供事务，因此实现预写日志的重担落到了它们的用户（比如 Flink）身上。通过先写临时的文件/对象，等 Flink Checkpoint 成功后再提交，Flink 的 FileSystem Connector 实现了 <code>Exactly-Once</code>。然而，预写日志只能保证事务的原子性和持久性，不能保证一致性和隔离性。为此 FileSystem Connector 通过将预写日志设为隐藏文件的方式提供了隔离性，至于一致性（比如临时文件的清理）则无法保证。</li></ul><p>为了保证 Flink 应用的确定性，在选用官方 Connector，特别是 Sink Connector 时，用户应该留意官方文档关于 Connector 投递语义的说明[3]。此外，在实现定制化的 Sink Connector 时也需要明确达到何种投递语义，可以参考利用外部系统的事务、写操作的幂等性或预写日志三种方式达到 <code>Exactly-Once</code> 语义。</p><h2 id="函数副作用"><a href="#函数副作用" class="headerlink" title="函数副作用"></a>函数副作用</h2><p>函数副作用是指用户函数对外界造成了计算框架意料之外的影响。比如典型的是在一个 Map 函数里将中间结果写到数据库，如果 Flink 作业异常自动重启，那么数据可能被写两遍，导致不确定性。对于这种情况，Flink 提供了基于 Checkpoint 的两阶段提交的钩子（<code>CheckpointedFunction</code> 和 <code>CheckpointListener</code>），用户可以用它来实现事务，以消除副作用的不确定性。另外还有一种常见的情况是，用户使用本地文件来保存临时数据，这些数据在 Task 重新调度的时候很可能丢失。其他的场景或许还有很多，总而言之，如果需要在用户函数里改变外部系统的状态，请确保 Flink 对这些操作是知情的（比如用 State API 记录状态，设置 Checkpoint 钩子）。</p><h2 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h2><p>在算法中引入当前时间作为参数是常见的操作，但在实时计算中引入当前系统时间，即 Processing Time，是造成不确定性的最常见也最难避免的原因。对 Processing 的引用可以是很明显、有完善文档标注的，比如 Flink 的 Time Characteristic，但也可能是完全出乎用户意料的，比如来源于缓存等常用的技术。为此，笔者总结了几类常见的 Processing Time 引用:</p><ul><li><p>Flink 提供的 Time Characteristic。Time Characteristic 会影响所有使用与时间相关的算子，比如 Processing Time 会让窗口聚合使用当前系统时间来分配窗口和触发计算，造成不确定性。另外，Processing Timer 也有类似的影响。</p></li><li><p>直接在函数里访问外部存储。因为这种访问是基于外部存储某个 Processing Time 时间点的状态，这个状态很可能在下次访问时就发生了变化，导致不确定性。要获得确定性的结果，比起简单查询外部存储的某个时间点的状态，我们应该获取它状态变更的历史，然后根据当前 Event Time 去查询对应的状态。这也是 Flink SQL 中 Temporary Table Join 的实现原理[1]。</p></li><li><p>对外部数据的缓存。在计算流量很大的数据时，很多情况下用户会选择用缓存来减轻外部存储的负载，但这可能会造成查询结果的不一致，而且这种不一致是不确定的。无论是使用超时阈值、LRU（Least Recently Used）等直接和系统时间相关的缓存剔除策略，还是 FIFO（First In First Out）、LFU（Less Frequently Used）等没有直接关联时间的剔除策略，访问缓存得到的结果通常和消息的到达顺序相关，而在上游经过 shuffle 的算子里面这是难以保证的（没有 shuffle 的 Embarrassingly Parallel 作业是例外）。</p></li><li><p>Flink 的 StateTTL。StateTTL 是 Flink 内置的根据时间自动清理 State 的机制，而这里的时间目前只提供 Processing Time，无论 Flink 本身使用的是 Processing Time 还是 Event Time 作为 Time Characteristic。BTW，StateTTL 对 Event Time 的支持可以关注 FLINK-12005[2]。</p></li></ul><p>综合来讲，要完全避免 Processing Time 造成的影响是非常困难的，不过轻微的不确定性对于业务来说通常是可以接受的，我们要做的更多是提前预料到可能的影响，保证不确定性在可控范围内。</p><h2 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h2><p>Watermark 作为计算 Event Time 的机制，其中一个很重要的用途是决定实时计算何时要输出计算结果，类似文件结束标志符（EOF）在离线批计算中达到的效果。然而，在输出结果之后可能还会有迟到的数据到达，这称为窗口完整性问题（Window Completeness）。</p><p>窗口完整性问题无法避免，应对办法是要么更新计算结果，要么丢弃这部分数据。因为离线场景延迟容忍度较大，离线作业可以推迟一定时间开始，尽可能地将延迟数据纳入计算。而实时场景对延迟有比较高的要求，因此一般是输出结果后让状态保存一段时间，在这段时间内根据迟到数据持续更新结果（即 Allowed Lateness），此后将数据丢弃。因为定位，实时计算天然可能出现更多被丢弃的迟到数据，这将和 Watermark 的生成算法紧密相关。</p><p>虽然 Watermark 的生成是流式的，但 Watermark 的下发是断点式的。Flink 的 Watermark 下发策略有 Periodic 和 Punctuated 两种，前者基于 Processing Time 定时触发，后者根据数据流中的特殊消息触发。</p><p><center><img src="/img/flink-determinism/img1.periodic-watermark.png" alt="图1. Periodic Watermark 正常状态与重放追数据状态" title="图1. Periodic Watermark 正常状态与重放追数据状态"></center></p><p>基于 Processing Time 的 Periodic Watermark 具有不确定。在平时流量平稳的时候 Watermark 的提升可能是阶梯式的（见图1(a)）；然而在重放历史数据的情况下，相同长度的系统时间内处理的数据量可能会大很多（见图1(b))，并且伴有 Event Time 倾斜（即有的 Source 的 Event Time 明显比其他要快或慢，导致取最小值的总体 Watermark 被慢 Watermark 拖慢），导致本来丢弃的迟到数据，现在变为 Allowed Lateness 之内的数据（见图1中红色元素）。</p><p><center><img src="/img/flink-determinism/img2.punctuated-watermark.png" alt="图2. Punctuated Watermark 正常状态与重放追数据状态" title="图2. Punctuated Watermark 正常状态与重放追数据状态"></center></p><p>相比之下 Punctuated Watermark 更为稳定，无论在正常情况（见图2(a)）还是在重放数据的情况（见图2(b)）下，下发的 Watermark 都是一致的，不过依然有 Event Time 倾斜的风险。对于这点，Flink 社区起草了 FLIP-27 来处理[4]。基本原理是 Source 节点会选择性地消费或阻塞某个 partition/shard，让总体的 Event Time 保持接近。</p><p>除了 Watermark 的下发有不确定之外，还有个问题是现在 Watermark 并没有被纳入 Checkpoint 快照中。这意味着在作业从 Checkpoint 恢复之后，Watermark 会重新开始算，导致 Watermark 的不确定。这个问题在 FLINK-5601[5] 有记录，但目前只体现了 Window 算子的 Watermark，而在 StateTTL 支持 Event Time 后，或许每个算子都要记录自己的 Watermark。</p><p>综上所述，Watermark 目前是很难做到非常确定的，但因为 Watermark 的不确定性是通过丢弃迟到数据导致计算结果的不确定性的，只要没有丢弃迟到数据，无论中间 Watermark 的变化如何，最终的结果都是相同的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>确定性不足是阻碍实时计算在关键业务应用的主要因素，不过当前业界已经具备了解决问题的理论基础，剩下的更多是计算框架后续迭代和工程实践上的问题。就目前开发 Flink 实时应用而言，需要注意投递语义、函数副作用、Processing Time 和 Watermark 这几点造成的不确定性。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://flink.apache.org/2019/05/14/temporal-tables.html" target="_blank" rel="external">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-12005" target="_blank" rel="external">[FLINK-12005][State TTL] Event time support</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/guarantees.html" target="_blank" rel="external">Fault Tolerance Guarantees of Data Sources and Sinks</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface" target="_blank" rel="external">FLIP-27: Refactor Source Interface</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-5601" target="_blank" rel="external">[FLINK-5601] Window operator does not checkpoint watermarks</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;确定性（Determinism）是计算机科学中十分重要的特性，确定性的算法保证对于给定相同的输入总是产生相同的输出。在分布式实时计算领域，确定性是业界一直难以解决的课题，由此导致用离线计算修正实时计算结果的 Lambda 架构成为大数据领域过去近十年的主流架构。而在最近几年随着 Google The Dataflow Model 的提出，实时计算和离线计算的关系逐渐清晰，在实时计算中提供与离线计算一致的确定性成为可能。本文将基于流行实时计算引擎 Apache Flink，梳理构建一个确定性的实时应用要满足什么条件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.11 Unaligned Checkpoint 解析</title>
    <link href="https://link3280.github.io/2020/06/08/Flink-1-11-Unaligned-Checkpoint-%E8%A7%A3%E6%9E%90/"/>
    <id>https://link3280.github.io/2020/06/08/Flink-1-11-Unaligned-Checkpoint-解析/</id>
    <published>2020-06-07T16:21:56.000Z</published>
    <updated>2020-06-13T02:06:33.590Z</updated>
    
    <content type="html"><![CDATA[<p>作为 Flink 最基础也是最关键的容错机制，Checkpoint 快照机制很好地保证了 Flink 应用从异常状态恢复后的数据准确性。同时 Checkpoint 相关的 metrics 也是诊断 Flink 应用健康状态最为重要的指标，成功且耗时较短的 Checkpoint 表明作业运行状况良好，没有异常或反压。然而，由于 Checkpoint 与反压的耦合，反压反过来也会作用于 Checkpoint，导致 Checkpoint 的种种问题。针对于此，Flink 在 1.11 引入 Unaligned Checkpint 来解耦 Checkpoint 机制与反压机制，优化高反压情况下的 Checkpoint 表现。</p><a id="more"></a><h1 id="当前-Checkpoint-机制简述"><a href="#当前-Checkpoint-机制简述" class="headerlink" title="当前 Checkpoint 机制简述"></a>当前 Checkpoint 机制简述</h1><p>相信不少读者对 Flink Checkpoint 基于 Chandy-Lamport 算法的分布式快照已经比较熟悉，该节简单回顾下算法的基础逻辑，熟悉算法的读者可放心跳过。</p><p>Chandy-Lamport 算法将分布式系统抽象成 DAG（暂时不考虑有闭环的图），节点表示进程，边表示两个进程间通信的管道。分布式快照的目的是记录下整个系统的状态，即可以分为节点的状态（进程的状态）和边的状态（信道的状态，即传输中的数据）。因为系统状态是由输入的消息序列驱动变化的，我们可以将输入的消息序列分为多个较短的子序列，图的每个节点或边先后处理完某个子序列后，都会进入同一个稳定的全局统状态。利用这个特性，系统的进程和信道在子序列的边界点分别进行本地快照，即使各部分的快照时间点不同，最终也可以组合成一个有意义的全局快照。</p><center><p><img src="/img/flink-unaligned-checkpoint/img1.checkpoint-barrier.png" alt="图1. Checkpoint Barrier" title="图1. Checkpoint Barrier"></p></center><p>从实现上看，Flink 通过在 DAG 数据源定时向数据流注入名为 Barrier 的特殊元素，将连续的数据流切分为多个有限序列，对应多个 Checkpoint 周期。每当接收到 Barrier，算子进行本地的 Checkpoint 快照，并在完成后异步上传本地快照，同时将 Barrier 以广播方式发送至下游。当某个 Checkpoint 的所有 Barrier 到达 DAG 末端且所有算子完成快照，则标志着全局快照的成功。</p><center><p><img src="/img/flink-unaligned-checkpoint/img2.barrier-alignment.png" alt="图2. Barrier Alignment" title="图2. Barrier Alignment"></p></center><p>在有多个输入 Channel 的情况下，为了数据准确性，算子会等待所有流的 Barrier 都到达之后才会开始本地的快照，这种机制被称为 Barrier 对齐。在对齐的过程中，算子只会继续处理的来自未出现 Barrier Channel 的数据，而其余 Channel 的数据会被写入输入队列，直至在队列满后被阻塞。当所有 Barrier 到达后，算子进行本地快照，输出 Barrier 到下游并恢复正常处理。</p><p>比起其他分布式快照，该算法的优势在于辅以 Copy-On-Write 技术的情况下不需要 “Stop The World” 影响应用吞吐量，同时基本不用持久化处理中的数据，只用保存进程的状态信息，大大减小了快照的大小。</p><h1 id="Checkpoint-与反压的耦合"><a href="#Checkpoint-与反压的耦合" class="headerlink" title="Checkpoint 与反压的耦合"></a>Checkpoint 与反压的耦合</h1><p>目前的 Checkpoint 算法在大多数情况下运行良好，然而当作业出现反压时，阻塞式的 Barrier 对齐反而会加剧作业的反压，甚至导致作业的不稳定。</p><p>首先， Chandy-Lamport 分布式快照的结束依赖于 Marker 的流动，而反压则会限制 Marker 的流动，导致快照的完成时间变长甚至超时。无论是哪种情况，都会导致 Checkpoint 的时间点落后于实际数据流较多。这时作业的计算进度是没有被持久化的，处于一个比较脆弱的状态，如果作业出于异常被动重启或者被用户主动重启，作业会回滚丢失一定的进度。如果 Checkpoint 连续超时且没有很好的监控，回滚丢失的进度可能高达一天以上，对于实时业务这通常是不可接受的。更糟糕的是，回滚后的作业落后的 Lag 更大，通常带来更大的反压，形成一个恶性循环。</p><p>其次，Barrier 对齐本身可能成为一个反压的源头，影响上游算子的效率，而这在某些情况下是不必要的。比如典型的情况是一个的作业读取多个 Source，分别进行不同的聚合计算，然后将计算完的结果分别写入不同的 Sink。通常来说，这些不同的 Sink 会复用公共的算子以减少重复计算，但并不希望不同 Source 间相互影响。</p><center><p><img src="/img/flink-unaligned-checkpoint/img3.barrier-alignment-case.png" alt="图3. Barrier Alignment 阻塞上游 Task" title="图3. Barrier Alignment 阻塞上游 Task"></p></center><p>假设一个作业要分别统计 A 和 B 两个业务线的以天为粒度指标，同时还需要统计所有业务线以周为单位的指标，拓扑如上图所示。如果 B 业务线某天的业务量突涨，使得 Checkpoint Barrier 有延迟，那么会导致公用的 Window Aggregate 进行 Barrier 对齐，进而阻塞业务 A 的 FlatMap，最终令业务 A 的计算也出现延迟。</p><p>当然这种情况可以通过拆分作业等方式优化，但难免引入更多开发维护成本，而且更重要的是这本来就符合 Flink 用户常规的开发思路，应该在框架内尽量减小出现用户意料之外的行为的可能性。</p><h1 id="Unaligned-Checkpoint"><a href="#Unaligned-Checkpoint" class="headerlink" title="Unaligned Checkpoint"></a>Unaligned Checkpoint</h1><p>为了解决这个问题，Flink 在 1.11 版本引入了 Unaligned Checkpoint 的特性。要理解 Unaligned Checkpoint 的原理，首先需要了解 Chandy-Lamport 论文中对于 Marker 处理规则的描述:</p><center><p><img src="/img/flink-unaligned-checkpoint/img4.chandy-lamport-marker-handling.png" alt="图4. Chandy-Lamport Marker 处理" title="图4. Chandy-Lamport Marker 处理"></p></center><p>其中关键是 <code>if q has not recorded its state</code>，也就是接收到 Marker 时算子是否已经进行过本地快照。一直以来 Flink 的 Aligned Checkpoint 通过 Barrier 对齐，将本地快照延迟至所有 Barrier 到达，因而这个条件是永真的，从而巧妙地避免了对算子输入队列的状态进行快照，但代价是比较不可控的 Checkpoint 时长和吞吐量的降低。实际上这和 Chandy-Lamport 算法是有一定出入的。</p><p>举个例子，假设我们对两个数据流进行 equal-join，输出匹配上的元素。按照 Flink Aligned Checkpoint 的方式，系统的状态变化如下（图中不同颜色的元素代表属于不同的 Checkpoint 周期）:</p><center><p><img src="/img/flink-unaligned-checkpoint/img5.aligned-checkpoint-status.png" alt="图5. Aligned Checkpoint 状态变化" title="图5. Aligned Checkpoint 状态变化"></p></center><ul><li>图 a: 输入 Channel 1 存在 3 个元素，其中 <code>2</code> 在 Barrier 前面；Channel 2 存在 4 个元素，其中 <code>2</code>、<code>9</code>、<code>7</code> 在 Barrier 前面。</li><li>图 b: 算子分别读取 Channel 一个元素，输出 <code>2</code>。随后接收到 Channel 1 的 Barrier，停止处理 Channel 1 后续的数据，只处理 Channel 2 的数据。</li><li>图 c: 算子再消费 2 个自 Channel 2 的元素，接收到 Barrier，开始本地快照并输出 Barrier。</li></ul><p>对于相同的情况，Chandy-Lamport 算法的状态变化如下:</p><center><p><img src="/img/flink-unaligned-checkpoint/img6.chandy-lamport-status.png" alt="图6. Chandy-Lamport 状态变化" title="图6. Chandy-Lamport 状态变化"></p></center><ul><li>图 a: 同上。</li><li>图 b: 算子分别处理两个 Channel 一个元素，输出结果 <code>2</code>。此后接收到 Channel 1 的 Barrier，算子开始本地快照记录自己的状态，并输出 Barrier。</li><li>图 c: 算子继续正常处理两个 Channel 的输入，输出 <code>9</code>。特别的地方是 Channel 2 后续元素会被保存下来，直到 Channel 2 的 Barrier 出现（即 Channel 2 的 <code>9</code> 和 <code>7</code>）。保存的数据会作为 Channel 的状态成为快照的一部分。</li></ul><p>两者的差异主要可以总结为两点:</p><ol><li>快照的触发是在接收到第一个 Barrier 时还是在接收到最后一个 Barrier 时。</li><li>是否需要阻塞已经接收到 Barrier 的 Channel 的计算。</li></ol><p>从这两点来看，新的 Unaligned Checkpoint 将快照的触发改为第一个 Barrier 且取消阻塞 Channel 的计算，算法上与 Chandy-Lamport 基本一致，同时在实现细节方面结合 Flink 的定位做了几个改进。</p><p>首先，不同于 Chandy-Lamport 模型的只需要考虑算子输入 Channel 的状态，Flink 的算子有输入和输出两种 Channel，在快照时两者的状态都需要被考虑。</p><p>其次，无论在 Chandy-Lamport 还是 Flink Aligned Checkpoint 算法中，Barrier 都必须遵循其在数据流中的位置，算子需要等待 Barrier 被实际处理才开始快照。而 Unaligned Checkpoint 改变了这个设定，允许算子优先摄入并优先输出 Barrier。如此一来，第一个到达 Barrier 会在算子的缓存数据队列（包括输入 Channel 和输出 Channel）中往前跳跃一段距离，而被”插队”的数据和其他输入 Channel 在其 Barrier 之前的数据会被写入快照中（图中黄色部分）。</p><center><p><img src="/img/flink-unaligned-checkpoint/img7.barrier-overtake-data.png" alt="图7. Barrier 越过数据" title="图8. Barrier 越过数据"></p></center><p>这样的主要好处是，如果本身算子的处理就是瓶颈，Chandy-Lamport 的 Barrier 仍会被阻塞，但 Unaligned Checkpoint 则可以在 Barrier 进入输入 Channel 就马上开始快照。这可以从很大程度上加快 Barrier 流经整个 DAG 的速度，从而降低 Checkpoint 整体时长。</p><p>回到之前的例子，用 Unaligned Checkpoint 来实现，状态变化如下:</p><center><p><img src="/img/flink-unaligned-checkpoint/img8.unaligned-checkpoint-status.png" alt="图8. Unaligned-Checkpoint 状态变化" title="图8. Unaligned-Checkpoint 状态变化"></p></center><ul><li>图 a: 输入 Channel 1 存在 3 个元素，其中 <code>2</code> 在 Barrier 前面；Channel 2 存在 4 个元素，其中 <code>2</code>、<code>9</code>、<code>7</code> 在 Barrier 前面。输出 Channel 已存在结果数据 <code>1</code>。</li><li>图 b: 算子优先处理输入 Channel 1 的 Barrier，开始本地快照记录自己的状态，并将 Barrier 插到输出 Channel 末端。</li><li>图 c: 算子继续正常处理两个 Channel 的输入，输出 <code>2</code>、<code>9</code>。同时算子会将 Barrier 越过的数据（即输入 Channel 1 的 <code>2</code> 和输出 Channel 的 <code>1</code>）写入 Checkpoint，并将输入 Channel 2 后续早于 Barrier 的数据（即 <code>2</code>、<code>9</code>、<code>7</code>）持续写入 Checkpoint。</li></ul><p>比起 Aligned Checkpoint 中不同 Checkpoint 周期的数据以算子快照为界限分隔得很清晰，Unaligned Checkpoint 进行快照和输出 Barrier 时，部分本属于当前 Checkpoint 的输入数据还未计算（因此未反映到当前算子状态中），而部分属于当前 Checkpoint 的输出数据却落到 Barrier 之后（因此未反映到下游算子的状态中）。这也正是 Unaligned 的含义: 不同 Checkpoint 周期的数据没有对齐，包括不同输入 Channel 之间的不对齐，以及输入和输出间的不对齐。而这部分不对齐的数据会被快照记录下来，以在恢复状态时重放。换句话说，从 Checkpoint 恢复时，不对齐的数据并不能由 Source 端重放的数据计算得出，同时也没有反映到算子状态中，但因为它们会被 Checkpoint 恢复到对应 Channel 中，所以依然能提供只计算一次的准确结果。</p><p>当然，Unaligned Checkpoint 并不是百分百优于 Aligned Checkpoint，它会带来的已知问题就有:</p><ol><li>由于要持久化缓存数据，State Size 会有比较大的增长，磁盘负载会加重。</li><li>随着 State Size 增长，作业恢复时间可能增长，运维管理难度增加。</li></ol><p>目前看来，Unaligned Checkpoint 更适合容易产生高反压同时又比较重要的复杂作业。对于像数据 ETL 同步等简单作业，更轻量级的 Aligned Checkpoint 显然是更好的选择。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink 1.11 的 Unaligned Checkpoint 主要解决在高反压情况下作业难以完成 Checkpoint 的问题，同时它以磁盘资源为代价，避免了 Checkpoint 可能带来的阻塞，有利于提升 Flink 的资源利用率。随着流计算的普及，未来的 Flink 应用大概会越来越复杂，在未来经过实战打磨完善后 Unaligned Checkpoint 很有可能会取代 Aligned Checkpoint 成为 Flink 的默认 Checkpoint 策略。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-76%3A+Unaligned+Checkpoints" target="_blank" rel="external">FLIP-76: Unaligned Checkpoints</a></li><li><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf" target="_blank" rel="external">Distributed Snapshots: Determining Global States of Distributed Systems</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/internals/stream_checkpointing.html" target="_blank" rel="external">Flink Docs: Data Streaming Fault Tolerance</a></li><li><a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Checkpointing-under-backpressure-td31616.html" target="_blank" rel="external">Checkpointing Under Backpressure</a></li><li><a href="https://zhuanlan.zhihu.com/p/87131964" target="_blank" rel="external">Flink Checkpoint 问题排查实用指南</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为 Flink 最基础也是最关键的容错机制，Checkpoint 快照机制很好地保证了 Flink 应用从异常状态恢复后的数据准确性。同时 Checkpoint 相关的 metrics 也是诊断 Flink 应用健康状态最为重要的指标，成功且耗时较短的 Checkpoint 表明作业运行状况良好，没有异常或反压。然而，由于 Checkpoint 与反压的耦合，反压反过来也会作用于 Checkpoint，导致 Checkpoint 的种种问题。针对于此，Flink 在 1.11 引入 Unaligned Checkpint 来解耦 Checkpoint 机制与反压机制，优化高反压情况下的 Checkpoint 表现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 流批一体的实践与探索</title>
    <link href="https://link3280.github.io/2020/03/30/Flink-%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E5%AE%9E%E8%B7%B5%E4%B8%8E%E6%8E%A2%E7%B4%A2/"/>
    <id>https://link3280.github.io/2020/03/30/Flink-流批一体的实践与探索/</id>
    <published>2020-03-30T15:37:18.000Z</published>
    <updated>2020-04-04T08:24:03.002Z</updated>
    
    <content type="html"><![CDATA[<p>自 Google Dataflow 模型被提出以来，流批一体就成为分布式计算引擎最为主流的发展趋势。流批一体意味着计算引擎同时具备流计算的低延迟和批计算的高吞吐高稳定性，提供统一编程接口开发两种场景的应用并保证它们的底层执行逻辑是一致的。对用户来说流批一体很大程度上减少了开发维护的成本，但同时这对计算引擎来说是一个很大的挑战。作为 Dataflow 模型的最早采用者之一，Apache Flink 在流批一体特性的完成度上在开源项目中是十分领先的。本文将基于社区资料和笔者的经验，介绍 Flink 目前（1.10）流批一体的现状以及未来的发展规划。</p><a id="more"></a><h1 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h1><p>相信不少读者都知道，Flink 遵循 Dataflow 模型的理念: 批处理是流处理的特例。不过出于批处理场景的执行效率、资源需求和复杂度各方面的考虑，在 Flink 设计之初流处理应用和批处理应用尽管底层都是流处理，但在编程 API 上是分开的。这允许 Flink 在执行层面仍沿用批处理的优化技术，并简化掉架构移除掉不需要的 watermark、checkpoint 等特性。</p><center><p><img src="/img/unified-stream-batch/img1.old_architecture.png" alt="图1. Flink 经典架构" title="图1. Flink 经典架构"></p></center><p>在 Flink 架构上，负责物理执行环境的 Runtime 层是统一的流处理，上面分别有独立的 DataStream 和 DataSet 两个 API，两者基于不同的任务类型（Stream Task/Batch Task）和 UDF 接口（Transformation/Operator）。而更上层基于关系代数的 Table API 和 SQL API 虽然表面上是统一的，但实际上编程入口（Environment）是分开的，且内部将流批作业分别翻译到 DataStream API 和 DataSet API 的逻辑也是不一致的。</p><p>因此，要实现真正的流批一体，Flink 需完成 Table/SQL API 的和 DataStream/DataSet API 两层的改造，将批处理完全移植到流处理之上，并且需要兼顾作为批处理立身之本的效率和稳定性。目前流批一体也是 Flink 长期目标中很重要一点，流批一体的完成将标志着 Flink 进入 2.x 的新大版本时代。</p><p>流批一体完成以后理想的架构如下:</p><center><p><img src="/img/unified-stream-batch/img2.new-architecture.png" alt="图2. Flink 未来架构" title="图2. Flink 未来架构"></p></center><p>其中 Planner 从 Table/SQL API 层独立出来变为可插拔的模块，而原先的 DataStream/DataSet 层则会简化为只有 DataStream（图 2 中的 StreamTransformation 和 Stream Operator 是 Stream DAG 的主要内容，分别表示 UDF 和执行 UDF 的算子），DataSet API 将被废弃。</p><h1 id="Table-SQL-API-的改进"><a href="#Table-SQL-API-的改进" class="headerlink" title="Table/SQL API 的改进"></a>Table/SQL API 的改进</h1><p>Table/SQL API 的改造开始得比较早，截止 1.10 版本发布已经达到阶段性的流批一体目标。然而在 1.7 版本时，Table API 只是作为基于 DataStream/DataSet API 的 lib，并没有得到社区的重点关注。而当时阿里的 Blink 已经在 Table/SQL 上做了大量的优化，为了合并 Blink 的先进特性到 Flink，阿里的工程师推进社区重构了 Table 模块的架构[5]并将 Table/SQL API 提升为主要编程 API。自此 Table 层中负责将 SQL/Table API 翻译为 DataStream/DataSet API 的代码被抽象为可插拔的 Table Planner 模块，而 Blink 也将主要的特性以 Blink Planner 的形式贡献给社区，于是有了目前两个 Planner 共存的状态。</p><center><p><img src="/img/unified-stream-batch/img3.backward-compatibility.png" alt="图3. Flink 目前过渡架构" title="图3. Flink 目前过渡架构"></p></center><p>Flink 默认的 Legacy Planner 会将 SQL/Table 程序翻译为 DataStream 或 DataSet 程序，而新的 Blink Planner 则统一翻译为 DataStream 程序。也就是说通过 Blink Planner，Flink Table API 事实上已经实现了流批一体的计算。要了解 Blink Planner 是如何做到的，首先要对 Planner 的工作原理有一定的了解。</p><p>Legacy Planner 对于用户逻辑的表示在 Flink 架构中不同层的演变过程如下:</p><center><p><img src="/img/unified-stream-batch/img4.legacy-planner-architecture.png" alt="图4. Legacy Planner 架构" title="图4. Legacy Planner 架构"></p></center><ol><li>用基于 Calcite 的 SQL parser 解析用户提交的 SQL，将不同类型的 SQL 解析为不同 Operation（比如 DDL 对应 CreateTableOperation，DSL 对应 QueryOperation），并将 AST 以关系代数 Calcite RelNode 的形式表示。</li><li>根据用户指定 TableEnvironment 的不同，分别使用不同的翻译途径，将逻辑关系代数节点 RelNode 翻译为 Stream 的 Transformation 或者 Batch 的 Operator Tree。</li><li>调用 DataStream 和 DataSet 对应环境的方法将 Transformation 或 Operator Tree 翻译为包含执行环境配置的作业表示，即 StreamGraph 或 Plan。</li><li>优化 StreamGraph 和 Plan，并包装为可序列化的 JobGraph。</li></ol><p>因为 Batch SQL 与 Streaming SQL 在大部分语法及语义上是一致的，不同点在于 Streaming SQL 另有拓展语法的来支持 Watermark、Time Characteristic 等流处理领域的特性，因此 SQL parser 是 Batch/Stream 共用的。关键点在于对于关系代数 RelNode 的翻译上。</p><center><p><img src="/img/unified-stream-batch/img5.relnode-uml.png" alt="图5. Legacy Planner RelNode 类图" title="图5. Flink RelNode 类图"></p></center><p>Flink 基于 Calcite RelNode 拓展了自己的 FlinkRelNode，FlinkRelNode 有三个子类 FlinkLogicalRel、DataSetRel 和 DataStreamRel。FlinkLogicalRel 表示逻辑的关系代数节点，比如常见的 Map 函数对应的 FlinkLogicalRel 是 DataStreamCalc。DataSetRel 和 DataStreamRel 则分别表示 FlinkLogicalRel 在批处理和流处理下各自的物理执行计算。</p><p>在 SQL 优化过程中，根据编程入口的不同 FlinkLogicalRel 被转化为 DataSetRel 或 DataStreamRel。BatchTableEnvironment 使用 BatchOptimizer 基于 Calcite Rule 的优化，而 StreamTableEnvironment 使用 StreamOptimizer 进行优化。比如 TableScan 这样一个 RelNode，在 Batch 环境下被翻译为 BatchTableSourceScan，在 Stream 环境下被翻译为 StreamTableSourceScan，而这两类物理关系代数节点将可以直接映射到 DataSet 的 Operator 或 DataStream 的 Transformation 上。</p><p>上述的方式最大的问题在于 Calcite 的优化规则无法复用，比如对数据源进行过滤器下推的优化，那么需要给 DateSetRel 和 DataStreamRel 分别做一套，而且 DataSet 和 DataStream 层的算子也要分别进行相应的修改，开发维护成本很高，而这也是 Blink Planner 推动流批一体的主要动力。</p><p>如上文所说，Blink Planner 做的最重要的一点就是废弃了 DataSet 相关的翻译途径，将 DateSetRel 也移植到 DataStream 之上，那么前提当然是 DataStream 要可以表达 DataSet 的语义。熟悉批处理的同学可能会有疑问: 批处理特有的排序等算子，在 DataStream 中是没有的，这将如何表达？事实上 Table Planner 广泛采用了动态代码生成，可以绕过 DataStream API 直接翻译至底层的 Transformation 和 StreamOperator 上，并不一定需要 DataStream 有现成的算子，因此使用 Blink Planner 的 Table API 与 DataStream API 的关系更多是并列的关系。这也是 FLIP-32[5] 所提到的解耦 Table API 和 DataStream/DataSet API 的意思:</p><blockquote><p><strong>Decouple table programs from DataStream/DataSet API</strong><br>Allow table programs to be self-contained. No need for a Stream/ExecutionEnvironment entrypoint anymore. A table program definition is just API that reads and writes to catalog tables.</p></blockquote><p>Table 改造完成后整个 API 架构如下，这也是目前 1.10 版本已经实现的架构:</p><center><p><img src="/img/unified-stream-batch/img6.blink-planner-architecture.png" alt="图6. Blink Planner 架构" title="图6. Blink Planner 架构"></p></center><p>事实上，早前版本的 DataStream 对批作业的支持并不是太好，为了支持 Blink Planner 的 Batch on Stream，DataStream 方面也先做了不少的优化。这些优化是对于 Table API 是必要的，因此在 Blink Planner 合并到 Flink master 的前置工作，这将和 DataStream 还未完成的改进一起放在下文分析。另外虽然 Blink Planner 在计算上是流批一体的，但 Flink Table API 的 TableSource 和 TableSink 仍是流批分离的，这意味着目前绝大数批处理场景的基于 BatchTableSource/BatchTableSink 的 Table 无法很好地跟流批一体的计算合作，这将在 FLIP-95[9] 中处理。</p><h1 id="DataStream-API-的改进"><a href="#DataStream-API-的改进" class="headerlink" title="DataStream API 的改进"></a>DataStream API 的改进</h1><p>在 DataStream API 方面，虽然目前的 DataStream API 已经可以支持有界数据流，但这个支持并不完整且效率上比起 DataSet API 仍有差距。为了实现完全的流批一体，Flink 社区准备在 DataStream 引入 BoundedStream 的概念来表示有界的数据流，完全从各种意义上代替 DataSet。BoundedStream 将是 DataStream 的特例，同样使用 Transformation 和 StreamOperator，且同时需要继承 DataSet 的批处理优化。这些优化可以分为 Task 线程模式、调度策略及容错和计算模型及算法这几部分。</p><h2 id="Task-线程模型"><a href="#Task-线程模型" class="headerlink" title="Task 线程模型"></a>Task 线程模型</h2><p>批处理业务场景通常更重视高吞吐，出于这点考虑，Batch Task 是 pull-based 的，方便 Task 批量拉取数据。Task 启动后会主动通过 DataSet 的 Source API InputFormat 来读取外部数据源，每个 Task 同时只读取和处理一个 Split。</p><p>相比之下，一般流处理业务场景则更注重延迟，因此 Stream Task 是 push-based 的。DataStream 的 Source API SourceFunction 会被独立的 Source Thread 执行，并一直读取外部数据，源源不断地将数据 push 给 Stream Task。每个 Source Thread 可以并发读取一个到多个 Split/Partition/Shard。</p><center><p><img src="/img/unified-stream-batch/img7.source-thread-model.png" alt="图7. Stream/Batch 线程模型" title="图7. Stream/Batch 线程模型（图来源 Flink Forward）"></p></center><p>为了解决 Task 线程模型上的差异，Flink 社区计划重构 Source API 来统一不同外部存储和业务场景下的 Task 线程模型。总体的思路是新增一套新的 Source API，可以支持多种线程模型，覆盖流批两种业务需求，具体可见 FLIP-27[6] 或笔者早前的一篇博客[7]。目前 FLIP-27 仍处于初步的开发阶段。</p><h2 id="调度策略及容错"><a href="#调度策略及容错" class="headerlink" title="调度策略及容错"></a>调度策略及容错</h2><p>众所周知，批处理作业和流处理作业在 Task 调度上是很不同的。批处理作业的多个 Task 并不需要同时在线，可以根据依赖关系先调度一批 Task，等它们结束后再运行另一批。相反地，流作业的所有 Task 需要在作业启动的时候就全部被调度，然后才可以开始处理数据。前一种调度策略通常称为懒调度（Lazy Scheduling），后一种通常称为激进调度（Eager Scheduling）。为了实现流批一体，Flink 需要在 StreamGraph 中同时支持这两种调度模式，也就是说新增懒调度。</p><p>随调度而来的问题还有容错，这并不难理解，因为 Task 出现错误后需要重新调度来恢复。而懒调度的一大特点是，Task 计算的中间结果需要保存在某个高可用的存储中，然后下个 Task 启动后才能去获取。而在 1.9 版本以前，Flink 并没有持久化中间结果。这就导致了如果该 TaskManager 崩溃，中间结果会丢失，整个作业需要从头读取数据或者从 checkpoint 来恢复。这对于实时流处理来说是很正常的，然而批处理作业并没有 checkpoint 这个概念，批处理通常依赖中间结果的持久化来减小需要重算的 Task 范围，因此 Flink 社区引入了可插拔的 Shuffle Service 来提供 Suffle 数据的持久化以支持细粒度的容错恢复，具体可见 FLIP-31[8]。</p><h2 id="计算模型及算法"><a href="#计算模型及算法" class="headerlink" title="计算模型及算法"></a>计算模型及算法</h2><p>与 Table API 相似，同一种计算在流处理和批处理中的算法可能是不同的。典型的一个例子是 Join: 它在流处理中表现为两个流的元素的持续关联，任何一方的有新的输入都需要跟另外一方的全部元素进行关联操作，也就是最基础的 Nested-Loop Join；而在批处理中，Flink 可以将它优化为 Hash Join，即先读取一方的全部数据构建 Hash Table，再读取另外一方进行和 Hash Table 进行关联（见图8）。</p><center><p><img src="/img/unified-stream-batch/img8.join-batch-optimization.png" alt="图8. Join 批处理优化" title="图8. Join 批处理优化"></p></center><p>这种差异性本质是算子在数据集有界的情况下的优化。拓展来看，数据集是否有界是 Flink 在判断算子如何执行时的一种优化参数，这也印证了批处理是流处理的特例的理念。因此从编程接口上看，BoundedStream 作为 DataStream 的子类，基于输入的有界性可以提供如下优化：</p><ul><li>提供只可以应用于有界数据流的算子，比如 sort。</li><li>对某些算子可以进行算法上的优化，比如 join。</li></ul><p>此外，批处理还有个特点是不需要在计算时输出中间结果，只要在结束时输出最终结果，这很大程度上避免了处理多个中间结果的复杂性。因此，BoundedStream 还会支持非增量（non-incremental）执行模式。这主要会作用于与 Time Charateritic 相关的算子:</p><ul><li>Processing Time Timer 将被屏蔽。</li><li>Watermark 的提取算法不再生效，Watermark 直接从开始时的 -∞ 跳到结束时的 +∞。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>基于批处理是流处理的特例的理念，用流处理表达批处理在语义上是完全可行的，而流批一体的难点在于批处理场景作为特殊场景的优化。对 Flink 而言，难点主要体现批处理作业在 Task 线程模型、调度策略和计算模型及算法的差异性上。目前 Flink 已经在偏声明式的 Table/SQL API 上实现了流批一体，而更底层偏过程式的 DataStream API 也将在 Flink 2.0 实现流批一体。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.infoq.cn/article/Wj8SYfBoVkgQoVP-Y2uf" target="_blank" rel="external">Flink 流批一体的技术架构以及在阿里的实践</a></li><li><a href="https://www.alibabacloud.com/blog/whats-all-involved-with-blink-merging-with-apache-flink_595401" target="_blank" rel="external">What’s All Involved with Blink Merging with Apache Flink?</a></li><li><a href="https://matt33.com/2019/12/09/flink-job-graph-3/" target="_blank" rel="external">Flink Streaming 作业如何转化为 JobGraph</a></li><li><a href="https://flink.apache.org/news/2019/02/13/unified-batch-streaming-blink.html" target="_blank" rel="external">Batch as a Special Case of Streaming and Alibaba’s contribution of Blink</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="external">FLIP-32: Restructure flink-table for future contributions</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface?src=contextnavpagetreemode" target="_blank" rel="external">FLIP-27: Refactor Source Interface</a></li><li><a href="http://www.whitewood.me/2020/02/11/%E6%BC%AB%E8%B0%88-Flink-Source-%E6%8E%A5%E5%8F%A3%E9%87%8D%E6%9E%84/" target="_blank" rel="external">漫谈 Flink Source 接口重构</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-31%3A+Pluggable+Shuffle+Service" target="_blank" rel="external">FLIP-31: Pluggable Shuffle Service</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-95%3A+New+TableSource+and+TableSink+interfaces" target="_blank" rel="external">FLIP-95: New TableSource and TableSink interfaces</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自 Google Dataflow 模型被提出以来，流批一体就成为分布式计算引擎最为主流的发展趋势。流批一体意味着计算引擎同时具备流计算的低延迟和批计算的高吞吐高稳定性，提供统一编程接口开发两种场景的应用并保证它们的底层执行逻辑是一致的。对用户来说流批一体很大程度上减少了开发维护的成本，但同时这对计算引擎来说是一个很大的挑战。作为 Dataflow 模型的最早采用者之一，Apache Flink 在流批一体特性的完成度上在开源项目中是十分领先的。本文将基于社区资料和笔者的经验，介绍 Flink 目前（1.10）流批一体的现状以及未来的发展规划。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Table 的三种 Sink 模式</title>
    <link href="https://link3280.github.io/2020/02/26/Flink-Table-%E7%9A%84%E4%B8%89%E7%A7%8D-Sink-%E6%A8%A1%E5%BC%8F/"/>
    <id>https://link3280.github.io/2020/02/26/Flink-Table-的三种-Sink-模式/</id>
    <published>2020-02-26T13:19:52.000Z</published>
    <updated>2020-02-26T13:25:34.037Z</updated>
    
    <content type="html"><![CDATA[<p>作为计算引擎 Flink 应用的计算结果总要以某种方式输出，比如调试阶段的打印到控制台或者生产阶段的写到数据库。而对于本来就需要在 Flink 内存保存中间及最终计算结果的应用来说，比如进行聚合统计的应用，输出结果便是将内存中的结果同步到外部。就 Flink Table/SQL API 而言，这里的同步会有三种模式，分别是 Append、Upsert 和 Retract。实际上这些输出计算结果的模式并不限于某个计算框架，比如 Storm、Spark 或者 Flink DataStream 都可以应用这些模式，不过 Flink Table/SQL 已有完整的概念和内置实现，更方便讨论。</p><a id="more"></a><h1 id="基础原理"><a href="#基础原理" class="headerlink" title="基础原理"></a>基础原理</h1><p>相信接触过 Streaming SQL 的同学都有了解或者听过流表二象性，简单来说流和表是同一事实的不同表现，是可以相互转换的。流和表的表述在业界不尽相同，笔者比较喜欢的一种是: 流体现事实在时间维度上的变化，而表则体现事实在某个时间点的视图。如果将流比作水管中流动的水，那么表将是杯子里静止的水。</p><p>将流转换为表的方法对于大多数读者都不陌生，只需将聚合统计函数应用到流上，流很自然就变为表（值得注意的是，Flink 的 Dynamic Table 和表的定义有细微不同，这将在下文讲述）。比如对于一个计算 PV 的简单流计算作业，将用户浏览日志数据流安 url 分类统计，变成 <code>(url, views)</code> 这样的一个表。然而对于如何将表转换成流，读者则未必有这么清晰的概念。</p><p>假设一个典型的实时流计算应用的工作流程可以被简化为下图:</p><center><p><img src="/img/flink-sink-pattern/img1.programing-model.png" alt="图1. Flink 编程模型"></p></center><p>其中很关键的一点是 Transformation 是否聚合类型的计算。若否，则输出结果依然是流，可以很自然地使用原本流处理的 Sink（与外部系统的连接器）；若是，则流会转换为表，那么输出的结果将是表，而一个表的输出通常是批处理的概念，不能直接简单地用流处理的 Sink 来表达。</p><p>这时有个很朴素的想法是，我们能不能避免批处理那种全量的输出，每次只输出表的 diff，也就是 changelog。这也是表转化为流的方法: 持续观察表的变化，并将每个变化记录成日志输出。因此，流和表的转换可以以下图表示:</p><center><p><img src="/img/flink-sink-pattern/img2.stream-table-conversion.png" alt="图2. Flink 编程模型"></p></center><p>其中表的变化具体可以分为 <code>INSERT</code>、<code>UPDATE</code> 和 <code>DELETE</code> 三类，而 Flink 根据这些变化类型分别总结了三种结果的输出模式。</p><table><thead><tr><th>模式</th><th>INSERT</th><th>UPDATE</th><th>DELETE</th></tr></thead><tbody><tr><td>Append</td><td>支持</td><td>不支持</td><td>不支持</td></tr><tr><td>Upsert</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>Retract</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table><p>通常来说 Append 是最容易实现但功能最弱的，Retract 是最难实现而功能最强的。下文分别谈谈三种模式的特点和应用场景。</p><h1 id="Append-输出模式"><a href="#Append-输出模式" class="headerlink" title="Append 输出模式"></a>Append 输出模式</h1><p>Append 是最为简单的输出模式，只支持追加结果记录的操作。因为结果一旦输出以后便不会再有变更，Append 输出模式的最大特性是不可变性（immutability），而不可变性最令人向往的优势便是安全，比如线程安全或者 Event Sourcing 的可恢复性，不过同时也会给业务操作带来限制。通常来说，Append 模式会用于写入不方便做撤回或者删除操作的存储系统的场景，比如 Kafka 等 MQ 或者打印到控制台。</p><p>在实时聚合统计中，聚合统计的结果输出是由 Trigger 决定的，而 Append-Only 则意味着对于每个窗口实例（Pane，窗格）Trigger 只能触发一次，则就导致无法在迟到数据到达时再刷新结果。通常来说，我们可以给 Watermark 设置一个较大的延迟容忍阈值来避免这种刷新（再有迟到数据则丢弃），但代价是却会引入较大的延迟。</p><p>不过对于不涉及聚合的 Table 来说，Append 输出模式是非常好用的，因为这类 Table 只是将数据流的记录按时间顺序排在一起，每条记录间的计算都是独立的。值得注意的是，从 DataFlow Model 的角度来看未做聚合操作的流不应当称为表，但是在 Flink 的概念里所有的流都可以称为 Dynamic Table。笔者认为这个设计也有一定的道理，原因是从流中截取一段出来依然可以满足表的定义，即”某个时间点的视图”，而且我们可以争辩说<code>不聚合</code>也是一种聚合函数。</p><h1 id="Upsert-输出模式"><a href="#Upsert-输出模式" class="headerlink" title="Upsert 输出模式"></a>Upsert 输出模式</h1><p>Upsert 是 Append 模式的升级版，支持 Append-Only 的操作和在有主键的前提下的 UPDATE 和 DELETE 操作。Upsert 模式依赖业务主键来实现输出结果的更新和删除，因此非常适合 KV 数据库，比如<br>HBase、JDBC 的 TableSink 都使用了这种方式。</p><p>在底层，Upsert 模式下的结果更新会被翻译为 (Boolean, ROW) 的二元组。其中第一个元素表示操作类型，<code>true</code> 对应 <code>UPSERT</code> 操作（不存在该元素则 <code>INSERT</code>，存在则 <code>UPDATE</code>），<code>false</code> 对应 <code>DELETE</code> 操作，第二个元素则是操作对应的记录。如果结果表本身是 Append-Only 的，第一个元素会全部为 <code>true</code>，而且也无需提供业务主键。</p><p>Upsert 模式是目前来说比较实用的模式，因为大部分业务都会提供原子或复合类型的主键，而在支持 KV 的存储系统也非常多，但要注意的是不要变更主键，具体原因会在下一节谈到。</p><h1 id="Retract-输出模式"><a href="#Retract-输出模式" class="headerlink" title="Retract 输出模式"></a>Retract 输出模式</h1><p>Retract 是三种输出模式中功能最强大但实现也最复杂的一种，它要求目标存储系统可以追踪每个条记录，而且这些记录至少在一定时间内都是可以撤回的，因此通常来说它会自带系统主键，不必依赖于业务主键。然而由于大数据存储系统很少有可以精确到一条记录的更新操作，因此目前来说至少在 Flink 原生的 TableSink 中还没有能在生产环境中满足这个要求的。</p><p>不同于 Upsert 模式更新时会将整条记录重新输出，Retract 模式会将更新分成两条表示增减量的消息，一条是 <code>(false, OldRow)</code> 的撤回（Retract）操作，一条是 <code>(true, NewRow)</code> 的积累（Accumulate）操作。这样的好处是，在主键出现变化的情况下，<code>Upsert</code> 输出模式无法撤回旧主键的记录，导致数据不准确，而 <code>Retract</code> 模式则不存在这个问题。</p><p>举个例子，假设我们将电商订单按照承运快递公司进行分类计数，有如下的结果表。</p><table><thead><tr><th>公司</th><th>订单数</th></tr></thead><tbody><tr><td>中通</td><td>2</td></tr><tr><td>圆通</td><td>1</td></tr><tr><td>顺丰</td><td>3</td></tr></tbody></table><p>那么如果原本一单为中通的快递，后续更新为用顺丰发货，对于 Upsert 模式会产生 <code>(true, (顺丰, 4))</code> 这样一条 changelog，但中通的订单数没有被修正。相比之下，Retract 模式产出 <code>(false, (中通, 1))</code> 和 <code>(true, (顺丰, 1))</code> 两条数据，则可以正确地更新数据。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink Table Sink 的三种模式本质上是如何监控结果表并产生 changelog，这可以应用于所有需要将表转为流的场景，包括同一个 Flink 应用的不同表间的联动。三种模式中 Append 模式只支持表的 <code>INSERT</code>，最为简单；Upsert 模式依赖业务主键提供 <code>INSERT</code>、<code>UPDATE</code> 和 <code>DELETE</code> 全部三类变更，比较实用；Retract 模式同样支持三类变更且不要求业务主键，但会将 <code>UPDATE</code> 翻译为旧数据的撤回和新数据的累加，实现上比较复杂。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.confluent.io/blog/making-sense-of-stream-processing/" target="_blank" rel="external">Stream Processing, Event Sourcing, and Data Streaming Explained</a></li><li><a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/" target="_blank" rel="external">Introducing Kafka Streams: Stream Processing Made Simple</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/streaming/dynamic_tables.html" target="_blank" rel="external">Dynamic Tables</a></li><li><a href="https://blog.jrwang.me/2019/2019-10-16-flink-sourcecode-stream-and-dynamic-table/" target="_blank" rel="external">Flink 源码阅读笔记（18）- Flink SQL 中的流和动态表</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为计算引擎 Flink 应用的计算结果总要以某种方式输出，比如调试阶段的打印到控制台或者生产阶段的写到数据库。而对于本来就需要在 Flink 内存保存中间及最终计算结果的应用来说，比如进行聚合统计的应用，输出结果便是将内存中的结果同步到外部。就 Flink Table/SQL API 而言，这里的同步会有三种模式，分别是 Append、Upsert 和 Retract。实际上这些输出计算结果的模式并不限于某个计算框架，比如 Storm、Spark 或者 Flink DataStream 都可以应用这些模式，不过 Flink Table/SQL 已有完整的概念和内置实现，更方便讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>漫谈 Flink Source 接口重构</title>
    <link href="https://link3280.github.io/2020/02/11/%E6%BC%AB%E8%B0%88-Flink-Source-%E6%8E%A5%E5%8F%A3%E9%87%8D%E6%9E%84/"/>
    <id>https://link3280.github.io/2020/02/11/漫谈-Flink-Source-接口重构/</id>
    <published>2020-02-11T12:24:52.000Z</published>
    <updated>2020-02-11T12:27:30.984Z</updated>
    
    <content type="html"><![CDATA[<p>对于大多数 Flink 应用开发者而言，无论使用高级的 Table API 或者是底层的 DataStream/DataSet API，Source 都是首先接触到且使用最多的 Operator 之一。然而其实从 2018 年 10 月开始，Flink 社区就开始计划重构这个稳定了多年的 Source 接口[1]，以满足更大规模数据以及对接更丰富的 connector 的要求，另外还有更重要的一个目的: 统一流批两种计算模式。重构后的 Source 接口在概念和使用方式上都会有较大不同，无论对 Flink 应用开发者还是 Flink 社区贡献者来说都是十分值得关注的，所以本文将从”为什么要这样设计”的角度来谈谈 Source 接口重构的前因后果。这会涉及到较多的底层架构内容，要求读者有一定的基础或者有探索的兴趣。</p><a id="more"></a><h1 id="现有-Source-接口"><a href="#现有-Source-接口" class="headerlink" title="现有 Source 接口"></a>现有 Source 接口</h1><p>目前（Flink 1.9）Source 接口分为 DataStream/DataSet/Table API 三个不同的栈，但因为 Table API 是基于前两者的封装，我们在讨论底层接口的时候可以先排除掉它。Source 接口在 DataStream/DataSet API 中同样是负责数据的生成或摄入，但除此之外的功能有不小的差异。</p><h2 id="DataStream-API"><a href="#DataStream-API" class="headerlink" title="DataStream API"></a>DataStream API</h2><p>在 DataStream API 中 Source 对应的核心接口为 SourceFunction 以及 SourceContext。前者直接继承 Function 接口与 Operator 交互，负责通用的状态管理（比如初始化或取消）；后者代表运行时的上下文，负责与单条记录级别的数据的交互。此外还有其他一些辅助类型的类或接口，整体的类图设计如下:</p><center><p><img src="/img/flip-27/img1.datastream_source.png" alt="图一. DataStream API 的 Source 接口" title="图一. DataStream API 的 Source 接口"></p></center><p>其中 ParallelSourceFunction 进一步继承 SourceFunction，标记该 Source 为可并行化的，否则直接实现 SourceFunction 的 Source 的并行度只能为 1。而 RichParallelSourceFunction 则是在 ParallelSourceFunction 基础之上再结合 AbstractRichFunction，提供有状态的并行 Source 基类。用户要实现一个 Source，可以选择 SourceFunction、ParallelSourceFunction 或<br>RichParallelSourceFunction 中任意一个来作为切入口。但值得注意的是，如果 Source 是有状态的，那么为了保证一致性，状态的更新和正常的数据输出是不可以并行的。为此，SourceContext 提供了 Checkpoint 锁来方便 Source 进行同步阻塞。</p><p>运行时，Source 主要通过 SourceContext 来控制数据的输出。从 SourceContext 接口的方法即可以看出，Source 在接受到数据后的主要工作有以下几点:</p><ol><li>从外部摄入数据或生成数据，输出到下游。</li><li>为数据生成 Event Time Timestamp（仅在 Time Characteristic 为 Event Time 时有用），比如 Kafka Source 的 Partition 级别的 Event time。</li><li>计算 Watermark 并输出（仅在 Time Charateristic 为 Event Time 时有用）。</li><li>当暂时不会有新数据时将自己标记为 Idle，以避免下游一直等待自己的 Watermark。</li></ol><h2 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h2><p>在 DataSet API 中 Source 对应的核心接口为 InputFormat。InputFormat 命名风格上借鉴了 Hadoop 的风格，在功能上也比较相近，具体有以下三点: </p><ol><li>描述输入的数据如何被划分为不同的 InputSplit（继承于 InputSplitSource）。</li><li>描述如何从单个 InputSplit 读取记录，具体包括如何打开一个分配到的 InputSplit，如何从这个 InputSplit 读取一条记录，如何得知记录已经读完和如何关闭这个 InputSplit。</li><li>描述如何获取输入数据的统计信息（比如文件的大小、记录的数目），以帮助更好地优化执行计划。</li></ol><p>第 1、3 两点功能会被 JobManager (JobMaster) 在调度 Exection 时使用，而第 2 点读取数据功能则会在运行时被 TaskManager 使用。</p><p>围绕 InputFormat，DataSet 还提供一系列接口，总体的类图如下:</p><center><p><img src="/img/flip-27/img2.dataset_source.png" alt="图二. DataSet API 的 Source 接口" title="图二. DataSet API 的 Source 接口"></p></center><ul><li>InputSplitSource 为 InputFormat 的超类，负责划分 InputSplit （第一点功能），不再赘述。</li><li>InputSplit 表示一个逻辑分区，必要的信息其实只有 Split 的 ID（或者下标），InputFormat 会根据这个 ID 来读取输入数据的对应分区。</li><li>RichInputFormat 拓展 InputFormat，加上 <code>openInputFormat()</code> 和 <code>closeInputFormat()</code> 方法来管理运行时的状态。比起 InputFormat 的 <code>open()</code> 和 <code>close()</code> 是在每个 InputSplit 级别调用，它们是在每次 Task Exectuion 级别调用，而每次  Task Exectuion 可以读多个 InputSplit。比如 TaskManager 要读取 HBase Table，那么它要打开和关闭一个 HTable 的连接，这个连接可以在多读多个 TableInputSplit 时复用。</li><li>ReplicatingInputFormat 拓展 RichInputFormat，为输入数据提供广播的能力。换句话说，通过 ReplicatingInputFormat 输入的数据会被每个实例重复读取，典型的应用是 Join 操作。</li></ul><p>有趣的是，除了上述典型的 DataSet 场景，InputFormat 还可以在 Streaming 场景中使用。通过 <code>StreamExecutionEnvironment#createInput(InputFormat)</code>，Flink 可以持续监控一个文件系统目录。InputFormat 会被传递给 <code>ContinuousFileReaderOperator</code>，后者是一个非并行化的算子（并行度只能为 1），会将目录新增的文件作为 <code>FileInputSplit</code> 传递给下游的 <code>ContinuousFileReaderOperator</code>，然后 <code>ContinuousFileReaderOperator</code> 再使用 InputFormat 来读取这些 InputSplit。所以虽然架构设计上不是特别一致，但 InputFormat 一定程度上是体现了流批统一的思想的。</p><h1 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h1><h2 id="不符合流批一体要求"><a href="#不符合流批一体要求" class="headerlink" title="不符合流批一体要求"></a>不符合流批一体要求</h2><p>首先，目前的 Source 接口栈最显而易见的问题 DataStream 和 DataSet 在 API 设计上的不统一。这个很大程度上是出于历史原因，在 Flink 最初开发之时业界普遍认为批处理和流处理是相对独立的，而直到 2016 年 Google 《The Dataflow Model》等文章的发表，业界才有比较完整的理论支持来统一两者。所以 Flink 社区当时分离开 DataStream 和 DataSet 来分别迭代开发也十分正常，但这也成了流批融合的新趋势下的负担。</p><p>现在的状况是 DataStream 和 DataSet 共享很多相同的代码，比如面向用户代码 UDF 的基础接口 Function 或计算逻辑的通用 RichFunction 都代码在 <code>flink-core</code> 中，但分场景使用的代码则分别存在于 <code>flink-java</code> 和 <code>flink-streaming-java</code> 中。典型的是两者在 Operator 上也是不同的，前者使用 <code>org.apache.flink.api.java.operators.Operator</code> 作为基类，而后者使用的是 <code>org.apache.flink.streaming.api.operators.StreamOperator</code>。StreamSource 和 DataSource 是 DataStream 和 DataSet 代表数据源的 Operator，它们分别封装了上文所说的 SourceFunction 和 InputFormat 两个接口。总体的关系大致如下（省去了部分非关键的类或接口）。</p><center><p><img src="/img/flip-27/img3.function_api.png" alt="图三. DataStream 与 DataSet Source 接口的关系" title="图三. DataStream 与 DataSet Source 接口的关系"></p></center><p>从图中可以方便地看到一个很不协调的地方便是 StreamSource 属于 AbstractUdfStreamOperator，因此可以直接使用 Function 接口，但是 DataSource 却不属于 SingleInputUdfOperator （这里应该是 AbstractUdfOperator 才合理，但 DataSet API 没有提供这层抽象），因此具体的读取数据源逻辑不是写在 Function 中，而是写在 InputFormat 中，这就造成需要为同一种外部存储系统开发维护两套重复性很高 Source。</p><h2 id="不便动态发现数据源变更"><a href="#不便动态发现数据源变更" class="headerlink" title="不便动态发现数据源变更"></a>不便动态发现数据源变更</h2><p>分布式存储系统通常都以某种”存储块”的方式来实现水平拓展，这种”存储块”在不同系统中有不同的命名，常见的有 Split/Partition/Shard，下游的计算引擎也会按照这些”存储块”的粒度进行工作分配。</p><p>在批处理计算中，输入数据源以作业启动时读取的元数据为准， Split 的数目不会在运行时改变，不需要动态监控数据源变化，但需要根据 TaskManager 处理的进度来动态分配 Split。因此 Flink DataSet 的做法是抽象出 Split Assigner（属于 InputFormat 的一部分）。作业启动时 Split Assigner 会读取数据源的元数据，随后一直运行在 JobManager 端负责将现有 Split 分配给空余的 TaskManager，直至所有的 Split 都完成处理 。</p><p>而实时流处理则却通常要动态发现新增的 Split，然后分配到现有的 TaskManager 上，比如 Kafka 的 Partition Discovery，同时也需要动态分配新 Partition 这样的一个机制。通常来说 Source 需要在初始化时新建一个线程来负责检测数据源的变更，若有则需要重新调整工作分配。不过与 DataSet 统一 JobManager 端中心化分配不同，DataStream 做动态检测的线程运行在每个 TaskManager 上，新发现的 Partition 是依靠每个 SubTask 按预先的规则分配。这样背后的原因是，InputFormat 设计了一部分逻辑运行在 JobManager 上，而 SourceFunction 则完全运行在 TaskManager 上，缺乏一个中心化的管理者。</p><h2 id="不便-Source-Subtask-间的协作"><a href="#不便-Source-Subtask-间的协作" class="headerlink" title="不便 Source Subtask 间的协作"></a>不便 Source Subtask 间的协作</h2><p>不同于 DataSet 中 Source Subtask 之间几乎是完全独立的，DataStream 中 Source Subtask 通常需要某种程度上的协作，比如不同 Subtask 之间的 Event Time 对齐。 </p><p>Event Time 对齐的背景是 Subtask 间的 Event Time 进度可能是不同的，但下游 Watermark 总是取最低者，这就导致对于基于 Watermark 的算子来说，它一直从 Event Time 快的 Subtask 摄入数据但这些数据总是得不到清理，进一步造成该算子的 State 逐渐膨胀。</p><p>解决这个问题的思路是，在一个 Source Subtask 自己 Event Time 明显先进于其他 Source Subtask 时，与其继续摄入数据并让下游自己缓存，不如直接阻塞自己的消费来等其他 Source Subtask 跟上，这就称为 Event Time 对齐。</p><p>Event Time 对齐要求 Source Subtask 间的协作，通常需要在 Master 节点上新增一个协调者（Coordinator），由协调者来管控 Split/Partition 的元数据（这点在目前的 SourceFunction 接口上是做不到的），来判定某个 Source Subtask 是否需要阻塞。另外，相似的协作需求还有 Work Stealing 等。</p><h2 id="容易造成瓶颈的-Checkpoint-Lock"><a href="#容易造成瓶颈的-Checkpoint-Lock" class="headerlink" title="容易造成瓶颈的 Checkpoint Lock"></a>容易造成瓶颈的 Checkpoint Lock</h2><p>在 DataStream 作业中，为了保证 State 更新和输出记录的一致性，两者是要通过 Checkpoint Lock 来进行同步的。SourceFunction 可以通过 SourceContext 来获取 Checkpoint Lock，例如如下代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">public void run(SourceContext&lt;T&gt; ctx) &#123;</div><div class="line">    while (isRunning) &#123;</div><div class="line">        synchronized (ctx.getCheckpointLock()) &#123;</div><div class="line">            ctx.collect(count);</div><div class="line">            count++;</div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>而问题在于这个锁并不是公平锁，也就是说 SourceFunction 有可能一直占据 Checkpoint Lock 导致 Checkpoint 被阻塞。另外这种比较重度的锁也不符合 actor 或者说 mailbox 模式的非阻塞设计。</p><h2 id="线程缺乏统一管理"><a href="#线程缺乏统一管理" class="headerlink" title="线程缺乏统一管理"></a>线程缺乏统一管理</h2><p>在 DataStream 应用中，Source 通常会需要一些 IO 线程来避免阻塞 Task 主线程，而这些线程目前是每个 Source 独立实现，这就造成各个 Source 需要自己设计复杂的线程模型。比如常用的 Kafka Connector，每个 FlinkKafkaConsumer 会额外启动一个 Fetcher 线程负责调用 Kafka Consumer API 进行消费，然后通过阻塞队列交给 TaskThread 来进行消费。</p><h1 id="改进思路"><a href="#改进思路" class="headerlink" title="改进思路"></a>改进思路</h1><h2 id="统一流批-Source-接口"><a href="#统一流批-Source-接口" class="headerlink" title="统一流批 Source 接口"></a>统一流批 Source 接口</h2><p>作为统一流批处理算子的最前一环，Source 接口首先需要被按照 Flink 推崇的”批处理是流处理的特例”的思想重新设计。按照社区长期目标，Flink 会新增 BoundedDataStream 来逐步取代 DataSet，而 BoundedDataStream 基于目前的 DataStream API，算子基本可以复用。</p><p>按照 FLIP-27，新的 Source 接口暂停名为 <code>Source</code>，它类似一个工厂类，主要构造 <code>SplitEnumerator</code> 和 <code>SplitReader</code>（该两者的作用将在下一节提及），并且可以同时为流批服务。<code>Source</code> 的使用方式将大致如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</div><div class="line"> </div><div class="line">FileSource&lt;MyType&gt; theSource = new ParquetFileSource(&quot;fs:///path/to/dir&quot;, AvroParquet.forSpecific(MyType.class));</div><div class="line"> </div><div class="line">// The returned stream will be a DataStream if theSource is unbounded.</div><div class="line">// After we add BoundedDataStream which extends DataStream, the returned stream will be a BoundedDataStream.</div><div class="line">// This allows users to write programs working in both batch and stream execution mode.</div><div class="line">DataStream&lt;MyType&gt; stream = env.source(theSource);</div><div class="line"> </div><div class="line">// Once we add bounded streams to the DataStream API, we will also add the following API.</div><div class="line">// The parameter has to be bounded otherwise an exception will be thrown.</div><div class="line">BoundedDataStream&lt;MyType&gt; batch = env.boundedSource(theSource);</div></pre></td></tr></table></figure><p>值得注意的是，<code>ExecutionEnvironment</code> 将根据 <code>Source#isBounded</code> 直接返回 DataStream 或者 BoundedDataStream，无需用户切换 Environmnet。</p><h2 id="独立出数据源的工作发现和分配"><a href="#独立出数据源的工作发现和分配" class="headerlink" title="独立出数据源的工作发现和分配"></a>独立出数据源的工作发现和分配</h2><p>在底层，<code>Source</code> 构造出的 <code>SplitEnumerator</code> 和 <code>SplitReader</code> 将分别负责发现和分配 Split 和 Split 的实际读取处理。相比之下，目前在 DataSet API 中的发现和分配 Split 由 JobManager 统一负责，而 DataStream 的对应工作则由 TaskManager 各自完成。</p><center><p><img src="/img/flip-27/img4.split-enumerator-reader.png" alt="图四. SplitEnumerator 和 SplitReader" title="图四. SplitEnumerator 和 SplitReader"></p></center><p><code>SplitEnumerator</code> 在作业启动时以单并行度运行，读取数据源元数据并构建 Split，按照分配策略将 Split 分配给 SplitReader，类似于现在 InputFormat 构造的 <code>SplitAssigner</code>，但不同点在于还要额外管理 DataStream 的管理工作，比如 Checkpoint 和 Watermark。<code>SplitEnumerator</code> 有三种现实方案: 运行在 JobManager 上（目前 <code>SplitAssigner</code> 的做法），或者以一个单并行度 Task 的方式运行在 TaskManager 上（类似目前 <code>ContinuousFileMonitoringFunction</code> 的做法），或者以一个新独立组件的方式运行。目前社区是比较偏向使用独立组件的方式，但未完全确定。感兴趣的读者可以研究下各种方案的优劣，相信可以从中学到不少东西。</p><p><code>SplitReader</code> 负责的工作则类似目前 DataStream 的 SourceFunction，不同点在于除了被动地接受 Split，<code>SplitReader</code> 还可以主动向 <code>SplitEnumerator</code> 请求 Split，这主要是满足批处理场景的需求。</p><p>通过这样的清晰分工，Source 的抽象性大大提升，新 Source 的开发和现有 Source 的迭代都更有规范可遵循，对用户来说也更容易理解。</p><h2 id="新增-Source-Subtask-间的通信机制"><a href="#新增-Source-Subtask-间的通信机制" class="headerlink" title="新增 Source Subtask 间的通信机制"></a>新增 Source Subtask 间的通信机制</h2><p>按照新架构，在运行期间 <code>SplitEnumerator</code> 和 <code>SplitReader</code> 不时会需要通信协作，比如分配新 Split 或 Event Time 对齐。这个通信将复用大部分现有的 JobManager 和 TaskManager 的 RPC 机制（基于 <code>SplitEnumerator</code> 以独立组件运行在 JobManager 端的方案），在这基础上加上 Operator 级别的协调者，比如上文提到的 Source 协调者。</p><center><p><img src="/img/flip-27/img5.source_component_rpc.png" alt="图五. Source 组件间的通信" title="图五. Source 组件间的通信"></p></center><p>其中 SourceEvent 是 <code>SplitEnumerator</code> 和 <code>SplitReader</code> 通信的消息，比如 <code>SplitEnumerator</code> 新分配 Split 或者 <code>SplitReader</code> 处理完已分配 Split 主动请求新 Split。而 OperatorEvent 则是更通用化的 Operator 协调者与 Operator 通信的消息。</p><h2 id="SplitReader-线程模型"><a href="#SplitReader-线程模型" class="headerlink" title="SplitReader 线程模型"></a>SplitReader 线程模型</h2><p>上文提到 Checkpoint Lock 是现在 Source 的瓶颈之一，这是因为 Checkpoint 和计算任务是由不同线程来执行，而新接口将遵循单线程的 actor/mailbox 模式，所以不再需要  Checkpoint Lock 来同步线程。</p><p>根据 FLIP-27 的设计，<code>SplitReader</code> 将调用外部存储系统客户端 API 读取数据，转换为目标数据类型后，push 到一个缓冲区（Buffer 或者 Queue），然后 Flink 内部的 Source Loop 线程再读取这个缓冲中的数据。</p><p>根据外部存储系统客户端的 API 调用方式（阻塞、非阻塞、异步）和 Flink 执行模式（流处理/批处理）的不同，Source 可以分为以下几种模式:</p><p>1) 单 Split 串行</p><center><p><img src="/img/flip-27/img6.single_split.png" alt="图六. 单 Split 串行" title="图六. 单 Split 串行"></p></center><p>这种模式通常符合批处理场景，比如 File Source、Database Source。工作流程是作业启动时 <code>SplitEnumerator</code> 会将 Split 分配到每个 <code>SplitReader</code> 的 Split Queue 中，然后 <code>SplitReader</code> 会逐一串行处理，并输出到 Buffer 供后续线程读取。</p><p>2）多 Split 多路复用</p><center><p><img src="/img/flip-27/img7.multi_split_multiplexed.png" alt="图七. 多 Split 多路复用" title="图七. 多 Split 多路复用"></p></center><p>多 Split 多路复用通常适用于流处理场景一个客户端可以处理多个 Split 的情况。典型的例子就是单个 Kafka Consumer 可以消费多个 Topic 的多个 Partition。工作流程是作业启动时 <code>SplitEnumerator</code> 会批量分配现有 Split 给 <code>SplitReader</code>，后者启动一个 IO 线程读取所有的 Split，处理后输出到 Buffer 或 Queue 供后续线程读取。</p><p>3) 多 Split 多线程</p><center><p><img src="/img/flip-27/img8.multi_split_multi_threaded.png" alt="图八. 多 Split 多线程" title="图八. 多 Split 多线程"></p></center><p>多 Split 多线程通常适用于流处理场景每个客户端只处理单个 Split 的情况，比如 Kinesis Consumer 会为每个 Kinesis Shard 单独起一个线程来读取数据。工作流程类似于 Kafka，不过每条 IO 线程都有单独的输出队列，这样下游可以选择性地读取某个 Shard 的数据，这对于 Event Time 对齐的特性十分重要。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>综上所述，目前的 Source 接口不符合流批的发展趋势，同时因为缺乏 Flink 引擎内置线程模型的支持，开发新的 Source 和为现有 Source 开发 Event Time 对齐等功能都十分不方便。为此 Flink 社区起草了 FLIP-27 来重构 Source 接口，核心是统一流批两种执行模式的 Source 架构，但底层的调度和算法则根据 Source 类型来判断。新接口的核心是 <code>SplitEnumerator</code> 和 <code>SplitReader</code>，前者负责发现和分配 Split、触发 Checkpoint 等管理工作， 后者负责 Split 的实际读取处理。此外，新增 Operator 间的通信机制，让 Source Subtask 之间可以协调完成 Event Time 对齐等新特性。最后，<code>SplitReader</code> 底层封装了通用的线程模型，相比目前的 <code>SourceFunction</code> 大大简化了 Source 的实现。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95653748" target="_blank" rel="external">FLIP-27: Refactor Source Interface</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95653748#FLIP-27:RefactorSourceInterface-where_run_enumerator" target="_blank" rel="external">Where To Run Enumerator</a></li><li><a href="https://lists.apache.org/thread.html/70484d6aa4b8e7121181ed8d5857a94bfb7d5a76334b9c8fcc59700c@%3Cdev.flink.apache.org%3E" target="_blank" rel="external">[DISCUSS] FLIP-27: Refactor Source Interface</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于大多数 Flink 应用开发者而言，无论使用高级的 Table API 或者是底层的 DataStream/DataSet API，Source 都是首先接触到且使用最多的 Operator 之一。然而其实从 2018 年 10 月开始，Flink 社区就开始计划重构这个稳定了多年的 Source 接口[1]，以满足更大规模数据以及对接更丰富的 connector 的要求，另外还有更重要的一个目的: 统一流批两种计算模式。重构后的 Source 接口在概念和使用方式上都会有较大不同，无论对 Flink 应用开发者还是 Flink 社区贡献者来说都是十分值得关注的，所以本文将从”为什么要这样设计”的角度来谈谈 Source 接口重构的前因后果。这会涉及到较多的底层架构内容，要求读者有一定的基础或者有探索的兴趣。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink DataStream 关联维表实战</title>
    <link href="https://link3280.github.io/2020/01/16/Flink-DataStream-%E5%85%B3%E8%81%94%E7%BB%B4%E8%A1%A8%E5%AE%9E%E6%88%98/"/>
    <id>https://link3280.github.io/2020/01/16/Flink-DataStream-关联维表实战/</id>
    <published>2020-01-16T12:24:09.000Z</published>
    <updated>2020-01-16T12:25:31.468Z</updated>
    
    <content type="html"><![CDATA[<p>上篇博客提到 Flink SQL 如何 Join 两个数据流，有读者反馈说如果不打算用 SQL 或者想自己实现底层操作，那么如何基于 DataStream API 来关联维表呢？实际上由于 Flink DataStream API 的灵活性，实现这个需求的方式是非常多样的，但是大部分用户很难在设计架构时就考虑得很全面，可能会走不少弯路。针对于此，笔者根据工作经验以及社区资源整理了用 DataStream 实现 Join 维表的常见方式，并给每种的方式优劣和适用场景给出一点可作为参考的个人观点。</p><a id="more"></a><h1 id="衡量指标"><a href="#衡量指标" class="headerlink" title="衡量指标"></a>衡量指标</h1><p>总体来讲，关联维表有三个基础的方式：实时数据库查找关联（Per-Record Reference Data Lookup）、预加载维表关联（Pre-Loading of Reference Data）和维表变更日志关联（Reference Data Change Stream），而根据实现上的优化可以衍生出多种关联方式，且这些优化还可以灵活组合产生不同效果（不过为了简单性这里不讨论同时应用多种优化的实现方式）。对于不同的关联方式，我们可以从以下 7 个关键指标来衡量（每个指标的得分将以 1-5 五档来表示）:</p><ol><li>实现简单性: 设计是否足够简单，易于迭代和维护。</li><li>吞吐量: 性能是否足够好。</li><li>维表数据的实时性: 维度表的更新是否可以立刻对作业可见。</li><li>数据库的负载: 是否对外部数据库造成较大的负载（负载越低分越高）。</li><li>内存资源占用: 是否需要大量内存来缓存维表数据（内存占用越少分越高）。</li><li>可拓展性: 在更大规模的数据下会不会出现瓶颈。</li><li>结果确定性: 在数据延迟或者数据重放情况下，是否可以得到一致的结果。</li></ol><p>和大多数架构设计一样，这三类关联方式不存在绝对的好坏，更多的是针对业务场景在各指标上的权衡取舍，因此这里的得分也仅仅是针对通用场景来说。</p><h1 id="实时数据库查找关联"><a href="#实时数据库查找关联" class="headerlink" title="实时数据库查找关联"></a>实时数据库查找关联</h1><p>实时数据库查找关联是在 DataStream API 用户函数中直接访问数据库来进行关联的方式。这种方式通常开发量最小，但一般会给数据库带来很大的压力，而且因为关联是基于 Processing Time 的，如果数据有延迟或者重放，会得到和原来不一致的数据。</p><h2 id="同步数据库查找关联"><a href="#同步数据库查找关联" class="headerlink" title="同步数据库查找关联"></a>同步数据库查找关联</h2><p>同步实时数据库查找关联是最为简单的关联方式，只需要在一个 Map 或者 FlatMap 函数中访问数据库，处理好关联逻辑后，将结果数据输出。</p><p><center><img src="/img/flink-datastream-join/img1.sync-db-lookup.png" alt="图1.同步数据库查找关联架构" title="图1.同步数据库查找关联架构"></center></p><p>这种方式的主要优点在于实现简单、不需要额外内存且维表的更新延迟很低，然而缺点也很明显: 1. 因为每条数据都需要请求一次数据库，给数据库造成的压力很大；2. 访问数据库是同步调用，导致 subtak 线程会被阻塞，影响吞吐量；3. 关联是基于 Processing Time 的，结果并不具有确定性；4. 瓶颈在数据库端，但实时计算的流量通常远大于普通数据库的设计流量，因此可拓展性比较低。</p><p><center><img src="/img/flink-datastream-join/img2.sync-db-lookup.png" alt="图2.同步数据库查找关联关键指标" title="图2.同步数据库查找关联关键指标"></center></p><p>从应用场景来说，同步数据库查找关联可以用于流量比较低的作业，但通常不是最好的选择。</p><h2 id="异步数据库查找关联"><a href="#异步数据库查找关联" class="headerlink" title="异步数据库查找关联"></a>异步数据库查找关联</h2><p>异步数据库查找关联是通过 AsyncIO[2]来访问外部数据库的方式。利用数据库提供的异步客户端，AsyncIO 可以并发地处理多个请求，很大程度上减少了对 subtask 线程的阻塞。</p><p>因为数据库请求响应时长是不确定的，可能导致后输入的数据反而先完成计算，所以 AsyncIO 提供有序和无序两种输出模式，前者会按请求返回顺序输出数据，后者则会缓存提前完成计算的数据，并按输入顺序逐个输出结果。</p><p><center><img src="/img/flink-datastream-join/img3.async-db-lookup.png" alt="图3.异步数据库查找关联架构" title="图3.异步数据库查找关联架构"></center></p><p>比起同步数据库查找关联，异步数据库查找关联稍微复杂一点，但是大部分的逻辑都由 Flink AsyncIO API 封装，因此总体来看还是比较简单。然而，有序输出模式下的 AsyncIO 会需要缓存数据，且这些数据会被写入 checkpoint，因此在内容资源方面的得分会低一点。另一方面，同步数据库查找关联的吞吐量问题得到解决，但仍不可避免地有数据库负载高和结果不确定两个问题。</p><p><center><img src="/img/flink-datastream-join/img4.async-db-lookup.png" alt="图4.异步数据库查找关联关键指标" title="图4.异步数据库查找关联关键指标"></center></p><p>从应用场景来说，异步数据库查找关联比较适合流量低的实时计算。</p><h2 id="带缓存的数据库查找关联"><a href="#带缓存的数据库查找关联" class="headerlink" title="带缓存的数据库查找关联"></a>带缓存的数据库查找关联</h2><p>为了解决上述两种关联方式对数据库造成太大压力的问题，可以引入一层缓存来减少直接对数据库的请求。缓存并一般不需要通过 checkpoint 机制持久化，因此简单地用一个 WeakHashMap 或者 Guava Cache 就可以实现。</p><p><center><img src="/img/flink-datastream-join/img5.cached-db-lookup.png" alt="图5.带缓存的数据库查找关联架构" title="图5.带缓存的数据库查找关联架构"></center></p><p>虽然在冷启动的时候仍会给数据库造成一定压力，但后续取决于缓存命中率，数据库的压力将得到一定程度的缓解。然而使用缓存带来的问题是维表的更新并不能及时反应到关联操作上，当然这也和缓存剔除的策略有关，需要根据维度表更新频率和业务对过时维表数据的容忍程度来设计。</p><p><center><img src="/img/flink-datastream-join/img6.cached-db-lookup.png" alt="图6.带缓存的数据库查找关联关键指标" title="图6.带缓存的数据库查找关联关键指标"></center></p><p>总而言之，带缓存的数据库查找关联适合于流量比较低，且对维表数据实时性要求不太高或维表更新比较少的业务场景。</p><h1 id="预加载维表关联"><a href="#预加载维表关联" class="headerlink" title="预加载维表关联"></a>预加载维表关联</h1><p>相比起实时数据库查找在运行期间为每条数据访问一次数据库，预加载维表关联是在作业启动时就将维表读到内存中，而在后续运行期间，每条数据都会和内存中的维表进行关联，而不会直接触发对数据的访问。与带缓存的实时数据库查找关联相比，区别是后者如果不命中缓存还可以 fallback 到数据库访问，而前者如果不名中则会关联不到数据。</p><h2 id="启动预加载维表"><a href="#启动预加载维表" class="headerlink" title="启动预加载维表"></a>启动预加载维表</h2><p>启动预加载维表是最为简单的一种方式，即在作业初始化的时候，比如用户函数的 <code>open()</code> 方法，直接从数据库将维表拷贝到内存中。维表并不需要用 State 来保存，因为无论是手动重启或者是 Flink 的错误重试机制导致的重启，<code>open()</code> 方法都会被执行，从而得到最新的维表数据。</p><p><center><img src="/img/flink-datastream-join/img7.startup-preloading.png" alt="图7.启动预加载维表架构" title="图7.启动预加载维表架构"></center></p><p>启动预加载维表对数据库的压力只持续很短时间，但因为是拷贝整个维表所以压力是很大的，而换来的优势是在运行期间不需要再访问数据库，可以提高效率，有点类似离线计算。相对地，问题在于运行期间维表数据不能更新，且对 TaskManager 内存的要求比较高。</p><p><center><img src="/img/flink-datastream-join/img8.startup-preloading.png" alt="图8.启动预加载维表关键指标" title="图8.启动预加载维表关键指标"></center></p><p>启动预加载维表适合于维表比较小、变更实时性要求不高的场景，比如根据 ip 库解析国家地区，如果 ip 库有新版本，重启作业即可。</p><h2 id="启动预加载分区维表"><a href="#启动预加载分区维表" class="headerlink" title="启动预加载分区维表"></a>启动预加载分区维表</h2><p>对于维表比较大的情况，可以启动预加载维表基础之上增加分区功能。简单来说就是将数据流按字段进行分区，然后每个 Subtask 只需要加在对应分区范围的维表数据。值得注意的是，这里的分区方式并不是用 keyby 这种通用的 hash 分区，而是需要根据业务数据定制化分区策略，然后调用 <code>DataStream#partitionCustom</code>。比如按照 <code>userId</code> 等区间划分，0-999 划分到 subtask 1，1000-1999 划分到 subtask 2，以此类推。而在 <code>open()</code> 方法中，我们再根据 subtask 的 id 和总并行度来计算应该加载的维表数据范围。</p><p><center><img src="/img/flink-datastream-join/img9.startup-partition-preloading.png" alt="图9.启动预加载分区维表架构" title="图9.启动预加载分区维表架构"></center></p><p>通过这种分区方式，维表的大小上限理论上可以线性拓展，解决了维表大小受限于单个 TaskManager 内存的问题（现在是取决于所有 TaskManager 的内存总量），但同时给带来设计和维护分区策略的复杂性。</p><p><center><img src="/img/flink-datastream-join/img10.startup-partition-preloading.png" alt="图10.启动预加载分区维表关键指标" title="图10.启动预加载分区维表关键指标"></center></p><p>总而言之，启动预加载分区维表适合维表比较大而变更实时性要求不高的场景，比如用户点击数据关联用户所在地。</p><h2 id="启动预加载维表并定时刷新"><a href="#启动预加载维表并定时刷新" class="headerlink" title="启动预加载维表并定时刷新"></a>启动预加载维表并定时刷新</h2><p>除了维表大小的限制，启动预加载维表的另一个主要问题在于维度数据的更新，我们可以通过引入定时刷新机制的办法来缓解这个问题。定时刷新可以通过 Flink ProcessFucntion 提供的 Timer 或者直接在 <code>open()</code> 初始化一个线程（池）来做这件事。不过 Timer 要求 KeyedStream，而上述的 <code>DataStream#partitionCustom</code> 并不会返回一个 KeyedStream，因此两者并不兼容。而如果使用额外线程定时刷新的办法则不受这个限制。</p><p><center><img src="/img/flink-datastream-join/img11.startup-preloading-refresh.png" alt="图11.启动预加载维表并定时刷新架构" title="图11.启动预加载维表并定时刷新架构"></center></p><p>比起基础的启动预加载维表 ，这种方式在于引入比较小复杂性的情况下大大缓解了的维度表更新问题，但也给维表数据库带来更多压力，因为每次 reload 的时候都是一次请求高峰。</p><p><center><img src="/img/flink-datastream-join/img12.startup-preloading-refresh.png" alt="图12.启动预加载维表并定时刷新关键指标" title="图12.启动预加载维表并定时刷新关键指标"></center></p><p>启动预加载维表和定时刷新的组合适合维表变更实时性要求不是特别高的场景。取决于定时刷新的频率和数据库的性能，这种方式可以满足大部分关联维表的业务。</p><h2 id="启动预加载维表-实时数据库查找"><a href="#启动预加载维表-实时数据库查找" class="headerlink" title="启动预加载维表 + 实时数据库查找"></a>启动预加载维表 + 实时数据库查找</h2><p>启动预加载维表还可以和实时数据库查找混合使用，即将预加载的维表作为缓存给实时关联时使用，若未名中则 fallback 到数据库查找。</p><p><center><img src="/img/flink-datastream-join/img13.startup-preloading-realtime-lookup.png" alt="图13.启动预加载维表结合实时数据库查找架构" title="图13.启动预加载维表结合实时数据库查找架构"></center></p><p>这种方式实际是带缓存的数据库查找关联的衍生，不同之处在于相比冷启动时未命中缓存导致的多次实时数据库访问，该方式直接批量拉取整个维表效率更高，但也有可能拉取到不会访问到的多余数据。下面雷达图中显示的是用异步数据库查找，如果是同步数据库查找吞吐量上会低一些。</p><p><center><img src="/img/flink-datastream-join/img14.startup-preloading-realtime-lookup.png" alt="图14.启动预加载维表结合实时数据库查找关键指标" title="图14.启动预加载维表结合实时数据库查找关键指标"></center></p><p>这种方式和带缓存的实时数据库查找关联基本相同，适合流量比较低，且对维表数据实时性要求不太高或维表更新比较少的业务场景。</p><h1 id="维表变更日志关联"><a href="#维表变更日志关联" class="headerlink" title="维表变更日志关联"></a>维表变更日志关联</h1><p>不同于上述两者将维表作为静态表关联的方式，维表变更日志关联将维表以 changelog 数据流的方式表示，从而将维表关联转变为两个数据流的 join。这里的 changelog 数据流类似于 MySQL 的 binlog，通常需要维表数据库端以 push 的方式将日志写到 Kafka 等消息队列中。Changelog 数据流称为 build 数据流，另外待关联的主要数据流成为 probe 数据流。</p><p>维表变更日志关联的好处在于可以获取某个 key 数据变化的时间，从而使得我们能在关联中使用 Event Time（当然也可以使用 Processing Time）。</p><h2 id="Processing-Time-维表变更日志关联"><a href="#Processing-Time-维表变更日志关联" class="headerlink" title="Processing Time 维表变更日志关联"></a>Processing Time 维表变更日志关联</h2><p>如果基于 Processing Time 做关联，我们可以利用 keyby 将两个数据流中关联字段值相同的数据划分到 KeyedCoProcessFunction 的同一个分区，然后用 ValueState 或者 MapState 将维表数据保存下来。在普通数据流的一条记录进到函数时，到 State 中查找有无符合条件的 join 对象，若有则关联输出结果，若无则根据 join 的类型决定是直接丢弃还是与空值关联。这里要注意的是，State 的大小要尽量控制好。首先是只保存每个 key 最新的维度数据值，其次是要给 State 设置好 TTL，让 Flink 可以自动清理。</p><p><center><img src="/img/flink-datastream-join/img15.processing-time-join.png" alt="图15.Processing Time 维表变更日志关联架构" title="图15.Processing Time 维表变更日志关联架构"></center></p><p>基于 Processing Time 的维表变更日志关联优点是不需要直接请求数据库，不会对数据库造成压力；缺点是比较复杂，相当于使用 changelog 在 Flink 应用端重新构建一个维表，会占用一定的 CPU 和比较多的内存和磁盘资源。值得注意的是，我们可以利用 Flink 提供的 RocksDB StateBackend，将大部分的维表数据存在磁盘而不是内存中，所以并不会占用很高的内存。不过基于 Processing Time 的这种关联对两个数据流的延迟要求比较高，否则如果其中一个数据流出现 lag 时，关联得到的结果可能并不是我们想要的，比如可能会关联到未来时间点的维表数据。</p><p><center><img src="/img/flink-datastream-join/img16.processing-time-join.png" alt="图16.Processing Time 维表变更日志关联关键指标" title="图16.Processing Time 维表变更日志关联关键指标"></center></p><p>基于 Processing Time 的维表变更日志关联比较适用于不便直接访问数据的场景（比如维表数据库是业务线上数据库，出于安全和负载的原因不能直接访问），或者对维表的变更实时性要求比较高的场景（但因为数据准确性的关系，一般用下文的 Event Time 关联会更好）。</p><h2 id="Event-Time-维表变更日志关联"><a href="#Event-Time-维表变更日志关联" class="headerlink" title="Event Time 维表变更日志关联"></a>Event Time 维表变更日志关联</h2><p>基于 Event Time 的维表关联实际上和基于 Processing Time 的十分相似，不同之处在于我们将维表 changelog 的多个时间版本都记录下来，然后每当一条记录进来，我们会找到对应时间版本的维表数据来和它关联，而不是总用最新版本，因此延迟数据的关联准确性大大提高。不过因为目前 State 并没有提供 Event Time 的 TTL，因此我们需要自己设计和实现 State 的清理策略，比如直接设置一个 Event Time Timer（但要注意 Timer 不能太多导致性能问题），再比如对于单个 key 只保存最近的 10 个版本，当有更新版本的维表数据到达时，要清理掉最老版本的数据。</p><p><center><img src="/img/flink-datastream-join/img17.event-time-join.png" alt="图17.Event Time 维表变更日志关联架构" title="图17.Event Time 维表变更日志关联架构"></center></p><p>基于 Event Time 的维表变更日志关联相对基于 Processing Time 的方式来说是一个改进，虽然多个维表版本导致空间资源要求更大，但确保准确性对于大多数场景来说都是十分重要的。相比 Processing Time 对两个数据的延迟都有要求，Event Time 要求 build 数据流的延迟低，否则可能一条数据到达时关联不到对应维表数据或者关联了一个过时版本的维表数据，</p><p><center><img src="/img/flink-datastream-join/img18.event-time-join.png" alt="图18.Event Time 维表变更日志关联关键指标" title="图18.Event Time 维表变更日志关联关键指标"></center></p><p>基于 Event Time 的维表变更日志关联比较适合于维表变更比较多且对变更实时性要求较高的场景 同时也适合于不便直接访问数据库的场景。</p><h2 id="Temporal-Table-Join"><a href="#Temporal-Table-Join" class="headerlink" title="Temporal Table Join"></a>Temporal Table Join</h2><p>Temporal Table Join 是 Flink SQL/Table API 的原生支持，它对两个数据流的输入都进行了缓存，因此比起上述的基于 Event Time 的维表变更日志关联，它可以容忍任意数据流的延迟，数据准确性更好。Temporal Table Join 在 SQL/Table API 使用时是十分简单的，但如果想在 DataStream API 中使用，则需要自己实现对应的逻辑。</p><p>总体思路是使用一个 CoProcessFunction，将 build 数据流以时间版本为 key 保存在 MapState 中（与基于 Event Time 的维表变更日志关联相同），再将 probe 数据流和输出结果也用 State 缓存起来（同样以 Event Time 为 key），一直等到 Watermark 提升到它们对应的 Event Time，才把结果输出和将两个数据流的输入清理掉。</p><p>这个 Watermark 触发很自然地是用 Event Time Timer 来实现，但要注意不要为每条数据都设置一遍 Timer，因为一旦 Watermark 提升会触发很多个 Timer 导致性能急剧下降。比较好的实践是为每个 key  只注册一个 Timer。实现上可以记录当前未处理的最早一个 Event Time，并用来注册 Timer。当前 Watermark。每当 Watermark 触发 Timer 时，我们检查处理掉未处理的最早 Event Time 到当前 Event Time 的所有数据，并将未处理的最早 Event Time 更新为当前时间。</p><p><center><img src="/img/flink-datastream-join/img19.temporal-table-join.png" alt="图19.Temporal Table Join 架构" title="图19.Temporal Table Join 架构"></center></p><p>Temporal Table Join 的好处在于对于两边数据流的延迟的容忍度较大，但作为代价会引入一定的输出结果的延迟，这也是基于 Watermark 机制的计算的常见问题，或者说，妥协。另外因为吞吐量较大的 probe 数据流也需要缓存，Flink 应用对空间资源的需求会大很多。最好，要注意的是如果维表变更太慢，导致 Watermark 提升太慢，会导致 probe 数据流被大量缓存，所以最好要确保 build 数据流尽量实时，同时给 Source 设置一个比较短的 idle timeout。</p><p><center><img src="/img/flink-datastream-join/img20.temporal-table-join.png" alt="图20.Temporal Table Join 关键指标" title="图20.Temporal Table Join 关键指标"></center></p><p>Temporal Table Join 这种方式最为复杂，但数据准确性最好，适合一些对数据准确性要求高且可以容忍一定延迟（一般分钟级别）的关键业务。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>用 Flink DataStream API 实现关联维表的方式十分丰富，可以直接访问数据库查找（实时数据库查找关联），可以启动时就将全量维表读到内存（预加载维表关联），也可以通过维表的 changelog 在 Flink 应用端实时构建一个新的维表（维表变更日志关联）。我们可以从实现简单性、吞吐量、维表数据的实时性、数据库的负载、内存资源占用、可拓展性和结果确定性这 7 个维度来衡量一个具体实现方式，并根据业务需求来选择最合适的实现。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.ververica.com/about/events-talks" target="_blank" rel="external">WEBINAR: 99 Ways to Enrich Streaming Data with Apache Flink</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/asyncio.html" target="_blank" rel="external">Asynchronous I/O for External Data Access</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇博客提到 Flink SQL 如何 Join 两个数据流，有读者反馈说如果不打算用 SQL 或者想自己实现底层操作，那么如何基于 DataStream API 来关联维表呢？实际上由于 Flink DataStream API 的灵活性，实现这个需求的方式是非常多样的，但是大部分用户很难在设计架构时就考虑得很全面，可能会走不少弯路。针对于此，笔者根据工作经验以及社区资源整理了用 DataStream 实现 Join 维表的常见方式，并给每种的方式优劣和适用场景给出一点可作为参考的个人观点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 如何实现数据流的 Join</title>
    <link href="https://link3280.github.io/2019/12/15/Flink-SQL-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84-Join/"/>
    <id>https://link3280.github.io/2019/12/15/Flink-SQL-如何实现数据流的-Join/</id>
    <published>2019-12-15T09:04:57.000Z</published>
    <updated>2019-12-17T13:34:58.666Z</updated>
    
    <content type="html"><![CDATA[<p>无论在 OLAP 还是 OLTP 领域，Join 都是业务常会涉及到且优化规则比较复杂的 SQL 语句。对于离线计算而言，经过数据库领域多年的积累 Join 的语义以及实现已经十分成熟，然而对于近年来刚兴起的 Streaming SQL 来说 Join 却处于刚起步的状态。其中最为关键的问题在于 Join 的实现依赖于缓存整个数据集，而 Streaming SQL Join 的对象却是无限的数据流，内存压力和计算效率在长期运行来说都是不可避免的问题。下文将结合 SQL 的发展解析 Flink SQL 是如何解决这些问题并实现两个数据流的 Join。</p><a id="more"></a><h1 id="离线-Batch-SQL-Join-的实现"><a href="#离线-Batch-SQL-Join-的实现" class="headerlink" title="离线 Batch SQL Join 的实现"></a>离线 Batch SQL Join 的实现</h1><p>传统的离线 Batch SQL （面向有界数据集的 SQL）有三种基础的实现方式，分别是 Nested-loop Join、Sort-Merge Join 和 Hash Join。</p><p>Nested-loop Join 最为简单直接，将两个数据集加载到内存，并用内嵌遍历的方式来逐个比较两个数据集内的元素是否符合 Join 条件。Nested-loop Join 虽然时间效率以及空间效率都是最低的，但胜在比较灵活适用范围广，因此其变体 BNL 常被传统数据库用作为 Join 的默认基础选项。</p><p>Sort-Merge Join 顾名思义，分为两个 Sort 和 Merge 阶段。首先将两个数据集进行分别排序，然后对两个有序数据集分别进行遍历和匹配，类似于归并排序的合并。值得注意的是，Sort-Merge 只适用于 Equi-Join（Join 条件均使用等于作为比较算子）。Sort-Merge Join 要求对两个数据集进行排序，成本很高，通常作为输入本就是有序数据集的情况下的优化方案。</p><p>Hash Join 同样分为两个阶段，首先将一个数据集转换为 Hash Table，然后遍历另外一个数据集元素并与 Hash Table 内的元素进行匹配。第一阶段和第一个数据集分别称为 build 阶段和 build table，第二个阶段和第二个数据集分别称为 probe 阶段和 probe table。Hash Join 效率较高但对空间要求较大，通常是作为 Join 其中一个表为适合放入内存的小表的情况下的优化方案。和 Sort-Merge Join 类似，Hash Join 也只适用于 Equi-Join。</p><h1 id="实时-Streaming-SQL-Join"><a href="#实时-Streaming-SQL-Join" class="headerlink" title="实时 Streaming SQL Join"></a>实时 Streaming SQL Join</h1><p>相对于离线的 Join，实时 Streaming SQL（面向无界数据集的 SQL）无法缓存所有数据，因此 Sort-Merge Join 要求的对数据集进行排序基本是无法做到的，而 Nested-loop Join 和 Hash Join 经过一定的改良则可以满足实时 SQL 的要求。</p><p>我们通过例子来看基本的 Nested Join 在实时 Streaming SQL 的基础实现（案例及图来自 Piotr Nowojski 在 Flink Forward San Francisco 的分享[2]）。</p><p><center><p><img src="/img/streaming-join/img1.join-in-continuous-query-1.png" alt="图1. Join-in-continuous-query-1" title="图1. Join-in-continuous-query-1"></p></center></p><p></p><p>Table A 有 <code>1</code>、<code>42</code> 两个元素，Table B 有 <code>42</code> 一个元素，所以此时的 Join 结果会输出 42。</p><p><center><p><img src="/img/streaming-join/img2.join-in-continuous-query-2.png" alt="图2. Join-in-continuous-query-2" title="图2. Join-in-continuous-query-2"></p></center></p><p></p><p>接着 Table B 依次接受到三个新的元素，分别是 <code>7</code>、<code>3</code>、<code>1</code>。因为 <code>1</code> 匹配到 Table A 的元素，因此结果表再输出一个元素 <code>1</code>。</p><p><center><p><img src="/img/streaming-join/img3.join-in-continuous-query-3.png" alt="图3. Join-in-continuous-query-3" title="图3. Join-in-continuous-query-3"></p></center></p><p></p><p>随后 Table A 出现新的输入 <code>2</code>、<code>3</code>、<code>6</code>，<code>3</code> 匹配到 Table B 的元素，因此再输出 <code>3</code> 到结果表。</p><p>可以看到在 Nested-Loop Join 中我们需要保存两个输入表的内容，而随着时间的增长 Table A 和 Table B 需要保存的历史数据无止境地增长，导致很不合理的内存磁盘资源占用，而且单个元素的匹配效率也会越来越低。类似的问题也存在于 Hash Join 中。</p><p>那么有没有可能设置一个缓存剔除策略，将不必要的历史数据及时清理呢？答案是肯定的，关键在于缓存剔除策略如何实现，这也是 Flink SQL 提供的三种 Join 的主要区别。</p><h1 id="Flink-SQL-的-Join"><a href="#Flink-SQL-的-Join" class="headerlink" title="Flink SQL 的 Join"></a>Flink SQL 的 Join</h1><h2 id="Regular-Join"><a href="#Regular-Join" class="headerlink" title="Regular Join"></a>Regular Join</h2><p>Regular Join 是最为基础的没有缓存剔除策略的 Join。Regular Join 中两个表的输入和更新都会对全局可见，影响之后所有的 Join 结果。举例，在一个如下的 Join 查询里，Orders 表的新纪录会和 Product 表所有历史纪录以及未来的纪录进行匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">SELECT * FROM Orders</div><div class="line">INNER JOIN Product</div><div class="line">ON Orders.productId = Product.id</div></pre></td></tr></table></figure><p>因为历史数据不会被清理，所以 Regular Join 允许对输入表进行任意种类的更新操作（insert、update、delete）。然而因为资源问题 Regular Join 通常是不可持续的，一般只用做有界数据流的 Join。</p><h2 id="Time-Windowed-Join"><a href="#Time-Windowed-Join" class="headerlink" title="Time-Windowed Join"></a>Time-Windowed Join</h2><p>Time-Windowed Join 利用窗口的给两个输入表设定一个 Join 的时间界限，超出时间范围的数据则对 JOIN 不可见并可以被清理掉。值得注意的是，这里涉及到的一个问题是时间的语义，时间可以是指计算发生的系统时间（即 Processing Time），也可以是指从数据本身的时间字段提取的 Event Time。如果是 Processing Time，Flink 根据系统时间自动划分 Join 的时间窗口并定时清理数据；如果是 Event Time，Flink 分配 Event Time 窗口并依据 Watermark 来清理数据。</p><p>以更常用的 Event Time Windowed Join 为例，一个将 Orders 订单表和 Shipments 运输单表依据订单时间和运输时间 Join 的查询如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">SELECT *</div><div class="line">FROM </div><div class="line">Orders o, </div><div class="line">Shipments s</div><div class="line">WHERE </div><div class="line">o.id = s.orderId AND</div><div class="line">s.shiptime BETWEEN o.ordertime AND o.ordertime + INTERVAL &apos;4&apos; HOUR</div></pre></td></tr></table></figure><p>这个查询会为 Orders 表设置了 <code>o.ordertime &gt; s.shiptime- INTERVAL &#39;4&#39;HOUR</code> 的时间下界（图4），</p><p><center><p><img src="/img/streaming-join/img4.time-window-orders-lower-bound.png" alt="图4. Time-Windowed Join 的时间下界 - Orders 表" title="图4. Time-Windowed Join 的时间下界 - Orders 表"></p></center></p><p></p><p>并为 Shipmenets 表设置了 <code>s.shiptime &gt;= o.ordertime</code> 的时间下界（图5）。</p><p><center><p><img src="/img/streaming-join/img5.time-window-shipment-lower-bound.png" alt="图5. Time-Windowed Join 的时间下界 - Shipment 表" title="图5. Time-Windowed Join 的时间下界 - Shipment 表"></p></center></p><p></p><p>因此两个输入表都只需要缓存在时间下界以上的数据，将空间占用维持在合理的范围。</p><p>不过虽然底层实现上没有问题，但如何通过 SQL 语法定义时间仍是难点。尽管在实时计算领域 Event Time、Processing Time、Watermark 这些概念已经成为业界共识，但在 SQL 领域对时间数据类型的支持仍比较弱[4]。因此，定义 Watermark 和时间语义都需要通过编程 API 的方式完成，比如从 DataStream 转换至 Table 时定义，而不能单纯靠 SQL 完成。这方面的支持 Flink 社区计划通过拓展 SQL 方言来完成，感兴趣的读者可以通过 FLIP-66[7] 来追踪进度。</p><h2 id="Temporal-Table-Join"><a href="#Temporal-Table-Join" class="headerlink" title="Temporal Table Join"></a>Temporal Table Join</h2><p>虽然 Timed-Windowed Join 解决了资源问题，但也限制了使用场景: Join 两个输入流都必须有时间下界，超过之后则不可访问。这对于很多 Join 维表的业务来说是不适用的，因为很多情况下维表并没有时间界限。针对这个问题，Flink 提供了 Temporal Table Join 来满足用户需求。</p><p>Temporal Table Join 类似于 Hash Join，将输入分为 Build Table 和 Probe Table。前者一般是纬度表的 changelog，后者一般是业务数据流，典型情况下后者的数据量应该远大于前者。在 Temporal Table Join 中，Build Table 是一个基于 append-only 数据流的带时间版本的视图，所以又称为 Temporal Table。Temporal Table 要求定义一个主键和用于版本化的字段（通常就是 Event Time 时间字段），以反映记录内容在不同时间的内容。</p><p>比如典型的一个例子是对商业订单金额进行汇率转换。假设有一个 Oders 流记录订单金额，需要和 RatesHistory 汇率流进行 Join。RatesHistory 代表不同货币转为日元的汇率，每当汇率有变化时就会有一条更新记录。两个表在某一时间节点内容如下:</p><p><center><p><img src="/img/streaming-join/img6.temporal-table-join-example.png" alt="图6. Temporal Table Join Example" title="图6. Temporal Table Join Example]"></p></center></p><p></p><p>我们将 RatesHistory 注册为一个名为 Rates 的 Temporal Table，设定主键为 currency，版本字段为 time。</p><p><center><p><img src="/img/streaming-join/img7.temporal-table-registration.png" alt="图7. Temporal Table Registration" title="图7. Temporal Table Registration]"></p></center></p><p></p><p>此后给 Rates 指定时间版本，Rates 则会基于 RatesHistory 来计算符合时间版本的汇率转换内容。</p><p><center><p><img src="/img/streaming-join/img8.temporal-table-content.png" alt="图8. Temporal Table Content" title="图8. Temporal Table Content]"></p></center></p><p></p><p>在 Rates 的帮助下，我们可以将业务逻辑用以下的查询来表达:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">SELECT </div><div class="line">o.amount * r.rate</div><div class="line">FROM</div><div class="line">Orders o,</div><div class="line">LATERAL Table(Rates(o.time)) r</div><div class="line">WHERE</div><div class="line">o.currency = r.currency</div></pre></td></tr></table></figure><p>值得注意的是，不同于在 Regular Join 和 Time-Windowed Join 中两个表是平等的，任意一个表的新记录都可以与另一表的历史记录进行匹配，在 Temporal Table Join 中，Temoparal Table 的更新对另一表在该时间节点以前的记录是不可见的。这意味着我们只需要保存 Build Side 的记录直到 Watermark 超过记录的版本字段。因为 Probe Side 的输入理论上不会再有早于 Watermark 的记录，这些版本的数据可以安全地被清理掉。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实时领域 Streaming SQL 中的 Join 与离线 Batch SQL 中的 Join 最大不同点在于无法缓存完整数据集，而是要给缓存设定基于时间的清理条件以限制 Join 涉及的数据范围。根据清理策略的不同，Flink SQL 分别提供了 Regular Join、Time-Windowed Join 和 Temporal Table Join 来应对不同业务场景。</p><p>另外，尽管在实时计算领域 Join 可以灵活地用底层编程 API 来实现，但在 Streaming SQL 中 Join 的发展仍处于比较初级的阶段，其中关键点在于如何将时间属性合适地融入 SQL 中，这点 ISO SQL 委员会制定的 SQL 标准并没有给出完整的答案。或者从另外一个角度来讲，作为 Streaming SQL 最早的开拓者之一，Flink 社区很适合探索出一套合理的 SQL 语法反过来贡献给 ISO。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://flink.apache.org/2019/05/14/temporal-tables.html" target="_blank" rel="external">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li><li><a href="https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-how-to-join-two-data-streams-piotr-nowojski" target="_blank" rel="external">How to Join Two Data Streams? - Piotr Nowojski</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/streaming/joins.html#joins-in-continuous-queries" target="_blank" rel="external">Joins in Continuous Queries</a></li><li><a href="https://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf" target="_blank" rel="external">Temporal features in SQL:2011</a></li><li><a href="https://mysqlserverteam.com/hash-join-in-mysql-8/" target="_blank" rel="external">Hash join in MySQL 8</a></li><li><a href="https://en.wikipedia.org/wiki/SQL:2011" target="_blank" rel="external">SQL:2011</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-66%3A+Support+Time+Attribute+in+SQL+DDL" target="_blank" rel="external">FLIP-66: Support Time Attribute in SQL DDL</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无论在 OLAP 还是 OLTP 领域，Join 都是业务常会涉及到且优化规则比较复杂的 SQL 语句。对于离线计算而言，经过数据库领域多年的积累 Join 的语义以及实现已经十分成熟，然而对于近年来刚兴起的 Streaming SQL 来说 Join 却处于刚起步的状态。其中最为关键的问题在于 Join 的实现依赖于缓存整个数据集，而 Streaming SQL Join 的对象却是无限的数据流，内存压力和计算效率在长期运行来说都是不可避免的问题。下文将结合 SQL 的发展解析 Flink SQL 是如何解决这些问题并实现两个数据流的 Join。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
      <category term="SQL" scheme="https://link3280.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>如何分析及处理 Flink 反压</title>
    <link href="https://link3280.github.io/2019/11/03/Flink-%E5%8F%8D%E5%8E%8B%E5%88%86%E6%9E%90%E5%8F%8A%E5%A4%84%E7%90%86/"/>
    <id>https://link3280.github.io/2019/11/03/Flink-反压分析及处理/</id>
    <published>2019-11-03T11:49:21.000Z</published>
    <updated>2019-11-03T12:06:04.979Z</updated>
    
    <content type="html"><![CDATA[<p>反压（backpressure）是实时计算应用开发中，特别是流式计算中，十分常见的问题。反压意味着数据管道中某个节点成为瓶颈，处理速率跟不上上游发送数据的速率，而需要对上游进行限速。由于实时计算应用通常使用消息队列来进行生产端和消费端的解耦，消费端数据源是 pull-based 的，所以反压通常是从某个节点传导至数据源并降低数据源（比如 Kafka consumer）的摄入速率。</p><a id="more"></a><p>关于 Flink 的反压机制，网上已经有不少博客介绍，中文博客推荐这两篇[1][2]。简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。而本文将着重结合官方的博客[4]分享笔者在实践中分析和处理 Flink 反压的经验。</p><h2 id="反压的影响"><a href="#反压的影响" class="headerlink" title="反压的影响"></a>反压的影响</h2><p>反压并不会直接影响作业的可用性，它表明作业处于亚健康的状态，有潜在的性能瓶颈并可能导致更大的数据处理延迟。通常来说，对于一些对延迟要求不太高或者数据量比较小的应用来说，反压的影响可能并不明显，然而对于规模比较大的 Flink 作业来说反压可能会导致严重的问题。</p><p>这是因为 Flink 的 checkpoint 机制，反压还会影响到两项指标: checkpoint 时长和 state 大小。前者是因为 checkpoint barrier 是不会越过普通数据的，数据处理被阻塞也会导致 checkpoint barrier 流经整个数据管道的时长变长，因而 checkpoint 总体时间（End to End Duration）变长。后者是因为为保证 EOS（Exactly-Once-Semantics，准确一次），对于有两个以上输入管道的 Operator，checkpoint barrier 需要对齐（Alignment），接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到state 里面，导致 checkpoint 变大。这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint 超时失败，而 state 大小同样可能拖慢 checkpoint 甚至导致 OOM （使用 Heap-based StateBackend）或者物理内存使用超出容器资源（使用 RocksDBStateBackend）的稳定性问题。因此，我们在生产中要尽量避免出现反压的情况（顺带一提，为了缓解反压给 checkpoint 造成的压力，社区提出了 FLIP-76: Unaligned Checkpoints[4] 来解耦反压和 checkpoint）。</p><h2 id="定位反压节点"><a href="#定位反压节点" class="headerlink" title="定位反压节点"></a>定位反压节点</h2><p>要解决反压首先要做的是定位到造成反压的节点，这主要有两种办法: 1.通过 Flink Web UI 自带的反压监控面板；2.通过 Flink Task Metrics。前者比较容易上手，适合简单分析，后者则提供了更加丰富的信息，适合用于监控系统。因为反压会向上游传导，这两种方式都要求我们从 Source 节点到 Sink 的逐一排查，直到找到造成反压的根源原因[4]。下面分别介绍这两种办法。</p><h3 id="反压监控面板"><a href="#反压监控面板" class="headerlink" title="反压监控面板"></a>反压监控面板</h3><p>Flink Web UI 的反压监控提供了 SubTask 级别的反压监控，原理是通过周期性对 Task 线程的栈信息采样，得到线程被阻塞在请求 Buffer（意味着被下游队列阻塞）的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1 以下则为 <code>OK</code>，0.1 至 0.5 为 <code>LOW</code>，而超过 0.5 则为 <code>HIGH</code>。</p><center><p><img src="/img/flink-backpressure-handling/back-pressure-sampling-high.png" alt="图1. Flink 1.8 的 Web UI 反压面板" title="图1. Flink 1.8 的 Web UI 反压面板(来自官方博客)"></p></center><p>如果处于反压状态，那么有两种可能性：</p><ol><li>该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。</li><li>下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。</li></ol><p>如果是第一种状况，那么该节点则为反压的根源节点，它是从 Source Task 到 Sink Task 的第一个出现反压的节点。如果是第二种情况，则需要继续排查下游节点。值得注意的是，反压的根源节点并不一定会在反压面板体现出高反压，因为反压面板监控的是发送端，如果某个节点是性能瓶颈并不会导致它本身出现高反压，而是导致它的上游出现高反压。总体来看，如果我们找到第一个出现反压的节点，那么反压根源要么是就这个节点，要么是它紧接着的下游节点。</p><p>那么如果区分这两种状态呢？很遗憾只通过反压面板是无法直接判断的，我们还需要结合 Metrics 或者其他监控手段来定位。此外如果作业的节点数很多或者并行度很大，由于要采集所有 Task 的栈信息，反压面板的压力也会很大甚至不可用。</p><h3 id="Task-Metrics"><a href="#Task-Metrics" class="headerlink" title="Task Metrics"></a>Task Metrics</h3><p>Flink 提供的 Task Metrics 是更好的反压监控手段，但也要求更加丰富的背景知识。首先我们简单回顾下 Flink 1.5 以后的网路栈，熟悉的读者可以直接跳过。</p><p>TaskManager 传输数据时，不同的 TaskManager 上的两个 Subtask 间通常根据 key 的数量有多个 Channel，这些 Channel 会复用同一个 TaskManager 级别的 TCP 链接，并且共享接收端 Subtask 级别的 Buffer Pool。在接收端，每个 Channl 在初始阶段会被分配固定数量的 Exclusive Buffer，这些 Buffer 会被用于存储接受到的数据，交给 Operator 使用后再次被释放。Channel 接收端空闲的 Buffer 数量称为 Credit，Credit 会被定时同步给发送端被后者用于决定发送多少个 Buffer 的数据。在流量较大时，Channel 的 Exclusive Buffer 可能会被写满，此时 Flink 会向 Buffer Pool 申请剩余的 Floating Buffer。这些 Floating Buffer 属于备用 Buffer，哪个 Channel 需要就去哪里。而在 Channel 发送端，一个 Subtask 所有的 Channel 会共享同一个 Buffer Pool，这边就没有区分 Exclusive Buffer 和 Floating Buffer。</p><center><p><img src="/img/flink-backpressure-handling/credit-based-network.png" alt="图2. Flink Credit-Based 网络" title="图2. Flink Credit-Based 网络"></p></center><p>我们在监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为有用的是以下几个 Metrics:</p><table><thead><tr><th>Metris</th><th>描述</th></tr></thead><tbody><tr><td>outPoolUsage</td><td>发送端 Buffer 的使用率</td></tr><tr><td>inPoolUsage</td><td>接收端 Buffer 的使用率</td></tr><tr><td>floatingBuffersUsage（1.9 以上）</td><td>接收端 Floating Buffer 的使用率</td></tr><tr><td>exclusiveBuffersUsage （1.9 以上）</td><td>接收端 Exclusive Buffer 的使用率</td></tr></tbody></table><p>其中 inPoolUsage 等于 floatingBuffersUsage 与 exclusiveBuffersUsage 的总和。</p><p>分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer 占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(图片来自官网):</p><center><p><img src="/img/flink-backpressure-handling/1.8-backpressure-table.png" alt="图3. 反压分析表" title="图3. 反压分析表"></p></center><p>outPoolUsage 和 inPoolUsage 同为低或同为高分别表明当前 Subtask 正常或处于被下游反压，这应该没有太多疑问。而比较有趣的是当 outPoolUsage 和 inPoolUsage 表现不同时，这可能是出于反压传导的中间状态或者表明该 Subtask 就是反压的根源。如果一个 Subtask 的 outPoolUsage 是高，通常是被下游 Task 所影响，所以可以排查它本身是反压根源的可能性。如果一个 Subtask 的 outPoolUsage 是低，但其 inPoolUsage 是高，则表明它有可能是反压的根源。因为通常反压会传导至其上游，导致上游某些 Subtask 的 outPoolUsage 为高，我们可以根据这点来进一步判断。值得注意的是，反压有时是短暂的且影响不大，比如来自某个 Channel 的短暂网络延迟或者 TaskManager 的正常 GC，这种情况下我们可以不用处理。</p><p>对于 Flink 1.9 及以上版本，除了上述的表格，我们还可以根据 floatingBuffersUsage/exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游 Subtask 的数据传输。</p><center><p><img src="/img/flink-backpressure-handling/1.9-backpressure-table.png" alt="图4. Flink 1.9 反压分析表" title="图4. Flink 1.9 反压分析表"></p></center><p>通常来说，floatingBuffersUsage 为高则表明反压正在传导至上游，而 exclusiveBuffersUsage 则表明了反压是否存在倾斜（floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数 channel 占用了大部分的 Floating Buffer）。</p><p>至此，我们已经有比较丰富的手段定位反压的根源是出现在哪个节点，但是具体的原因还没有办法找到。另外基于网络的反压 metrics 并不能定位到具体的 Operator，只能定位到 Task。特别是那种 embarrassingly parallel（易并行）的作业（所有的 Operator 会被放入一个 Task，因此只有一个节点），反压 metrics 则排不上用场。</p><h2 id="分析具体原因及处理"><a href="#分析具体原因及处理" class="headerlink" title="分析具体原因及处理"></a>分析具体原因及处理</h2><p>定位到反压节点后，分析造成原因的办法和我们分析一个普通程序的性能瓶颈的办法是十分类似的，可能还要更简单一点，因为我们要观察的主要是 Task Thread。</p><p>在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI 各个 SubTask 的 Records Sent 和 Record Received 来确认，另外 Checkpoint detail 里不同 SubTask 的 State size 也是一个分析数据倾斜的有用指标。</p><p>此外，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题）。最有用的办法就是对 TaskManager 进行 CPU profile，从中我们可以分析到 Task Thread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面，比如我们生产环境中就偶尔遇到卡在 Regex 的用户函数（ReDoS）；如果不是的话要看 Task Thread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是 checkpoint 或者 GC 等系统活动导致的暂时系统暂停。当然，性能分析的结果也可能是正常的，只是作业申请的资源不足而导致了反压，这就通常要求拓展并行度。值得一提的，在未来的版本 Flink 将会直接在 WebUI 提供 JVM 的 CPU 火焰图[5]，这将大大简化性能瓶颈的分析。</p><p>另外 TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManager JVM 各区内存不合理导致的频繁 Full GC 甚至失联。推荐可以通过给 TaskManager 启用 G1 垃圾回收器来优化 GC，并加上 <code>-XX:+PrintGCDetails</code> 来打印 GC 日志的方式来观察 GC 的问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>反压是 Flink 应用运维中常见的问题，它不仅意味着性能瓶颈还可能导致作业的不稳定性。定位反压可以从 Web UI 的反压监控面板和 Task Metric 两者入手，前者方便简单分析，后者适合深入挖掘。定位到反压节点后我们可以通过数据分布、CPU Profile 和 GC 指标日志等手段来进一步分析反压背后的具体原因并进行针对性的优化。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1.<a href="http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/" target="_blank" rel="external">Flink 原理与实现：如何处理反压问题</a><br>2.<a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1571628927&amp;ver=1925&amp;signature=cHpaczGLH6QninlmHmM0mDKbb2-fuTMw83YjIFQFa7iN3omCrdlL51zCKo7N0n1uwM7*9JL-DmsQXhR*1Uh0YiUpVLHEzklFN9KUK33PVeF2fnoXcr0cDPPZ2s8HmK-D&amp;new=1" target="_blank" rel="external">一文彻底搞懂 Flink 网络流控与反压机制</a><br>3.<a href="http://www.whitewood.me/2018/05/13/Flink-%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%BC%82%E6%AD%A5%E5%BF%AB%E7%85%A7-ABS-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" target="_blank" rel="external">Flink 轻量级异步快照 ABS 实现原理</a><br>4.<a href="https://flink.apache.org/2019/07/23/flink-network-stack-2.html" target="_blank" rel="external">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a><br>5.<a href="https://issues.apache.org/jira/browse/FLINK-13550" target="_blank" rel="external">Support for CPU FlameGraphs in new web UI</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;反压（backpressure）是实时计算应用开发中，特别是流式计算中，十分常见的问题。反压意味着数据管道中某个节点成为瓶颈，处理速率跟不上上游发送数据的速率，而需要对上游进行限速。由于实时计算应用通常使用消息队列来进行生产端和消费端的解耦，消费端数据源是 pull-based 的，所以反压通常是从某个节点传导至数据源并降低数据源（比如 Kafka consumer）的摄入速率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.10 细粒度资源管理解析</title>
    <link href="https://link3280.github.io/2019/10/17/Flink-1-10-%E7%BB%86%E7%B2%92%E5%BA%A6%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>https://link3280.github.io/2019/10/17/Flink-1-10-细粒度资源管理解析/</id>
    <published>2019-10-17T12:44:50.000Z</published>
    <updated>2019-10-17T13:05:59.466Z</updated>
    
    <content type="html"><![CDATA[<p>相信不少读者在开发 Flink 应用时或多或少会遇到在内存调优方面的问题，比如在我们生产环境中遇到最多的 TaskManager 在容器化环境下占用超出容器限制的内存而被 YARN/Mesos kill 掉[1]，再比如使用 heap-based StateBackend 情况下 State 过大导致 GC 频繁影响吞吐。这些问题对于不熟悉 Flink 内存管理的用户来说十分难以排查，而且 Flink 晦涩难懂的内存配置参数更是让用户望而却步，结果是往往将内存调大至一个比较浪费的阈值以尽量避免内存问题。</p><a id="more"></a><p>对于作业规模不大的普通用户而言，这些通常在可以接受的范围之内，但对于上千并行度的大作业来说，浪费资源的总量会非常可观，而且进程的不稳定性导致的作业恢复时间也会比普通作业长得多，因此阿里巴巴的 Blink 团队针对内存管理机制做了大量的优化，并于近期开始合并到 Flink。本文的内容主要基于阿里团队工程师宋辛童在 Flink Forward Beijing 的分享[1]，以及后续相关的几个 FLIP 提案。</p><h1 id="Flink-目前（1-9）的内存管理"><a href="#Flink-目前（1-9）的内存管理" class="headerlink" title="Flink 目前（1.9）的内存管理"></a>Flink 目前（1.9）的内存管理</h1><p>TaskManager 作为 Master/Slave 架构中的 Slave 提供了作业执行需要的环境和资源，最为重要而且复杂，因此 Flink 的内存管理也主要指 TaskManager 的内存管理。</p><p>TaskManager 的资源（主要是内存）分为三个层级，分别是最粗粒度的进程级（TaskManager 进程本身），线程级（TaskManager 的 slot）和 SubTask 级（多个 SubTask 共用一个 slot）。</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-hierachy.png" alt="图1.TaskManager 资源层级" title="图1.TaskManager 资源层级"></p></center><p>在进程级，TaskManager 将内存划分为以下几块:</p><ul><li>Heap Memory: 由 JVM 直接管理的 heap 内存，留给用户代码以及没有显式内存管理的 Flink 系统活动使用（比如 StateBackend、ResourceManager 的元数据管理等）。</li><li>Network Memory: 用于网络传输（比如 shuffle、broadcast）的内存 Buffer 池，属于 Direct Memory 并由 Flink 管理。</li><li>Cutoff Memory: 在容器化环境下进程使用的物理内存有上限，需要预留一部分内存给 JVM 本身，比如线程栈内存、class 等元数据内存、GC 内存等。</li><li>Managed Memory: 由 Flink Memory Manager 直接管理的内存，是数据在 Operator 内部的物理表示。Managed Memory 可以被配置为 on-heap 或者 off-heap (direct memory)的，off-heap 的 Managed Memory 将有效减小 JVM heap 的大小并减轻 GC 负担。目前 Managed Memory 只用于 Batch 类型的作业，需要缓存数据的操作比如 hash join、sort 等都依赖于它。</li></ul><p>根据 Managed Memory 是 on-heap 或 off-heap 的不同，TaskManager 的进程内存与 JVM 内存分区关系分别如下:</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-partitions.png" alt="图2.TaskManager 内存分区" title="图2.TaskManager 内存分区"></p></center><p>在线程级别，TaskManager 会将其资源均分为若干个 slot (在 YARN/Mesos/K8s 环境通常是每个 TaskManager 只包含 1 个 slot)，没有 slot sharing 的情况下每个 slot 可以运行一个 SubTask 线程。除了 Managed Memory，属于同一 TaskManager 的 slot 之间基本是没有资源隔离的，包括 Heap Memory、Network Buffer、Cutoff Memory 都是共享的。所以目前 slot 主要的用处是限制一个 TaskManager 的 SubTask 数。</p><p>从作为资源提供者的 TaskManager 角度看， slot 是资源的最小单位，但从使用者 SubTask 的角度看，slot 的资源还可以被细分，因为 Flink 的 slot sharing 机制。默认情况下， Flink 允许多个 SubTask 共用一个 slot 的资源，前提是这些 SubTask 属于同一个 Job 的不同 Task。以官网的例子来说，一个拓扑为 <code>Source(6)-map(6)-keyby/window/apply(6)-sink(1)</code> 的作业，可以运行在 2 个 slot 数为 3 的 TaskManager 上（见图3）。</p><center><p><img src="/img/flink-new-mem-management/slot-sharing.png" alt="图3.TaskManager Slot Sharing" title="图3.TaskManager Slot Sharing"></p></center><p>这样的好处是，原本一共需要 19 个 slot 的作业，现在只需要作业中与 Task 最大并行度相等的 slot， 即 6 个 slot 即可运行起来。此外因为不同 Task 通常有不同的资源需求，比如 source 主要使用网络 IO，而 map 可能主要需要 cpu，将不同 Task 的 subtask 放到同一 slot 中有利于资源的充分利用。</p><p>可以看到，目前 Flink 的内存管理是比较粗粒度的，资源隔离并不是很完整，而且在不同部署模式下（Standalone/YARN/Mesos/K8s）或不同计算模式下（Streaming/Batch）的内存分配也不太一致，为深度平台化及大规模应用增添了难度。</p><h1 id="Flink-1-10-细粒度的资源管理"><a href="#Flink-1-10-细粒度的资源管理" class="headerlink" title="Flink 1.10 细粒度的资源管理"></a>Flink 1.10 细粒度的资源管理</h1><p>为了改进 Flink 内存管理机制，阿里巴巴的工程师结合 Blink 的优化经验分别就进程、线程、SubTask（Operator）三个层面分别提出了 3 个 FLIP，均以 1.10 为目标 release 版本。下面将逐一介绍每个提案的内容。</p><h2 id="FLIP-49-统一-TaskExecutor-的内存配置"><a href="#FLIP-49-统一-TaskExecutor-的内存配置" class="headerlink" title="FLIP-49: 统一 TaskExecutor 的内存配置"></a>FLIP-49: 统一 TaskExecutor 的内存配置</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>TaskExecutor 在不同部署模式下具体负责作业执行的进程，可以简单视为 TaskManager。目前 TaskManager 的内存配置存在不一致以及不够直观的问题，具体有以下几点:</p><ul><li>流批作业内容配置不一致。Managed Memory 只覆盖 DataSet API，而 DataStream API 的则主要使用 JVM 的 heap 内存，相比前者需要更多的调优参数且内存消耗更难把控。</li><li>RocksDB 占用的 native 内存并不在内存管理里，导致使用 RocksDB 时内存需要很多手动调优。</li><li>不同部署模式下，Flink 内存计算算法不同，并且令人难以理解。</li></ul><p>针对这些问题，FLIP-49[4] 提议通过将 Managed Memory 的用途拓展至 DataStream 以解决这个问题。DataStream 中主要占用内存的是 StateBackend，它可以从管理 Managed Memory 的 MemoryManager 预留部分内存或分配内存。通过这种方式同一个 Flink 配置可以运行 Batch 作业和 Streaming 作业，有利于流批统一。</p><h3 id="改进思路"><a href="#改进思路" class="headerlink" title="改进思路"></a>改进思路</h3><p>总结一下，目前 DataStream 和 DataSet 的内存使用可以分为如下几类:</p><table><thead><tr><th>场景</th><th>内存类型</th><th>内存分配方式</th><th>内存限制</th></tr></thead><tbody><tr><td>Streaming(Heap-based StateBackend)</td><td>heap</td><td>隐式分配</td><td>小于 JVM heap size</td></tr><tr><td>Streaming(RocksDB StateBackend)</td><td>off-heap</td><td>隐式分配</td><td>只受限于机器内存</td></tr><tr><td>Batch</td><td>heap 或 off-heap</td><td>显式通过 MemoryManager 分配</td><td>不大于显式分配的内存数</td></tr></tbody></table><p>可以看到目前 DataStream 作业的内存分配没有经过 MemoryManager 而是直接向 JVM 申请，容易造成 heap OOM 或者物理内存占用过大[3]，因此直接的修复办法是让 MemoryManager 了解到 StateBackend 的内存占用。这会有两种方式，一是直接通过 MemoryManager 申请内存，二是仍使用隐式分配的办法，但需要通知 MemoryManager 预留这部分内存。此外 MemoryManager 申请 off-heap 的方式也会有所变化，从 <code>ByteBuffer#allocateDirect()</code> 变为 <code>Unsafe#allocateMemory()</code>，这样的好处是显式管理的 off-heap 内存可以从 JVM 的 <code>-XX:MaxDirectMemorySize</code> 参数限制中分离出来。</p><p>另外 MemoryManager 将不只可以被配置为 heap/off-heap，而是分别拥有对应的内存池。这样的好处是在同一个集群可以运行要求不同类型内存的作业，比如一个 FsStateBackend 的 DataStream 作业和一个 RocksDBStateBackend 的 DataStream 作业。heap/off-heap 的比例可以通过参数配置，1/0 则代表了完全的 on-heap 或者 off-heap。</p><p>改进之后 TaskManager 的各内存分区如下:</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-partitions.png" alt="图4.TaskManager 新内存结构" title="TaskManager 新内存结构"></p></center><table><thead><tr><th>分区</th><th>内存类型</th><th>描述</th><th>配置项</th><th>默认值</th></tr></thead><tbody><tr><td>Framework Heap Memory</td><td>heap</td><td>Flink 框架消耗的 heap 内存</td><td>taskmanager.memory.<br>framework.heap</td><td>128mb</td></tr><tr><td>Task Heap Memory</td><td>heap</td><td>用户代码使用的 heap 内存</td><td>taskmanager.memory.<br>task.heap</td><td>无</td></tr><tr><td>Task Off-Heap Memory</td><td>off-heap</td><td>用户代码使用的 off-heap 内存</td><td>taskmanager.memory.<br>task.offheap</td><td>0b</td></tr><tr><td>Shuffle Memory</td><td>off-heap</td><td>网络传输/suffle 使用的内存</td><td>taskmanager.memory.<br>shuffle.[min/max/fraction]</td><td>min=64mb, max=1gb, fraction=0.1</td></tr><tr><td>Managed Heap Memory</td><td>heap</td><td>Managed Memory 使用的 heap 内存</td><td>taskmanager.memory.<br>managed.[size/fraction]</td><td>fraction=0.5</td></tr><tr><td>Managed Off-heap Memory</td><td>off-heap</td><td>Managed Memory 使用的 off-heap 内存</td><td>taskmanager.memory.<br>managed.offheap-fraction</td><td>0.0</td></tr><tr><td>JVM Metaspace</td><td>off-heap</td><td>JVM metaspace 使用的 off-heap 内存</td><td>taskmanager.memory.jvm-metaspace</td><td>192mb</td></tr><tr><td>JVM Overhead</td><td>off-heap</td><td>JVM 本身使用的内存</td><td>taskmanager.memory.jvm-overhead.[min/max/fraction]</td><td>min=128mb, max=1gb, fraction=0.1)</td></tr><tr><td>Total Flink Memory</td><td>heap &amp; off-heap</td><td>Flink 框架使用的总内存，是以上除 JVM Metaspace 和 JVM Overhead 以外所有分区的总和</td><td>taskmanager.memory.total-flink.size</td><td>无</td></tr><tr><td>Total Process Memory</td><td>heap &amp; off-heap</td><td>进程使用的总内存，是所有分区的总和，包括 JVM Metaspace 和 JVM Overhead</td><td>taskmanager.memory.total-process.size</td><td>无</td></tr></tbody></table><p>值得注意的是有 3 个分区是没有默认值的，包括 Framework Heap Memory、Total Flink Memory 和 Total Process Memory，它们是决定总内存的最关键参数，三者分别满足不同部署模式的需要。比如在 Standalone 默认下，用户可以配置 Framework Heap Memory 来限制用户代码使用的 heap 内存；而在 YARN 部署模式下，用户可以通过配置 YARN container 的资源来间接设置 Total Process Memory。</p><h2 id="FLIP-56-动态-slot-分配"><a href="#FLIP-56-动态-slot-分配" class="headerlink" title="FLIP-56: 动态 slot 分配"></a>FLIP-56: 动态 slot 分配</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p>目前 Flink 的资源是预先静态分配的，也就是说 TaskManager 进程启动后 slot 的数目和每个 slot 的资源数都是固定的而且不能改变，这些 slot 的生命周期和 TaskManager 是相同的。Flink Job 后续只能向 TaskManager 申请和释放这些 slot，而没有对 slot 资源数的话语权。</p><center><p><img src="/img/flink-new-mem-management/static-slot.png" alt="图5. 静态 slot 分配" title="图5. 静态 slot 分配"></p></center><p>这种粗粒度的资源分配假定每个 SubTask 的资源需求都是大致相等的，优点是较为简单易用，缺点在于如果出现 SubTask 的资源需求有倾斜的情况，用户则需要按其中某个 SubTask 最大资源来配置总体资源，导致资源浪费且不利于多个作业复用相同 Flink 集群。</p><h3 id="改进思路-1"><a href="#改进思路-1" class="headerlink" title="改进思路"></a>改进思路</h3><p>FLIP-56[5] 提议通过将 TaskManager 的资源改为动态申请来解决这个问题。TaskManager 启动的时候只需要确定资源池大小，然后在有具体的 Flink Job 申请资源时再按需动态分配 slot。Flink Job 申请 slot 时需要附上资源需求，TaskManager 会根据该需求来确定 slot 资源。</p><center><p><img src="/img/flink-new-mem-management/dynamic-slot.png" alt="图6. 动态 slot 分配" title="图6. 动态 slot 分配"></p></center><p>值得注意的是，slot 资源需求可以是 <code>unknown</code>。提案引入了一个新的默认 slot 资源要求配置项，它表示一个 slot 占总资源的比例。如果 slot 资源未知，TaskManager 将按照该比例切分出 slot 资源。为了保持和现有静态 slot 模型的兼容性，如果该配置项没有被配置，TaskManager 会根据 slot 数目均等分资源生成 slot。</p><p>目前而言，该 FLIP 主要涉及到 Managed Memory 资源，TaskManager 的其他资源比如 JVM heap 还是多个 slot 共享的。</p><h2 id="FLIP-53-细粒度的算子资源管理"><a href="#FLIP-53-细粒度的算子资源管理" class="headerlink" title="FLIP-53: 细粒度的算子资源管理"></a>FLIP-53: 细粒度的算子资源管理</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p>FLIP-56 使得 slot 的资源可以根据实际需求确定，而 FLIP-53 则探讨了 Operator (算子)层面如何表达资源需求，以及如何根据不同 Operator 的设置来计算出总的 slot 资源。</p><p>目前 DataSet API 以及有可以指定 Operator 资源占比的方法（TaskConfig 和 ChainedDriver），因此这个 FLIP 只涉及到 DataStream API 和 Table/SQL API (先在 Blink Planner 实现)。不过提案并没有包括用户函数 API 上的变化（类似新增 <code>dataStream.setResourceSpec()</code> 函数），而是主要讨论 DataStream 到 StreamGraph 的翻译过程如何计算 slot 资源。改进完成后，这三个 API 的资源计算逻辑在底层会是统一的。</p><h3 id="改进思路-2"><a href="#改进思路-2" class="headerlink" title="改进思路"></a>改进思路</h3><p>要理解 Flink 内部如何划分资源，首先要对 Flink 如何编译用户代码并部署到分布式环境的过程有一定的了解。</p><center><p><img src="/img/flink-new-mem-management/flink-graph.jpg" alt="图7. Flink 作业编译部署流程" title="图7. Flink 作业编译部署流程"></p></center><p>以 DataStream API 为例，用户为 DataStream 新增 Operator 时，Flink 在底层会将以一个对应的 Transform 来封装。比如 <code>dataStream.map(new MyMapFunc())</code> 会新增一个 <code>OneInputTransformation</code> 实例，里面包括了序列化的 <code>MyMapFunc</code> 实例，以及 Operator 的配置（包括名称、uid、并行度和资源等），并且记录了它在拓扑中的前一个 Transformation 作为它的数据输入。</p><p>当 <code>env.execute()</code> 被调用时，在 client 端 StreamGraphGenerator 首先会遍历 Transformation 列表构造出 StreamGraph 对象（每个 Operator 对应一个 StreamNode），然后 StreamingJobGraphGenerator 再将 StreamGraph 翻译成 DataStream/DataSet/Table/SQL 通用的 JobGraph（此时会应用 chaining policy 将可以合并的 Operator 合并为 OperatorChain，每个 OperatorChain 或不能合并的 Operator 对应一个 JobVertex），并将其传给 JobManager。</p><p>JobManager 收到 JobGraph 后首先会将其翻译成表示运行状态的 ExecutionGraph，ExecutionGraph 的每个节点称为 ExecutionJobVertex，对应一个 JobVertex。ExecutionJobVertex 有一个或多个并行度且可能被调度和执行多次，其中一个并行度的一次执行称为 Execution，JobManager 的 Scheduler 会为每个 Execution 分配 slot。</p><p>细粒度的算子资源管理将以下面的方式作用于目前的流程:</p><ol><li>用户使用 API 构建的 Operator（以 Transformation 表示）会附带 <code>ResourceSpecs</code>，描述该 Operator 需要的资源，默认为 <code>unknown</code>。</li><li>当生成 JobGraph 的时候，StreamingJobGraphGenerator 根据 <code>ResourceSpecs</code> 计算出每个 Operator 占的资源比例（主要是 Managed Memory 的比例）。</li><li>进行调度的时候，Operator 的资源将被加总成为 Task 的 <code>ResourceProfiles</code> （包括 Managed Memory 和根据 Task 总资源算出的 Network Memory）。这些 Task 会被划分为 SubTask 实例被部署到 TaskManager 上。</li><li>当 TaskManager 启动 SubTask    的时候，会根据各 Operator 的资源占比划分 Slot Managed Memory。划分的方式可以是用户指定每个 Operator 的资源占比，或者默认均等分。</li></ol><p>值得注意的是，Scheduler 的调度有分 EAGER 模式和 LAZY_FROM_SOURCE 两种模式，分别用于 Stream 作业和 Batch 作业，它们会影响到 slot 的资源计算。Stream 类型的作业要求所有的 Operator 同时运行，因此资源的需求是急切的（EAGER）；而 Batch 类型的作业可以划分为多个阶段，不同阶段的 Operator 不需要同时运行，可以等输入数据准备好了再分配资源（LAZY_FROM_SOURCE）。这样的差异导致如果要充分利用 slot，Batch 作业需要区分不同阶段的 Task，同一时间只考虑一个阶段的 Task 资源。</p><p>解决的方案是将 slot sharing 的机制拓展至 Batch 作业。默认情况下 Stream 作业的所有 Operator 都属于 default sharing group，所以全部 Operator 都能共用都一个 slot。对于 Batch 作业而言，我们将整个 JobGraph 根据 suffle 划分为一至多个 Region，每个 Region 属于独立的 sharing group，因而不会被放到同一个 slot 里面。</p><center><p><img src="/img/flink-new-mem-management/slot-sharing-group.png" alt="图8. 不同作业类型的 Slot Sharing Group" title="图8. 不同作业类型的 Slot Sharing Group"></p></center><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>随着 Flink 的越来越大规模地被应用于各种业务，目前资源管理机制的灵活性、易用性不足的问题越发凸显，新的细粒度资源管理机制将大大缓解这个问题。此外，新资源管理机制将统一流批两者在 runtime 层资源管理，这也为将最终的流批统一打下基础。对于普通用户而言，这里的大多数变动是透明的，主要的影响应该是出现新的内存相关的配置项需要了解一下。</p><p>1.<a href="https://issues.apache.org/jira/browse/FLINK-13477" target="_blank" rel="external">[FLINK-13477] Containerized TaskManager killed because of lack of memory overhead</a><br>2.<a href="https://www.bilibili.com/video/av68914405/?p=3" target="_blank" rel="external">机遇与挑战：Apache Flink 资源管理机制解读与展望</a><br>3.<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="external">[FLINK-7289] Memory allocation of RocksDB can be problematic in container environments</a><br>4.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors?src=contextnavpagetreemode" target="_blank" rel="external">FLIP-49: Unified Memory Configuration for TaskExecutors</a><br>5.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-56%3A+Dynamic+Slot+Allocation" target="_blank" rel="external">FLIP-56: Dynamic Slot Allocation</a><br>6.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-53%3A+Fine+Grained+Operator+Resource+Management" target="_blank" rel="external">FLIP-53: Fine Grained Operator Resource Management</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;相信不少读者在开发 Flink 应用时或多或少会遇到在内存调优方面的问题，比如在我们生产环境中遇到最多的 TaskManager 在容器化环境下占用超出容器限制的内存而被 YARN/Mesos kill 掉[1]，再比如使用 heap-based StateBackend 情况下 State 过大导致 GC 频繁影响吞吐。这些问题对于不熟悉 Flink 内存管理的用户来说十分难以排查，而且 Flink 晦涩难懂的内存配置参数更是让用户望而却步，结果是往往将内存调大至一个比较浪费的阈值以尽量避免内存问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.9 Release 解读</title>
    <link href="https://link3280.github.io/2019/09/21/Flink-1-9-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2019/09/21/Flink-1-9-Release-解读/</id>
    <published>2019-09-21T03:00:41.000Z</published>
    <updated>2019-09-21T03:02:57.160Z</updated>
    
    <content type="html"><![CDATA[<p>距离上个发行版近 4 个月后，不久前 Apache Flink 发行了 1.9 系列的首个版本。Flink 1.9 是个有重要意义的版本，它初步合并了 Blink 的大部分新特性（虽然是预览特性），其中包括 Blink planner、Hive 集成、Python Table API 和新版 Web UI。此外，1.9 版本正式引入了 Savepoint Processor 来提供离线访问和修改 State 的能力，这也是社区呼声比较高的一个特性。下文将选取一些笔者认为比较重要的特性、improvement 和 bugfix 进行解读（主要集中在实时场景），详细的变动进参考 [1]。</p><a id="more"></a><h1 id="Blink-Planner"><a href="#Blink-Planner" class="headerlink" title="Blink Planner"></a>Blink Planner</h1><p>在阿里巴巴内部，Table API 和 SQL API 是开发 Flink 应用使用得最多的 API，因此阿里巴巴也花费了大量的精力在这两个 API 的优化上，其中最重要的一个便是 Blink Planner。Planner 是 SQL/Table API 和 runtime 的桥梁，它负责将 SQL/Table API 翻译为物理执行计划，也就是 runtime 的 operator。</p><p><center><img src="/img/flink-1.9-release/planner-architecture.jpeg" alt="图一. Planner 架构" title="图一. Planner 架构"></center></p><p>比起 Flink 原生的 Planner，Blink Planner 主要有以下的优势: </p><ol><li>流批统一。无论是 Stream 作业还是 Batch 作业都会直接被翻译为 StreamGraph，也就是常说的将批处理作为流处理的特例。</li><li>解耦 SQL/Table API 和 DataStream/DataSet API。这与流批统一是紧密联系的，因为 DataStream/DataSet 两者是独立的，基于它们很难建立简洁的流批统一架构。</li><li>建立统一的 Table/SQL 入口，简化当前多个 *Environment 的复杂 API 设计。</li><li>更多的 SQL 优化规则，比如 Join 的谓词下推和多余的 aggregate 移除。</li></ol><p>Blink Planner 作为预览功能在 1.9 版本发布，用户可以通过引入依赖:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;1.9.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line"></div><div class="line">&lt;dependency&gt;</div><div class="line">&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;1.9.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure><p>并在 main 函数中设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">EnvironmentSettings.useBlinkPlanner();</div></pre></td></tr></table></figure><p>来启用 Blink Planner。但值得注意的是目前 Blink Planner 仍有些未解决的 issue，比如不能在同一个 TableEnvironment 执行多条 SQL 语句，所以并不推荐在生产中使用。</p><h1 id="SQL-DDL"><a href="#SQL-DDL" class="headerlink" title="SQL DDL"></a>SQL DDL</h1><p>在以往版本，Flink SQL 只提供了 DSL 和 DML，而缺少了 DDL，这意味着我们不能在 SQL Client 或者其他 Flink SQL 程序中持久化创建的表。这个问题在 1.9 版本得到改善。1.9 支持了可用于 Batch 场景的标准 SQL DDL [2]，但因为 Stream 场景的 DDL 要求定义时间属性（Time Characteristic）、 Watermark 算法和 append mode 等额外的参数，需要进一步考虑实现细节，则预计在 1.10 版本再支持。尽管两种 DDL 要求的信息略有不同，但最终的目标是提供统一的语法，换句话说用户不需要区分定义的 Table 是基于无边界还有有边界的数据集，Flink 会自动根据上下文来判断。</p><h1 id="Hive-集成"><a href="#Hive-集成" class="headerlink" title="Hive 集成"></a>Hive 集成</h1><p>目前 Flink 主要作为实时计算引擎，在与离线数据仓库组件 Hive 的集成方面做得并不足够，但随着流批统一的大趋势，Flink 在批处理方面的潜力也会逐渐被挖掘，比如在阿里巴巴 Flink 已经被引用于大部分的批处理场景。在与 Hive 的集成方面，我们可以看到另一个主流分布式计算引擎 Apache Spark 做得很好，而 Flink 的 Hive 集成也会主要参考 SparkSQL 的特性，其中比较重要的比如打通 Flink Table/SQL API 和 Hive Metasstaore、支持 Flink 作为 Hive 的计算引擎。</p><p>整体工作会分为三个步骤来完成[3]：</p><ol><li><p>Flink SQL 基础集成<br>这包括在 Flink SQL API 集成 Hive 的基本功能，比如通过 Hive Connector 读写 Hive 表、支持 Hive 的数据类型（Timestamp/String 等）、可以在 Flink SQL 中使用 Hive 的内置函数和在 Flink SQL 中支持 Hive 的 DDL/DML 操作。这部分工作在 1.9 版本基本得到实现，主要的功能通过 HiveCatalog 封装的形式暴露给用户。顺带一提，为了支持 HiveCatalog，原本 Table API 的 Catalag 接口也进行了大量的重构。</p></li><li><p>Hive 兼容性<br>这部分可以概括为充分利用 Hive 的高级特性，其中包括完整的 Hive 数据类型支持、Thrift Server（类似于 Spark SQL Thrift Server）、在 Beeline 中支持 Flink 作为计算引擎、JDBC/ODBC 驱动支持、支持多种 Hive SerDe 等等。这部分的内容较多，将在后续版本逐步实现。</p></li><li><p>Flink SQL 优化<br>SQL 优化是 SQL 计算引擎老生常谈的课题，在 Hive on Flink SQL 上，可预见的优化工作当然也不少。这里的内容可以细分为三个方面：SQL 优化规则、Query metric（包括资源消耗、执行时间等）以及 Flink runtime 的 Task 调度和容错。</p></li></ol><p>总而言之，Flink 和 Hive 集成将极大地提升 Flink SQL 在 Batch 场景的应用能力，同时随着流批统一，尤其是 SQL 的流批统一，Streaming SQL 也将从中受益。</p><h1 id="终止-暂停作业"><a href="#终止-暂停作业" class="headerlink" title="终止/暂停作业"></a>终止/暂停作业</h1><p>目前 Flink 提供了 <code>cancel-with-savepoint</code> 的选项以方便用户在停止作业时持久化作业状态，在底层它会分为两步: 1. 触发 Savepoint 快照；2. Savepoint 完成后 cancel 作业。这会主要带来三个数据一致性上的问题。</p><p>首先，Flink 依靠两步提交（Tow-Phase Commit）来确保 Exactly-Once，简单来说 Operator 收到 checkpoint barrier 时进行 State 快照，等全部 Operator 都完成快照后再统一由 JobManager 通知 commit。这里的 commit 是一个 best-efford 的操作，不保证每次成功，而是依靠失败状况下的重试保证最终成功（eventually succeed）。如果一次 checkpoint 顺利完成，但某个 Operator commit 时失败了，Flink 作业会重启并从这次成功的 checkpoint 恢复，恢复完毕后再次触发 JobManager 的 checkpoint 完成通知。问题在于在 <code>cancel-with-savepoint</code> 场景，savepoint 完成之后 Task 立刻被取消，这很可能发生在 Operator 进行 commit 之前，导致 commit 被跳过。</p><p>其次，checkpoint/savepoint 过程并不阻止 source 摄入数据，在 <code>cancel-with-savepoint</code> 取消作业时作业通常会多处理一部分数据，如果使用 At-Least-Once 的 sink 则会造成数据的重复。</p><p>最后，在 event-time 窗口统计的业务场景下，窗口数据的输出依赖于 watermark 的提升，如果用户希望在停止作业时输出目前的结果，比如作业准备下线需要保存最新的计算结果，则无法简单地做到这点，也就是说停止作业时总是得不到最新数据。</p><p>FLIP-34[4] 通过重构作业停止的流程设计解决了以上的问题。第一个问题根源在于 cancel 命令应该在 Task commit 之后再执行；第二个问题根源在于 Savepoint 设计上不能阻止消费，因为 Savepoint 可能被简单用于保存某个时间点的状态，之后并不一定会 cancel 作业；第三个问题类似于第二个问题，没有区分是要临时停止还是完全下线。</p><p>针对第二、第三点，FLIP-34 引入 <code>TERMINATE</code> 和 <code>SUSPEND</code> 两种停止作业的方式，前者表示完全下线希望全部提交中间状态，后者表示临时下线，比如维护升级等。两者都会触发 Source 发出 EOS (End of Stream)的信号，令 Task 变为 <code>FINISHED</code> 状态（目前停止作业后 Task 是 <code>CANCELED</code> 状态）。此外 <code>TERMINATE</code> 停止还会令 Source 额外发出一个 LONG 最大值的 Watermak，这会强制触发所有基于 event time 的操作，比如 event time 的窗口统计。</p><p><center><img src="/img/flink-1.9-release/new-stop-properties.png" alt="图二. TERMINATE/SUSPEND 总览" title="图二. TERMINATE/SUSPEND 总览"></center></p><p>针对第一点，Savepoint 由默认的异步执行改为同步执行，因此 Task 的 commit 会阻塞其他操作，包括 cancel 命令，保证 Task 的 commit 总是能保证被执行。</p><p>重构之后停止作业的流程如下:</p><ol><li>JobManager 从 Source 端开始触发同步的 Savepoint（包括 <code>TERMINATE</code> 或 <code>SUSPEND</code>）。</li><li>如果是 <code>TERMINATE</code> 停止，Source 会额外发出 MAX_WATERMARK。</li><li>TaskManager 收到 Savepoint barrier 之后执行同步的 Savepoint 快照，这会阻塞数据处理以及其他控制命令，直到快照结束。</li><li>TaskManager 向 JobManager 确认 Savepoint 成功。</li><li>JobManager     确认 Savepoint 完成并通知 TaskManager 进行第二阶段的 commit。</li><li>TaskManager 进行 commit，并移除阻塞状态。</li><li>Source 发出 EOS 信号，接受到信号的 TaskManager 依次关闭 Task。</li><li>JobManager 会在等待所有 Task 和 Job 变为 FINISHED 后关闭。</li></ol><p>在 CLI 端，Flink 新增了 <code>stop</code> 命令，其用法示例如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/flink stop [-p targetDirectory] [-d] &lt;jobID&gt;</div></pre></td></tr></table></figure><p>其中 <code>-p</code> 指定 Savepoint 的存储路径，<code>-d</code> 表示 <code>TERMINATE</code> 类型 stop。</p><h1 id="重构-WebUI"><a href="#重构-WebUI" class="headerlink" title="重构 WebUI"></a>重构 WebUI</h1><p>1.9 版本合并了 Blink 的新 WebUI。新的 WebUI 从风格来说更加明亮简洁，交互也更加友好。此外之前版本的 WebUI 对日志的支持比较糟糕，是直接拉取机器上的日志文件展示在页面上，这在日志比较大的时候很卡并容易导致标签页崩溃。</p><p>新版的 WebUI 提供分页并有一个类似 IDE 的全局缩略图，可以很容易定位到某个位置（似乎就是通过 VS Code 的库，所以也附送了语法高亮的特性）。</p><p><center><img src="/img/flink-1.9-release/blink-webui-logs.png" alt="图三. 新 WebUI 日志页面" title="图三. 新 WebUI 日志页面"></center></p><p>此外在 JobGraph 上也提供了更多的监控信息，比如 InQueue 和 OutQueue 被暴露到 Operator 上，方面用户排查作业瓶颈。</p><p><center><img src="/img/flink-1.9-release/blink-webui-operators.png" alt="图四. 新 WebUI Operators" title="图四. 新 WebUI Operators"></center></p><h1 id="Failover-策略"><a href="#Failover-策略" class="headerlink" title="Failover 策略"></a>Failover 策略</h1><p>在 1.9 以前 Flink 遇到 Task 错误的默认行为是重启整个 Job，在作业比较大的情况下可能会带来很高的 downtime 成本。针对这个问题 FLIP1[5] 提出了细粒度的容错机制，提供 RestartRegion 的 Failover 策略，使得只有与错误 Task 有数据联系的 Task 会被重启。RestartRegion 其实并不是在 1.9 版本才用，但在之前存在一个严重的 bug 导致使用 RestartRegion 并不会恢复作业状态，因此应用范围很有限。1.9 版本修复了这个问题，并且将 RestartRegion 设为默认的策略。</p><h1 id="Runtime-稳定性"><a href="#Runtime-稳定性" class="headerlink" title="Runtime 稳定性"></a>Runtime 稳定性</h1><p>1.9 版本修复了几个 Flink runtime 比较严重的 bug，将在这里统一整理。</p><h2 id="Per-job-集群在-job-失败后没有自动退出"><a href="#Per-job-集群在-job-失败后没有自动退出" class="headerlink" title="Per-job 集群在 job 失败后没有自动退出"></a>Per-job 集群在 job 失败后没有自动退出</h2><p>相信不少 Flink 用户都遇到过的一个问题是以 detached 模式，即 per-job cluster，运行作业时，作业失败后有一定几率出现 Yarn application 仍没有退出，变成类似一个没有作业的 session cluster。根据 FLINK-12219[6] 这个问题在于 JobManager 在退出先需要将作业的归档信息持久化（给 HistoryServer 用），但这个过程没有异常处理，如果出现出错将导致 JobManager 不执行关闭的命令。</p><h2 id="单-Task-包含多个-Stateful-Operator-时-RocksDB-StateBackend-会丢失数据"><a href="#单-Task-包含多个-Stateful-Operator-时-RocksDB-StateBackend-会丢失数据" class="headerlink" title="单 Task 包含多个 Stateful Operator 时 RocksDB StateBackend 会丢失数据"></a>单 Task 包含多个 Stateful Operator 时 RocksDB StateBackend 会丢失数据</h2><p>这个问题在于 RocksDBStateBackend 使用的本地快照目录以 VertexID 而不是 Operator 作为生成目录的参数，当多个 Stateful Operator 被 chained 到一起时它们的本地快照目录会冲突，此时 RocksDB 会前一个 Operator 的状态会被后一个覆盖，导致状态丢失。FLINK-12296[7] 通过重构 RocksDB 本地快照目录的生成规则来解决了这个问题。</p><h2 id="已取消-Checkpoint-可能造成作业失败"><a href="#已取消-Checkpoint-可能造成作业失败" class="headerlink" title="已取消 Checkpoint 可能造成作业失败"></a>已取消 Checkpoint 可能造成作业失败</h2><p>当一个 checkpoint 被取消时，其 checkpoint 目录（比如常见的 HDFS 目录）会被删除，但 Task 本地仍有可能会访问这个被删除的目录，此时会抛出 <code>org.apache.hadoop.ipc.RemoteException: java.io.IOException: Path doesn&#39;t exist</code> 的异常，导致作业失败。FLINK-11662[8] 通过忽略已取消的 checkpoint 抛出的异常来修复了这个问题。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12344601" target="_blank" rel="external">Flink 1.9 Release Changelog</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-6962" target="_blank" rel="external">[FLINK-6962] Add a create table SQL DDL</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10556" target="_blank" rel="external">[FLINK-10556] Integration with Apache Hive</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212" target="_blank" rel="external">FLIP-34: Terminate/Suspend Job with Savepoint</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="external">FLIP-1 : Fine Grained Recovery from Task Failures</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-12219" target="_blank" rel="external">[FLINK-12219] Yarn application can’t stop when flink job failed in per-job yarn cluster mode</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-12296" target="_blank" rel="external">[FLINK-12296] Data loss silently in RocksDBStateBackend when more than one operator(has states) chained in a single task</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11662" target="_blank" rel="external">[FLINK-11662] Discarded checkpoint can cause Tasks to fail</a></li><li><a href="https://ververica.cn/developers/flink1-9-hive/" target="_blank" rel="external">如何在 Flink 1.9 中使用 Hive？</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;距离上个发行版近 4 个月后，不久前 Apache Flink 发行了 1.9 系列的首个版本。Flink 1.9 是个有重要意义的版本，它初步合并了 Blink 的大部分新特性（虽然是预览特性），其中包括 Blink planner、Hive 集成、Python Table API 和新版 Web UI。此外，1.9 版本正式引入了 Savepoint Processor 来提供离线访问和修改 State 的能力，这也是社区呼声比较高的一个特性。下文将选取一些笔者认为比较重要的特性、improvement 和 bugfix 进行解读（主要集中在实时场景），详细的变动进参考 [1]。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Flink 容错机制</title>
    <link href="https://link3280.github.io/2019/07/28/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Flink-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
    <id>https://link3280.github.io/2019/07/28/深入理解-Flink-容错机制/</id>
    <published>2019-07-28T01:26:55.000Z</published>
    <updated>2019-07-29T13:28:56.119Z</updated>
    
    <content type="html"><![CDATA[<p>作为分布式系统，尤其是对延迟敏感的实时计算引擎，Apache Flink 需要有强大的容错机制，以确保在出现机器故障或网络分区等不可预知的问题时可以快速自动恢复并依旧能产生准确的计算结果。事实上，Flink 有一套先进的快照机制来持久化作业状态[1]，确保中间数据不会丢失，这通常需要和错误恢复机制（作业重启策略或 failover 策略）配合使用。在遇到错误时，Flink 作业会根据重启策略自动重启并从最近一个成功的快照（checkpoint）恢复状态。合适的重启策略可以减少作业不可用时间和避免人工介入处理故障的运维成本，因此对于 Flink 作业稳定性来说有着举足轻重的作用。下文就将详细解读 Flink 的错误恢复机制。</p><a id="more"></a><p>Flink 容错机制主要有作业执行的容错以及守护进程的容错两方面，前者包括 Flink runtime 的 ExecutionGraph 和 Execution 的容错，后者则包括 JobManager 和 TaskManager 的容错。</p><h1 id="作业执行容错"><a href="#作业执行容错" class="headerlink" title="作业执行容错"></a>作业执行容错</h1><p>众所周知，用户使用 Flink 编程 API（DataStream/DataSet/Table/SQL）编写的作业最终会被翻译为 JobGraph 对象再提交给 JobManager 去执行，而后者会将 JobGraph 结合其他配置生成具体的 Task 调度到 TaskManager 上执行。</p><p>相信不少读者应该见过来自官网文档的这张架构图（图1），它清晰地描绘了作业的分布式执行机制: 一个作业有多个 Operator，相互没有数据 shuffle 、并行度相同且符合其他优化条件的相邻 Operator 可以合并成 OperatorChain，然后每个 Operator 或者 OperatorChain 称为一个 JobVertex；在分布式执行时，每个 JobVertex 会作为一个 Task，每个 Task 有其并行度数目的 SubTask，而这些 SubTask 则是作业调度的最小逻辑单元。</p><center><p><img src="/img/flink-recovery/img1.distributed-dataflow.png" alt="图1. 作业的分布式执行" title="图1. 作业的分布式执行"></p></center><p>该图主要从 TaskManager 角度出发，而其实在 JobManager 端也存在一个核心的数据结构来映射作业的分布式执行，即 ExecutionGraph。ExecutionGraph 类似于图中并行视角的 Streaming Dataflow，它代表了 Job 的一次执行。从某种意义上讲，如果 JobGraph 是一个类的话，ExecutionGraph 则是它的一个实例。ExecutionGraph 中包含的节点称为 ExecutionJobVertex，对应 JobGrap 的一个 JobVertex 或者说图中的一个 Task。ExecutionJobVertex 可以有多个并行实例，即 ExecutionVertex，对应图中的一个 SubTask。在一个 ExecutionGraph 的生命周期中，一个 ExecutionVertex 可以被执行（重启）多次，每次则称为一个 Execution。小结一下，ExecutionGraph 对应 Flink Job 的一次执行，Execution 对应 SubTask 的一次执行。</p><p>相对地，Flink 的错误恢复机制分为多个级别，即 Execution 级别的 Failover 策略和 ExecutionGraph 级别的 Job Restart 策略。当出现错误时，Flink 会先尝试触发范围小的错误恢复机制，如果仍处理不了才会升级为更大范围的错误恢复机制，具体可以用下面的序列图来表达（其中省略了Exection 和 ExecutionGraph 的非关键状态转换）。</p><p><img src="/img/flink-recovery/img2.fault-tolerance-overview.png" alt="图2. 作业执行容错" title="图2. 作业执行容错"><br>当 Task 发生错误，TaskManager 会通过 RPC 通知 JobManager，后者将对应 Execution 的状态转为 <code>failed</code> 并触发 Failover 策略。如果符合 Failover 策略，JobManager 会重启 Execution，否则升级为 ExecutionGraph 的失败。ExecutionGraph 失败则进入 <code>failing</code> 的状态，由 Restart 策略决定其重启（<code>restarting</code> 状态）还是异常退出（<code>failed</code> 状态）。</p><p>下面分别分析两个错误恢复策略的场景及实现。</p><h2 id="Task-Failover-策略"><a href="#Task-Failover-策略" class="headerlink" title="Task Failover 策略"></a>Task Failover 策略</h2><p>作为计算的最小执行单位，Task 错误是十分常见的，比如机器故障、用户代码抛出错误或者网络故障等等都可能造成 Task 错误。对于分布式系统来说，通常单个 Task 错误的处理方式是将这个 Task 重新调度至新的 worker 上，不影响其他 Task 和整体 Job 的运行，然而这个方式对于流处理的 Flink 来说并不可用。</p><p>Flink 的容错机制主要分为从 checkpoint 恢复状态和重流数据两步，这也是为什么 Flink 通常要求数据源的数据是可以重复读取的。对于重启后的新 Task 来说，它可以通过读取 checkpoint 很容易地恢复状态信息，但是却不能独立地重流数据，因为 checkpoint 是不包含数据的，要重流数据只可以要求依赖到的全部上游 Task 重新计算，通常来说会一直追溯到数据源 Task。熟悉 Spark 的同学大概会联想到 Spark 的血缘机制。简单来说，Spark 依据是否需要 shuffle 将作业分划为多个 Stage，每个 Stage 的计算都是独立的 Task，其结果可以被缓存起来。如果某个 Task 执行失败，那么它只要重读上个 Stage 的计算缓存结果即可，不影响其他 Task 的计算。Spark 可以独立地恢复一个 Task，很大程度上是因为它的批处理特性，这允许了作业通过缓存中间计算结果来解耦上下游 Task 的联系。而 Flink 作为流计算引擎，显然是无法简单做到这点的。</p><p>要做到细粒度的错误恢复机制，减小单个 Task 错误对于整体作业的影响，Flink 需要实现一套更加复杂的算法，也就是 FLIP-1 [2] 引入的 Task Failover 策略。Task Failover 策略目前有三个，分别是<br><code>RestartAll</code>、<code>RestartIndividualStrategy</code> 和 <code>RestartPipelinedRegionStrategy</code>。</p><center><p><img src="/img/flink-recovery/img3.task-failover-strategies.png" alt="图3. Restart Region 策略重启有数据交换的 Task" title="图3. Restart Region 策略重启有数据交换的 Task"></p></center><ul><li>RestartAll: 重启全部 Task，是恢复作业一致性的最安全策略，会在其他 Failover 策略失败时作为保底策略使用。目前是默认的 Task Failover 策略。</li><li>RestartPipelinedRegionStrategy: 重启错误 Task 所在 Region 的全部 Task。Task Region 是由 Task 的数据传输决定的，有数据传输的 Task 会被放在同一个 Region，而不同 Region 之间没有数据交换。</li><li>RestartIndividualStrategy: 恢复单个 Task。因为如果该 Task 没有包含数据源，这会导致它不能重流数据而导致一部分数据丢失。考虑到至少提供准确一次的投递语义，这个策略的使用范围比较有限，只应用于 Task 间没有数据传输的作业。不过也有部分业务场景可能需要这种 at-most-once 的投递语义，比如对延迟敏感而对数据一致性要求相对低的推荐系统。</li></ul><p>总体来说，<code>RestartAll</code> 较为保守会造成资源浪费，而 <code>RestartIndividualStrategy</code> 则太过激进不能保证数据一致性，而 <code>RestartPipelinedRegionStrategy</code> 重启的是所有 Task 里最小必要子集，其实是最好的 Failover 策略。而实际上 Apache 社区也正准备在 1.9 版本将其设为默认的 Failover 策略[3]。不过值得注意的是，在 1.9 版本以前 <code>RestartPipelinedRegionStrategy</code> 有个严重的问题是在重启 Task 时并不会恢复其状态[4]，所以请在 1.9 版本以后才使用它，除非你在跑一个无状态的作业。</p><h2 id="Job-Restart-策略"><a href="#Job-Restart-策略" class="headerlink" title="Job Restart 策略"></a>Job Restart 策略</h2><p>如果 Task 错误最终触发了 Full Restart，此时 Job Restart 策略将会控制是否需要恢复作业。Flink 提供三种 Job 具体的 Restart Strategy。</p><ul><li>FixedDelayRestartStrategy: 允许指定次数内的 Execution 失败，如果超过该次数则导致 Job 失败。FixedDelayRestartStrategy 重启可以设置一定的延迟，以减少频繁重试对外部系统带来的负载和不必要的错误日志。目前 FixedDelayRestartStrategy 是默认的 Restart Strategy。</li><li>FailureRateRestartStrategy: 允许在指定时间窗口内的指定次数内的 Execution 失败，如果超过这个频率则导致 Job 失败。同样地，FailureRateRestartStrategy 也可以设置一定的重启延迟。</li><li>NoRestartStrategy: 在 Execution 失败时直接让 Job 失败。</li></ul><p>目前的 Restart Strategy 可以基本满足“自动重启挂掉的作业”这样的简单需求，然而并没有区分作业出错的原因，这导致可能会对不可恢复的错误（比如用户代码抛出的 NPE 或者某些操作报 Permission Denied）进行不必要的重试，进一步的后果是没有第一时间退出，可能导致用户没有及时发现问题，其外对于资源来说也是一种浪费，最后还可能导致一些副作用（比如有些 at-leaset-once 的操作被执行多次）。</p><p>对此，社区在 1.7 版本引入了 Exception 的分类[5]，具体会将 Runtime 抛出的 Exception 分为以下几类:</p><ul><li>NonRecoverableError: 不可恢复的错误。不对此类错误进行重试。</li><li>PartitionDataMissingError: 当前 Task 读不到上游 Task 的某些数据，需要上游 Task 重跑和重发数据。</li><li>EnvironmentError: 执行环境的错误，通常是 Flink 以外的问题，比如机器问题、依赖问题。这种错误的一个明显特征是会在某些机器上执行成功，但在另外一些机器上执行失败。Flink 后续可以引入黑名单机器来更聪明地进行 Task 调度以暂时避免这类问题的影响。</li><li>RecoverableError: 可恢复错误。不属于上述类型的错误都暂设为可恢复的。</li></ul><p>其实这个分类会应用于 Task Failover 策略和 Job Restart 策略，不过目前只有后者会分类处理，而且 Job Restart 策略对 Flink 作业的稳定性影响显然更大，因此放在这个地方讲。值得注意的是，截至目前（1.8 版本）这个分类只处于很初级的阶段，像 NonRecoverable 只包含了作业 State 命名冲突等少数几个内部错误，而 PartitionDataMissingError 和 EnvironmentError 还未有应用，所以绝大多数的错误仍是 RecoverableError。</p><h1 id="守护进程容错"><a href="#守护进程容错" class="headerlink" title="守护进程容错"></a>守护进程容错</h1><p>对于分布式系统来说，守护进程的容错是基本要求而且已经比较成熟，基本包括故障检测和故障恢复两个部分：故障检测通常通过心跳的方式来实现，心跳可以在内部组件间实现或者依赖于 zookeeper 等外部服务；而故障恢复则通常要求将状态持久化到外部存储，然后在故障出现时用于初始化新的进程。</p><p>以最为常用的 on YARN 的部署模式来讲，Flink 关键的守护进程有 JobManager 和 TaskManager 两个，其中 JobManager 的主要职责协调资源和管理作业的执行分别为 ResourceManager 和 JobMaster 两个守护线程承担，三者之间的关系如下图所示。</p><center><p><img src="/img/flink-recovery/img4.flip-6-architecture.png" alt="图4. ResourceManager、JobMaster 和 TaskManager 三者关系" title="图4. ResourceManager、JobMaster 和 TaskManager 三者关系"></p></center><p>在容错方面，三个角色两两之间相互发送心跳来进行共同的故障检测[7]。此外在 HA 场景下，ResourceManager 和 JobMaster 都会注册到 zookeeper 节点上以实现 leader 锁。</p><h2 id="TaskManager-的容错"><a href="#TaskManager-的容错" class="headerlink" title="TaskManager 的容错"></a>TaskManager 的容错</h2><p>如果 ResouceManager 通过心跳超时检测到或者通过集群管理器的通知了解到 TaskManager 故障，它会通知对应的 JobMaster 并启动一个新的 TaskManager 以做代替。注意 ResouceManager 并不关心 Flink 作业的情况，这是 JobMaster 的职责去管理 Flink 作业要做何种反应。</p><p>如果 JobMaster 通过 ResouceManager 的通知了解到或者通过心跳超时检测到 TaskManager 故障，它首先会从自己的 slot pool 中移除该 TaskManager，并将该 TaskManager 上运行的所有 Tasks 标记为失败，从而触发 Flink 作业执行的容错机制以恢复作业。</p><p>TaskManager 的状态已经写入 checkpoint 并会在重启后自动恢复，因此不会造成数据不一致的问题。</p><h2 id="ResourceManager-的容错"><a href="#ResourceManager-的容错" class="headerlink" title="ResourceManager 的容错"></a>ResourceManager 的容错</h2><p>如果 TaskManager 通过心跳超时检测到 ResourceManager 故障，或者收到 zookeeper 的关于 ResourceManager 失去 leadership 通知，TaskManager 会寻找新的 leader ResourceManager 并将自己重启注册到其上，期间并不会中断 Task 的执行。</p><p>如果 JobMaster 通过心跳超时检测到 ResourceManager 故障，或者收到 zookeeper 的关于 ResourceManager 失去 leadership 通知，JobMaster 同样会等待新的 ResourceManager 变成 leader，然后重新请求所有的 TaskManager。考虑到 TaskManager 也可能成功恢复，这样的话 JobMaster 新请求的 TaskManager 会在空闲一段时间后被释放。</p><p>ResourceManager 上保持了很多状态信息，包括活跃的 container、可用的 TaskManager、TaskManager 和 JobMaster 的映射关系等等信息，不过这些信息并不是 ground truth，可以从与 JobMaster 及 TaskManager 的状态同步中再重新获得，所以这些信息并不需要持久化。</p><h2 id="JobMaster-的容错"><a href="#JobMaster-的容错" class="headerlink" title="JobMaster 的容错"></a>JobMaster 的容错</h2><p>如果 TaskManager 通过心跳超时检测到 JobMaster 故障，或者收到 zookeeper 的关于 JobMaster 失去 leadership 通知，TaskManager 会触发自己的错误恢复（目前是释放所有 Task），然后等待新的 JobMaster。如果新的 JobMaster 在一定时间后仍未出现，TaskManager 会将其 slot 标记为空闲并告知 ResourceManager。</p><p>如果 ResourceManager 通过心跳超时检测到 JobMaster 故障，或者收到 zookeeper 的关于 JobMaster 失去 leadership 通知，ResourceManager 会将其告知 TaskManager，其他不作处理。</p><p>JobMaster 保存了很多对作业执行至关重要的状态，其中 JobGraph 和用户代码会重新从 HDFS 等持久化存储中获取，checkpoint 信息会从 zookeeper 获得，Task 的执行信息可以不恢复因为整个作业会重新调度，而持有的 slot 则从 ResourceManager 的 TaskManager 的同步信息中恢复。</p><h2 id="并发故障"><a href="#并发故障" class="headerlink" title="并发故障"></a>并发故障</h2><p>在 on YARN 部署模式下，因为 JobMaster 和 ResourceManager 都在 JobManager 进程内，如果 JobManager 进程出问题，通常是 JobMaster 和 ResourceManager 并发故障，那么 TaskManager 会按以下步骤处理:</p><ol><li>按照普通的 JobMaster 故障处理。</li><li>在一段时间内不断尝试将 slot 提供给新的 JobMaster。</li><li>不断尝试将自己注册到 ResourceManager 上。</li></ol><p>值得注意的是，新 JobManager 的拉起是依靠 YARN 的 Application attempt 重试机制来自动完成的，而根据 Flink 配置的 YARN Application <code>keep-containers-across-application-attempts</code> 行为，TaskManager 不会被清理，因此可以重新注册到新启动的 Flink ResourceManager 和 JobMaster 中。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink 容错机制确保了 Flink 的可靠性和持久性，是 Flink 应用于企业级生产环境的重要保证，具体来说它包括作业执行的容错和守护进程的容错两个方面。在作业执行容错方面，Flink 提供 Task 级别的 Failover 策略和 Job 级别的 Restart 策略来进行故障情况下的自动重试。在守护进程的容错方面，在on YARN 模式下，Flink 通过内部组件的心跳和 YARN 的监控进行故障检测。TaskManager 的故障会通过申请新的 TaskManager 并重启 Task 或 Job 来恢复，JobManager 的故障会通过集群管理器的自动拉起新 JobManager 和 TaskManager 的重新注册到新 leader JobManager 来恢复。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="http://www.whitewood.me/2018/05/13/Flink-%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%BC%82%E6%AD%A5%E5%BF%AB%E7%85%A7-ABS-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" target="_blank" rel="external">Flink 轻量级异步快照 ABS 实现原理</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="external">FLIP-1 : Fine Grained Recovery from Task Failures</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-13223" target="_blank" rel="external">[FLINK-13223] Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml    </a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10712" target="_blank" rel="external">[FLINK-10712] RestartPipelinedRegionStrategy does not restore state</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10289" target="_blank" rel="external">[FLINK-10289] Classify Exceptions to different category for apply different failover strategy</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10288" target="_blank" rel="external">[FLINK-10288] Failover Strategy improvement</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为分布式系统，尤其是对延迟敏感的实时计算引擎，Apache Flink 需要有强大的容错机制，以确保在出现机器故障或网络分区等不可预知的问题时可以快速自动恢复并依旧能产生准确的计算结果。事实上，Flink 有一套先进的快照机制来持久化作业状态[1]，确保中间数据不会丢失，这通常需要和错误恢复机制（作业重启策略或 failover 策略）配合使用。在遇到错误时，Flink 作业会根据重启策略自动重启并从最近一个成功的快照（checkpoint）恢复状态。合适的重启策略可以减少作业不可用时间和避免人工介入处理故障的运维成本，因此对于 Flink 作业稳定性来说有着举足轻重的作用。下文就将详细解读 Flink 的错误恢复机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 非 JVM 语言支持计划</title>
    <link href="https://link3280.github.io/2019/07/11/Flink-%E9%9D%9E-JVM-%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%E8%AE%A1%E5%88%92/"/>
    <id>https://link3280.github.io/2019/07/11/Flink-非-JVM-语言支持计划/</id>
    <published>2019-07-11T12:52:04.000Z</published>
    <updated>2019-07-11T13:06:54.362Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，Apache Flink 是基于 JVM 语言开发的，所以提供的运行环境和编程 API 都是 JVM 语言（目前 Flink Python API 是用 Jython 实现的，因此也算 JVM 语言）。然而基于 JVM 开发的计算引擎普遍会遇到的一个问题是，做数据分析或机器学习的用户通常主要使用更声明式的语言，比如 Python 或者 R。因此为了支持多语言，尤其是非 JVM 语言，分布式计算领域业界在计算引擎的语言可移植性上做了不少的努力，其中比较出名的项目包括 SparkR、PySpark 和 Apache Beam。而目前 Apache Flink 社区也计划推进多语言支持，其中将优先支持 Python，下文将详细解析实现的关键点及具体方案。</p><a id="more"></a><h1 id="业界实践经验"><a href="#业界实践经验" class="headerlink" title="业界实践经验"></a>业界实践经验</h1><p>分布式计算引擎对语言的支持通常分为 SDK 和 UDF 两个部分: 其中 SDK 面向用户，用于构建作业逻辑；而 UDF 则面向运行时，为运行环境中实际调用的函数。相对地，实现多语言支持需要分为两个组件: 一是将提供目标语言和 JVM 之间的桥梁，令目标语言 SDK 可以调用 JVM SDK；二是提供目标语言 UDF 运行时环境及其和 JVM 交互的途径。值得注意的是，这里所说的 UDF 是与语言执行原生环境绑定的，而不是只在 JVM 里访问其他语言定义的函数。</p><p>以 SparkR 为例，其架构如下:</p><center><p><img src="/img/flink-non-jvm/img1.sparkr-architecture.png" alt="SparkR 架构" title="图1. SparkR 架构"></p></center><p>用户用 R 定义的作业逻辑拓扑会通过 R 与 JVM 间的语言桥梁翻译为 Java Spark SDK，即 Java Spark Context，后者与普通 Java 开发的作业一样启动 SparkExecutor，此为 SDK 部分；后续 SparkExecutor 并不是直接在进程内而是启动另外一个 R 进程来执行 UDF，此为 UDF 部分。</p><p>除计算引擎原生的支持外，Apache Beam 为计算引擎提供了另外一个更通用化的支持多语言的途径。Apache Beam 是一个统一流批处理并提供多语言支持的分布式计算框架面门（Facade），提供了计算逻辑在开发运行语言上和在计算引擎上的可移植性。Apache Beam 目前正在开发可移植性层框架，它主要由三个组件组成:</p><center><p><img src="/img/flink-non-jvm/img2.beam-protability-architecture.png" alt="Apache Beam 可移植层架构" title="图2. Apache Beam 可移植层架构"></p></center><ul><li>SDK: SDK 负责提供 Java、Python 和 Go 等语言的编程接口，用于定义数据处理管道（Pipeline）及设置执行选项，比如运行的 Runner、并行度等。</li><li>Runner: Runner 是底层负责将 Pipeline 翻译成执行引擎作业的组件。每个计算计引擎有对应的 Runner，目前已支持 Flink、Spark、Samza、Google Dataflow 等计算引擎。</li><li>SDK harness: SDK harness 是执行用户代码（Beam Fn API， 即 UDF）的组件。SDK harness 可以以独立进程或者容器化的方式实现，环境是与 Runner 相互独立的。SDK harness 会根据执行时的二进制协议与 Runner 交互，包括如何管理 Task、何如传输数据。</li></ul><p>不同组件间通过基于 protobuf 和 gPRC 的跨语言协议交流，以支持多种语言。</p><h1 id="Flink-社区-Roadmap"><a href="#Flink-社区-Roadmap" class="headerlink" title="Flink 社区 Roadmap"></a>Flink 社区 Roadmap</h1><p>基于上述背景，Flink 社区在支持非 JVM 语言上也分别有两种实现方案，即原生的 SDK 支持、原生的 UDF 支持或通过 Beam 来提供。其中社区可以选择其中一种或者多种都实现，关键点在于是否需要 Flink 自身的可移植性层，即 SDK 的可移植层和 UDF 的可移植层。</p><p>在 SDK 的可移植性上，Flink 社区基本认同这是必要的，因为 SDK 负责主要是定义作业逻辑拓扑和配置，比较轻量级，而且原生 SDK 作业用户入口需要尽可能完整地暴露 Flink 的功能，完全依赖 Beam 来实现是不可取的。</p><p>在 UDF 的可移植层上，社区则出现了两种观点：一种观点主要是来自同样活跃在 Beam 社区的工程师，他们认为可移植性层需要大量的工作，Beam 社区花费至少一年重构了大部分的代码才基本完成，所以比起重新发明可移植性层， Flink 社区更应该把精力用在和 Beam 的集成上；另一种观点主要来自阿里巴巴 Blink 团队的工程师，他们已经在 Blink 上实现了对 Python UDF 的支持，并认为 Flink 的可移植层可以更好地利用引擎原生的优化，所以可以在支持 Beam 的基础上也实现 Flink 本身的可移植层。</p><p>社区讨论最后并没有达成一致，不过作为进一步的探索，Flink 将首先在 Table API 上实现 Python SDK，用于满足数据分析用户的需求，不过 Python 或其他语言的 UDF 需等后续收集用户反馈再决定是否要提供。Python Table API 的设计文档见 FLIP-38 [2]，虽然目前只针对 Table API，但我们可以从中窥见 Flink 对非 JVM 语言支持的发展方向。</p><h1 id="Flink-Table-Python-支持"><a href="#Flink-Table-Python-支持" class="headerlink" title="Flink Table Python 支持"></a>Flink Table Python 支持</h1><p>之所以首先在 Table API 支持 Python，原因是 Table API 将逐渐成为数据分析用户主要使用的 API，Python Table API 将大大提升用户友好度，而对于其他语言的需求还没有那么迫切。此外，Table API 比起底层的 DataStream/DataSet API 更为声明式，而且更符合流批处理统一的原则，并且可以在翻译成 JobGraph 时灵活应用优化规则。</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>Python Table API 需要达成以下目标:</p><ul><li>用户可以在 Python 环境使用与 Java 版本相同的 Flink Table API。</li><li>用户可以通过现有所有途径提交以 Python Table API 编写的作业，包括通过脚本提交（类似于 <code>bin/flink run</code>），通过 REST 接口提交，以交互式 shell 的方式提交和本地 IDE 调试运行。</li><li>用户可以用 Python 编写 UDF、UDAF 和 UDTF。</li><li>Pandas 函数可以用于 Python Table API。</li></ul><p>简单来说，通过 Python Table API 用户可以不依赖 Java API 进行开发和部署，而运行时对于 Java 环境的依赖则是对用户透明的。另外，Python Table API 在首个 release 将只支持离线批处理模式和用于探索开发的交互式 shell 模式，后续再拓展至实时流处理模式。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="SDK"><a href="#SDK" class="headerlink" title="SDK"></a>SDK</h3><center><p><img src="/img/flink-non-jvm/img3.flink-sdk-protability.png" alt="Flink SDK 可移植层架构" title="图3. Flink SDK 可移植层架构"></p></center><p>类似于 SparkR 的实现，Python SDK 相当于是在 Java SDK 基础上进行的一层封装，它通过 rpc 协议来与 Java SDK 交互，并将后者的功能完全暴露至 Python 解释器。其中 rpc 协议是实现跨语言的关键，具体来说有两种 rpc 实现方案。</p><p>一是利用已有的语言桥梁 lib，比如 Py4j。Py4j 自动负责 Java class 和 Python class 之间的映射，这个映射是通过 Python 解释器中的 stub（Python Gateway）和 JVM 中的 stub（Java Gateway Server）完成的（要避免误会的一点是，Py4j 是双向的，因此也可以是 Java Gateway 和 Python Gateway Server）。因此，在 Python SDK 中我们可以完全复用 Java Table API 的 class，开发量最少。</p><center><p><img src="/img/flink-non-jvm/img4.py4j-approach.png" alt="基于 Py4j 的 Flink SDK 可移植层" title="图4. 基于 Py4j 的 Flink SDK 可移植层"></p></center><p>二是开发完整的 SDK 可移植层，每个语言的 SDK 可以独立建立 JobGraph 对象，而不是基于 Java 已有的实现。这要求将 DAG 生成和作业配置等 SDK 功能提到每个语言的 SDK 来实现，并且 JobGraph 需要重构为和具体语言无关的数据结构，用 protobuf 等序列化框架来表达。</p><center><p><img src="/img/flink-non-jvm/img5.jobgraph-approach.png" alt="原生实现的 Flink SDK 可移植层" title="图5. 原生实现的 Flink SDK 可移植层"></p></center><p>从长远来说方案二是模块化和可拓展性更好的架构，不过出于大方向仍未完全确定的考虑，Flink 在初期将先以开发成本较低的方案一实现 Python Table API。</p><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>UDF 则是支持非 JVM 语言最为复杂的一部分，尤其是对于 Flink 这样的有状态计算来说。从设计原则来说，UDF 会运行在一个类似容器的独立环境，需要通过 RPC 的方式来和 Flink Operator 进行通信，因此如何实现 RPC 服务将 Flink Operator 的功能暴露给外部是关键点。在这个问题上，Beam 社区已经有了很好的实践经验。对于有状态计算， Beam 将对 UDF 的支持抽象为四个服务:</p><center><p><img src="/img/flink-non-jvm/img6.beam-udf-architecture.png" alt="Beam Runner 的 UDF 架构" title="图6. Beam Runner 的 UDF 架构"></p></center><ul><li>Control Service: 负责 UDF 的管理，比如启停 SDC harness（运行 UDF 的环境）。</li><li>Data Service: 负责提供 Runner 和 UDF 间的数据流传输。</li><li>State Service: 负责提供 UDF 中 State 的管理。</li><li>Logging Service: 负责接收并处理 UDF 的输出日志。</li></ul><p>相比之下，Flink 提供多了一个 Metrics Service 来采集 UDF 产生的 metric。</p><center><p><img src="/img/flink-non-jvm/img7.flink-python-sdk-overview.png" alt="Flink Python UDF 架构" title="图7. Flink Python UDF 架构"></p></center><p>无论是 Beam 或事 Flink，UDF 架构中 Data Service 和 State Service 涉及到计算效率和数据准确性，是最为核心的服务。更加重要的是，两者的通信管道都是双工的，因此在实现上也最为复杂。下文将结合 UDF 的类型重点说明这两个服务的架构。</p><h4 id="Table-API-UDF-分类"><a href="#Table-API-UDF-分类" class="headerlink" title="Table API UDF 分类"></a>Table API UDF 分类</h4><p>熟悉 Hive 的同学应该知道用户定义的函数除了 UDF、还有 UDAF（User Defined Aggregate Function）和 UDTF（User Defined Table Function），不过为了方便起见都称为广义的 UDF 或者 UDX。三者主要不同在于输入行数与输出行数的关系，可以用以下的表格概括，其中 UDF、UDAF 和 UDTF 分别对应 Table API 的 ScalarFcuntion、AggregateFunction 和 TableFunction。</p><center><p><img src="/img/flink-non-jvm/img8.table-api-udx.png" alt="Flink Table API UDFs" title="图8. Flink Table API UDFs"></p></center><p>其中 ScalarFcuntion 和 TableFunction 输入均为一行，因此没有涉及到跨行的状态处理，属于无状态的 UDF；与之相对，AggregateFunction 以多行为输入，涉及中间状态，属于有状态的 UDF。下面分别讨论对于该两种 UDF 的实现。</p><h4 id="无状态的-UDF"><a href="#无状态的-UDF" class="headerlink" title="无状态的 UDF"></a>无状态的 UDF</h4><p>对于无状态的 UDF 来说，最为重要的服务是 DataService。DataService 负责 JVM 和 Python 解释器间的数据传输，性能极其关键并需要重点优化，这主要分为数据处理模式（Data Processing Mode）和数据传输模式（Data Transmission Mode）两个方面。</p><p>数据处理模式指的是 UDF 以同步还是异步的方式处理数据。同步的处理方式意味着 UDF 将逐条处理数据，在一条数据被处理完并输出前，Flink Operator 不会将下一条数据发送给 UDF。相对地，异步的处理方式意味着数据传输和 UDF 计算解耦，Flink Operator 可以将数据批量传输到 UDF 的输入队列缓存起来，UDF 计算的输出也可以先放入 UDF 的输出队列。</p><p>数据传输模式指的是 Flink Operator 与 UDF 间的数据传输是逐条传输还是批量传输。值得注意的是，逐条传输可以和同步数据处理和异步数据处理配合使用，批量传输则必须和异步的数据处理模式配合使用。</p><p>显而易见，性能关键的 DataService 使用异步数据处理模式加上批量传输是更合适的。总体的架构会如下:</p><center><p><img src="/img/flink-non-jvm/img9.stateless-udf-architecture.png" alt="Stateless UDF 架构" title="图9. Stateless UDF 架构"></p></center><p>首先 Flink Operator 将摄入的数据放到 input buffer 并发送至 Python Worker，随后该 buffer 变为 waiting buffer 仍保存原始数据；然后 Python Worker 端将 input buffer 的数据逐条交给 UDF 执行，其输出结果作为 result buffer 发回给 Flink Operator；最后 Flink Operator 将 result buffer 和 waiting buffer 合并生成最终结果并发给下游。对于为什么需要 waiting buffer，笔者猜测是 rpc 发给 Python Worker 的数据只包含了 UDF 调用的列，所以 result buffer 并不是完整数据，需要和 waiting buffer 合并才是最终结果。</p><h4 id="有状态的-UDF"><a href="#有状态的-UDF" class="headerlink" title="有状态的 UDF"></a>有状态的 UDF</h4><p>有状态的 UDF 是最为复杂的一类 UDF，因为 Python Worker 需要和 Flink Operator 同步 state 信息。</p><center><p><img src="/img/flink-non-jvm/img10.stateful-udf-architecture.png" alt="Stateful UDF 架构" title="图10. Stateful UDF 架构"></p></center><p>相比起无状态的 UDF，最大的不同有两点:</p><ul><li>数据先执行 group 后再发至 Python Worker。这样的好处是 UDF 输入是按 key 排序的，所以方便合并 UDF 的输出。另外一点是减轻了 Python Worker 的负担，Python Worker 在同一时间只会处理一个 key 的数据，不需要维持多个 key 的状态。</li><li>UDF 可以访问 Flink Operator 端的 state。Flink Operator 的 state 会通过 State Service 以 DataView 的形式暴露给 Python Worker，后者对 state 的操作会映射到 StateBackend。为了避免频繁的 state 访问，Python Worker 会缓存部分 state 数据，这部分 state 会在 checkpoint 快照期间同步会 Flink Operator 以确保一致性。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实时计算引擎对非 JVM 语言的支持将主要分为客户端的 SDK 和运行时的 UDF 两部分，其中最为核心的部分是定义跨语言的 rpc 协议层来负责不同语言运行环境间的数据通信。对于 Flink 来说，要支持非 JVM 语言有两种途径: 通过集成 Apache Beam 或者实现原生支持。从长远来说，和 Apache Beam 集成应该没有太大疑问，而对于原生支持则尚未确定，所以社区会先通过实现较为独立的 Python Table API 来作为后续计划的探索。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://beam.apache.org/roadmap/portability/" target="_blank" rel="external">Apache Beam - Portability Framework Roadmap</a></li><li><a href="https://docs.google.com/document/d/1ybYt-0xWRMa1Yf5VsuqGRtOfJBz4p74ZmDxZYg3j_h8/edit#heading=h.p6h40rdmqvbx" target="_blank" rel="external">[DISCUSS] FLIP-38 Support python language in flink Table API</a></li><li><a href="https://docs.google.com/document/d/1XYzb1Fnt2sam7u2MsGFaZp-2qSIGxUn66VLer-bcXAk/edit#heading=h.p6lvszfbmyj6" target="_blank" rel="external">Apache Beam Fn API Overview</a></li><li><a href="https://docs.google.com/document/d/1cKOB9ToasfYs1kLWQgffzvIbJx2Smy4svlodPRhFrk4/edit#" target="_blank" rel="external">Apache Beam Portability API</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;众所周知，Apache Flink 是基于 JVM 语言开发的，所以提供的运行环境和编程 API 都是 JVM 语言（目前 Flink Python API 是用 Jython 实现的，因此也算 JVM 语言）。然而基于 JVM 开发的计算引擎普遍会遇到的一个问题是，做数据分析或机器学习的用户通常主要使用更声明式的语言，比如 Python 或者 R。因此为了支持多语言，尤其是非 JVM 语言，分布式计算领域业界在计算引擎的语言可移植性上做了不少的努力，其中比较出名的项目包括 SparkR、PySpark 和 Apache Beam。而目前 Apache Flink 社区也计划推进多语言支持，其中将优先支持 Python，下文将详细解析实现的关键点及具体方案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink State As Database</title>
    <link href="https://link3280.github.io/2019/06/02/Flink-State-As-Database/"/>
    <id>https://link3280.github.io/2019/06/02/Flink-State-As-Database/</id>
    <published>2019-06-02T07:02:03.000Z</published>
    <updated>2019-06-02T07:07:37.664Z</updated>
    
    <content type="html"><![CDATA[<p>有状态的计算作为容错以及数据一致性的保证，是当今实时计算必不可少的特性之一，流行的实时计算引擎包括 Google Dataflow、Flink、Spark (Structure) Streaming、Kafka Streams 都分别提供对内置 State 的支持。State 的引入使得实时应用可以不依赖外部数据库来存储元数据及中间数据，部分情况下甚至可以直接用 State 存储结果数据，这让业界不禁思考: State 和 Database 是何种关系？有没有可能用 State 来代替数据库呢？</p><a id="more"></a><p>在这个课题上，Flink 社区是比较早就开始探索的。总体来说，Flink 社区的努力可以分为两条线: 一是在作业运行时通过作业查询接口访问 State 的能力，即 QueryableState；二是通过 State 的离线 dump 文件（Savepoint）来离线查询和修改 State 的能力，即即将引入的 Savepoint Connector。</p><h1 id="QueryableState"><a href="#QueryableState" class="headerlink" title="QueryableState"></a>QueryableState</h1><p>在 2017 年发布的 Flink 1.2 版本，Flink 引入了 QueryableState 的特性以允许用户通过特定的 client 查询作业 State 的内容 [1]，这意味着 Flink 应用可以在完全不依赖 State 存储介质以外的外部存储的情况下提供实时访问计算结果的能力。</p><p><img src="/img/QS-NoKV-Architecture-QS-Post-1.png" alt="只通过 Queryable State 提供实时数据访问" title="只通过 Queryable State 提供实时数据访问"></p><p>然而，QueryableState 虽然设想上比较理想化，但由于依赖底层架构的改动较多且功能也比较受限，它一直处于 Beta 版本并不能用于生产环境。针对这个问题，在前段时间腾讯的工程师杨华提出 QueryableState 的改进计划 [2]。在邮件列表中，社区就 QueryableState 是否可以用于代替数据库作了讨论并出现了不同的观点。笔者结合个人见解将 State as Database 的主要优缺点整理如下。</p><p>优点: </p><ul><li>更低的数据延迟。一般情况下 Flink 应用的计算结果需要同步到外部的数据库，比如定时触发输出窗口计算结果，而这种同步通常是定时的会带来一定的延迟，导致计算是实时的而查询却不是实时的尴尬局面，而直接 State 则可以避免这个问题。</li><li>更强的数据一致性保证。根据外部存储的特性不同，Flink Connector 或者自定义的 SinkFunction 提供的一致性保障也有所差别。比如对于不支持多行事务的 HBase，Flink 只能通过业务逻辑的幂等性来保障 Exactly-Once 投递。相比之下 State 则有妥妥的 Exactly-Once 投递保证。</li><li>节省资源。因为减少了同步数据到外部存储的需要，我们可以节省序列化和网络传输的成本，另外当然还可以节省数据库成本。</li></ul><p>缺点:</p><ul><li>SLA 保障不足。数据库技术已经非常成熟，在可用性、容错性和运维上都很多的积累，在这点上 State 还相当于是处于原始人时期。另外从定位上来看，Flink 作业有版本迭代维护或者遇到错误自动重启带来的 down time，并不能达到数据库在数据访问上的高可用性。</li><li>可能导致作业的不稳定。未经过考虑的 Ad-hoc Query 可能会要求扫描并返回夸张量级的数据，这会系统带来很大的负荷，很可能影响作业的正常执行。即使是合理的 Query，在并发数较多的情况下也可能影响作业的执行效率。</li><li>存储数据量不能太大。State 运行时主要存储在 TaskManager 本地内存和磁盘，State 过大会造成 TaskManager OOM 或者磁盘空间不足。另外 State 大意味着 checkpoint 大，导致 checkpoint 可能会超时并显著延长作业恢复时长。</li><li>只支持最基础的查询。State 只能进行最简单的数据结构查询，不能像关系型数据库一样提供函数等计算能力，也不支持谓词下推等优化技术。</li><li>只可以读取，不能修改。State 在运行时只可以被作业本身修改，如果实在要修改 State 只能通过下文的 Savepoint Connector 来实现。</li></ul><p>总体来说，目前 State 代替数据库的缺点还是远多于其优点，不过对于某些对数据可用性要求不高的作业来说，使用 State 作为数据库还是完全合理的。由于定位上的不同，Flink State 在短时间内很难看到可以完全替代数据库的可能性，但在数据访问特性上 State 往数据库方向发展是无需质疑的。</p><h1 id="Savepoint-Connector"><a href="#Savepoint-Connector" class="headerlink" title="Savepoint Connector"></a>Savepoint Connector</h1><p>Savepoint Connector 是社区最近提出的一个新特性（见 FLIP-42 [3]），用于离线对 State 的 dump 文件 Savepoint 进行分析、修改或者直接根据数据构建出一个初始的 Savepoint。Savepoint Connector 属于 Flink State Evolution 的 State Management。如果说 QueryableState 是 DSL 的话，Flink State Evolution 就是 DML，而 Savepoint Connector 就是 DML 中最为重要的部分。</p><p>Savepoint Connector 的前身是第三方的 Bravo 项目 [4]，主要思路提供 Savepoint 和 DataSet 相互转换的能力，典型应用是 Savepoint 读取成 DataSet，在 DataSet 上进行修改，然后再写为一个新的 Savepoint。这适合用于以下的场景:</p><ul><li>分析作业 State 以研究其模式和规律</li><li>排查问题或者审计</li><li>为新的应用构建的初始 State</li><li>修改 Savepoint，比如:<ul><li>改变作业最大并行度</li><li>进行巨大的 Schema 改动</li><li>修正有问题的 State</li></ul></li></ul><p>Savepoint 作为 State 的 dump 文件，通过 Savepoint Connector 可以暴露数据查询和修改功能，类似于一个离线的数据库，但 State 的概念和典型关系型数据的概念还是有很多不同，FLIP-43 也对这些差异进行了类比和总结。</p><p>首先 Savepoint 是多个 operator 的 state 的物理存储集合，不同 operator 的 state 是独立的，这类似于数据库下不同 namespace 之间的 table。我们可以得到 Savepoint 对应数据库，单个 operator 对应 Namespace。</p><table><thead><tr><th>Database</th><th>Savepoint</th></tr></thead><tbody><tr><td>Namespace</td><td>Uid</td></tr><tr><td>Table</td><td>State</td></tr></tbody></table><p>但就 table 而言，其在 Savepoint 里对应的概念根据 State 类型的不同而有所差别。State 有 Operator State、Keyed State 和 Broadcast State 三种，其中 Operator State 和 Broadcast State 属于 non-partitioned state，即没有按 key 分区的 state，而相反地 Keyed State 则属于 partitioned state。对于 non-partitioned state 来说，state 是一个 table，state 的每个元素即是 table 里的一行；而对于 partitioned state 来说，同一个 operator 下的所有 state 对应一个 table。 这个 table 像是 HBase 一样有个 row key，然后每个具体的 state 对应 table 里的一个 column。</p><p>举个例子，假设有一个游戏玩家得分和在线时长的数据流，我们需要用 Keyed State 来记录玩家所在组的分数和游戏时长，用 Operator State 记录玩家的总得分和总时长。</p><p>在一段时间内数据流的输入如下:</p><table><thead><tr><th>user_id</th><th>user_name</th><th>user_group</th><th>score</th></tr></thead><tbody><tr><td>1001</td><td>Paul</td><td>A</td><td>5,000</td></tr><tr><td>1002</td><td>Charlotte</td><td>A</td><td>3,600</td></tr><tr><td>1003</td><td>Kate</td><td>C</td><td>2,000</td></tr><tr><td>1004</td><td>Robert</td><td>B</td><td>3,900</td></tr></tbody></table><table><thead><tr><th>user_id</th><th>user_name</th><th>user_group</th><th>time</th></tr></thead><tbody><tr><td>1001</td><td>Paul</td><td>A</td><td>1,800</td></tr><tr><td>1002</td><td>Charlotte</td><td>A</td><td>1,200</td></tr><tr><td>1003</td><td>Kate</td><td>C</td><td>600</td></tr><tr><td>1004</td><td>Robert</td><td>B</td><td>2,000</td></tr></tbody></table><p>用 Keyed State ，我们分别注册 <code>group_score</code> 和 <code>group_time</code> 两个 MapState<string, integer=""> 表示组总得分和组总时长，并根据 <code>user_group</code> keyby 数据流之后将两个指标的累积值更新到 State 里，得到的表如下:</string,></p><table><thead><tr><th>user_group</th><th>group_score</th><th>group_time</th></tr></thead><tbody><tr><td>A</td><td>8,600</td><td>3,000</td></tr><tr><td>C</td><td>2,00</td><td>600</td></tr><tr><td>B</td><td>3,900</td><td>2,000</td></tr></tbody></table><p>相对地，假如用 Operator State 来记录总得分和总时长（并行度设为 1），我们注册 <code>total_score</code> 和 <code>total_time</code> 两个 State，得到的表有两个:</p><table><thead><tr><th>total_score</th></tr></thead><tbody><tr><td>14,500</td></tr></tbody></table><table><thead><tr><th>total_time</th></tr></thead><tbody><tr><td>5,600</td></tr></tbody></table><p>至此 Savepoint 和 Database 的对应关系应该是比较清晰明了的。而对于 Savepoint 来说还有不同的 StateBackend 来决定 State 具体如何持续化，这显然对应的是数据库的存储引擎。在 MySQL 中，我们可以通过简单的一行命令 <code>ALTER TABLE xxx ENGINE = InnoDB;</code> 来改变存储引擎，在背后 MySQL 会自动完成繁琐的格式转换工作。而对于 Savepoint 来说，由于 StateBackend 各自的存储格式不兼容，目前尚不能方便地切换 StateBackend。为此，社区在不久前创建 FLIP-41 [5] 来进一步完善 Savepoint 的可操作性。Savepoint 相关的这两个 FLIP 的目标版本都是预计在 7 月发布的 1.9 版本，敬请期待。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>State as Database 是实时计算发展的大趋势，它并不是要代替数据库的使用，而是借鉴数据库领域的经验拓展 State 接口使其操作方式更接近我们熟悉的数据库。对于 Flink 而言，State 的外部使用可以分为在线的实时访问和离线的访问和修改，分别将由 Queryable State 和 Savepoint Connector 两个特性支持。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://www.ververica.com/blog/queryable-state-use-case-demo" target="_blank" rel="external">Queryable State in Apache Flink® 1.2.0: An Overview &amp; Demo</a></li><li><a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Improve-Queryable-State-and-introduce-a-QueryServerProxy-component-td28578.html#a28581" target="_blank" rel="external">Improve Queryable State and Introduce a QueryServerProxy Component</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-43%3A+Savepoint+Connector" target="_blank" rel="external">FLIP-43: Savepoint Connector</a></li><li><a href="https://github.com/king/bravo" target="_blank" rel="external">Bravo: Utilities for processing Flink checkpoints/savepoints</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-41%3A+Unify+Keyed+State+Snapshot+Binary+Format+for+Savepoints" target="_blank" rel="external">FLIP-41: Unify Keyed State Snapshot Binary Format for Savepoints</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有状态的计算作为容错以及数据一致性的保证，是当今实时计算必不可少的特性之一，流行的实时计算引擎包括 Google Dataflow、Flink、Spark (Structure) Streaming、Kafka Streams 都分别提供对内置 State 的支持。State 的引入使得实时应用可以不依赖外部数据库来存储元数据及中间数据，部分情况下甚至可以直接用 State 存储结果数据，这让业界不禁思考: State 和 Database 是何种关系？有没有可能用 State 来代替数据库呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.8 Release 解读</title>
    <link href="https://link3280.github.io/2019/04/14/Flink-1-8-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2019/04/14/Flink-1-8-Release-解读/</id>
    <published>2019-04-14T11:58:26.000Z</published>
    <updated>2019-04-14T12:28:28.939Z</updated>
    
    <content type="html"><![CDATA[<p><center><p><img src="/img/Flink-1.8.png" alt="Flink 1.8"></p></center></p><p></p><p>在距离上个 feature 版本发布近四个月之后， 近日 Apache Flink 发布了 1.8 版本。该版本处理了 420 个 issue，其中新 feature 及改进主要集中在 State、Connector 和 Table API 三者上，并 fix 了一些在生产部署中较为常见的问题。下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes <a href="https://flink.apache.org/news/2019/04/09/release-1.8.0.html" target="_blank" rel="external">[1]</a>。</p><a id="more"></a><h2 id="State"><a href="#State" class="headerlink" title="State"></a>State</h2><h3 id="State-Schema-Evolution-新增对内置序列化器的支持"><a href="#State-Schema-Evolution-新增对内置序列化器的支持" class="headerlink" title="State Schema Evolution 新增对内置序列化器的支持"></a>State Schema Evolution 新增对内置序列化器的支持</h3><p>Flink 1.7 版本新增 State Schema Evolution 的特性，可以在支持 State POJO 随业务逻辑演变（见之前一篇博客<a href="http://www.whitewood.me/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/" target="_blank" rel="external">[9]</a>），但要求序列化器向后兼容，而当时 Flink 内置的 POJOSerializer 并不能做到这点，因此 1.7 版本只提供了 AvroSerializer 的 State Schema Evolution。1.8 版本修复了这个尴尬的情况，现在用户新增或者移除 POJO 的任意字段不会不破坏 Savepoint 兼容性。</p><h3 id="State-支持基于-TTL-的持续增量清理"><a href="#State-支持基于-TTL-的持续增量清理" class="headerlink" title="State 支持基于 TTL 的持续增量清理"></a>State 支持基于 TTL 的持续增量清理</h3><p>Flink 自 1.6 版本提供的 State TTL 现已被广泛使用，但是用户经常反馈的一个痛点是 Keyed State 在一个 key 过期之后，只有被再次访问后才会被发现，然后通过后台线程物理上清理掉。这会造成一个问题是有的 key 只是在一段时期内出现，如果依赖 State TTL 的话它可能永远不会被发现和清理，造成 State size 可能一直增长，因此在实践中用户常常还要通过设置各种 Timer 来手动清理不会再出现的 key 的 State。</p><p>Flink 1.8 实现了 State 的持续增量清理（continuous cleanup），即每次 full snapshot（即一般 checkpoint/savepoint，或者 incremental checkpoint 的 full snapshot 运行周期）的时候扫描全部的 entry 来检查和清理其中过期的部分，不过代价也是显而易见的更长的 checkpoint 时间。</p><h2 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h2><h3 id="FlinkKafkaConsumer-支持访问-Kafka-消息-header"><a href="#FlinkKafkaConsumer-支持访问-Kafka-消息-header" class="headerlink" title="FlinkKafkaConsumer 支持访问 Kafka 消息 header"></a>FlinkKafkaConsumer 支持访问 Kafka 消息 header</h3><p>Flink 1.8 新增了 KafkaDeserializationSchema 来给予用户对 Kafka 消息的完整访问权限，在这之前用户只可以访问反序列后的消息 body 而不能访问 header。这个 feature 使得用户可以在 Flink Application 里实现更多的定制化需求，比如我们正想做的基于 Kafka header timestamp 的类似于 Prevega 的 StreamCut 功能，以满足重流数据的场景。</p><h3 id="将从-State-恢复的-Kafka-topic-partition-从消费列表过滤掉"><a href="#将从-State-恢复的-Kafka-topic-partition-从消费列表过滤掉" class="headerlink" title="将从 State 恢复的 Kafka topic partition 从消费列表过滤掉"></a>将从 State 恢复的 Kafka topic partition 从消费列表过滤掉</h3><p>在 1.8 版本之前，FlinkKafkaConsumer 从 savepoint/checkpoint 恢复时会将读取所有 Kafka topic partition offset，在 topic name 或者 pattern 出现变化时，比如用户想从 topic A 改为 topic B，恢复后的作业会读取同时两个 topic。如果用户不再需要读取 topic A，一个办法是将 Kafka source 的 uid 改变来避免 FlinkKafkaConsumer 恢复状态。</p><p>Flink 1.8 修复了这个问题，并默认会根据新的 topic name 或者 pattern 判断需要恢复哪些 offset。如果用户希望保留之前的行为，可以通过 <code>FlinkKafkaConsumer#disableFilterRestoredPartitionsWithSubscribedTopics()</code> 来禁用过滤。</p><h3 id="StreamingFileSink-支持-SequenceFile"><a href="#StreamingFileSink-支持-SequenceFile" class="headerlink" title="StreamingFileSink 支持 SequenceFile"></a>StreamingFileSink 支持 SequenceFile</h3><p>Flink 1.6 新增了 StreamingFileSink 来代替 BucketingSink 成为写文件或对象到下游系统的主流 connector，但是并不支持 Hadoop SequenceFile 格式（虽然支持了 Parquet 和 ORC），这个问题同样在 1.8 版本得到修复。</p><h2 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h2><h3 id="重构-Table-API"><a href="#重构-Table-API" class="headerlink" title="重构 Table API"></a>重构 Table API</h3><p>Table API 的重构主要是为 Merge 阿里巴巴贡献的 Blink 做准备。Blink 在 SQL 层和 Table 层上做了比较多的改进，因此为了实现一个更为平滑的 Merge 进程，社区提出了 FLIP-32 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="external">[3]</a> 以重构 Table API，主要目的降低 Table API 内部以及自身和其他模块（DataSet/DataStream 等）的耦合性。</p><p>Merge 的进程会分为以下几个步骤实现:  </p><ol><li>移除 Table API 对于 Scala 的依赖。由于历史原因，Table API 主要使用 Scala 来编写，其中主要问题是 Scala class 中的 private 或者 protected 变量对于 Java class 来说是可见的，因此会造成潜在的安全问题且对用户不友好。  </li><li>优化 API 设计。目前 flink-table 模块有 7 个 TableEnvironment（3 个 base class + 4 个 Java/Scala 的 TableEnvironment）作为编程接口，不够高内聚。  </li><li>Table API 与 DataStream/DataSet API 解耦。让 Table API self-container，在编程接口上不需要依赖 DataStream/DataSet。  </li><li>统一批处理和流处理。这是 Blink 团队一直向社区推的一个重要 feature，也是 Flink 的核心理念”批处理是流处理的特例”的体现。这个目标会将 DataSet 从直接基于 runtime 迁移到 DataStream 之上，因此毋需额外为 DataSet 维护一个执行层栈，详情可见<a href="https://docs.google.com/document/d/1G0NUIaaNJvT6CMrNCP6dRXGv88xNhDQqZFrQEuJ0rVU/edit#heading=h.ob9i0lcn7ulz" target="_blank" rel="external">[4]</a>。</li><li>处理 Table API 向后兼容性。以上的步骤涉及到 TableEnvironmnet 等面向用户的 API 改动，因此需要提前识别出来并在过渡版本标记为 @Deprecated，以便在后续版本移除。</li><li>Merga Blink 的 SQL feature 以及架构上的改进。该步也会分成多个阶段进行，比如首先是 Blink planer（计划器），然后再逐个 Operator 改动。</li></ol><p>Flink 1.8 目前来说在步骤 1 2 3 上都有初步的进度。</p><h3 id="Table-API-支持-Kafka-CSV-格式数据的-Schema"><a href="#Table-API-支持-Kafka-CSV-格式数据的-Schema" class="headerlink" title="Table API 支持 Kafka CSV 格式数据的 Schema"></a>Table API 支持 Kafka CSV 格式数据的 Schema</h3><p>在 1.8 版本以前，在 Table API 使用 CSV 格式数据时没有充分利用 CVS 的 header 来定义 Schema，导致用户需要在 Table 定义中和 CSV 格式中重复定义 Schema，比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.withFormat(</div><div class="line">  new Csv()</div><div class="line">    .field(&quot;field1&quot;, Types.STRING)    // required: ordered format fields</div><div class="line">    .field(&quot;field2&quot;, Types.TIMESTAMP)</div><div class="line">    .fieldDelimiter(&quot;,&quot;)              // optional: string delimiter &quot;,&quot; by default</div><div class="line">    .lineDelimiter(&quot;\n&quot;)              // optional: string delimiter &quot;\n&quot; by default</div><div class="line">    .quoteCharacter(&apos;&quot;&apos;)              // optional: single character for string values, empty by default</div><div class="line">    .commentPrefix(&apos;#&apos;)               // optional: string to indicate comments, empty by default</div><div class="line">    .ignoreFirstLine()                // optional: ignore the first line, by default it is not skipped</div><div class="line">    .ignoreParseErrors()              // optional: skip records with parse error instead of failing by default</div><div class="line">)</div></pre></td></tr></table></figure><p>1.8 版本引入符合 <a href="https://tools.ietf.org/html/rfc4180" target="_blank" rel="external">RFC-4180</a> 的新版本 CVS 格式，当 CSV header 和 Table 定义的 Schema 吻合时，可以自动生成数据 Schema，因此代码可以简化为: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">.withFormat(</div><div class="line">  new Csv()</div><div class="line"></div><div class="line">    // required: define the schema either by using type information</div><div class="line">    .schema(Type.ROW(...))</div><div class="line"></div><div class="line">    // or use the table&apos;s schema</div><div class="line">    .deriveSchema()</div><div class="line"></div><div class="line">    .fieldDelimiter(&apos;;&apos;)         // optional: field delimiter character (&apos;,&apos; by default)</div><div class="line">    .lineDelimiter(&quot;\r\n&quot;)       // optional: line delimiter (&quot;\n&quot; by default;</div><div class="line">                                 //   otherwise &quot;\r&quot; or &quot;\r\n&quot; are allowed)</div><div class="line">    .quoteCharacter(&apos;\&apos;&apos;)        // optional: quote character for enclosing field values (&apos;&quot;&apos; by default)</div><div class="line">    .allowComments()             // optional: ignores comment lines that start with &apos;#&apos; (disabled by default);</div><div class="line">                                 //   if enabled, make sure to also ignore parse errors to allow empty rows</div><div class="line">    .ignoreParseErrors()         // optional: skip fields and rows with parse errors instead of failing;</div><div class="line">                                 //   fields are set to null in case of errors</div><div class="line">    .arrayElementDelimiter(&quot;|&quot;)  // optional: the array element delimiter string for separating</div><div class="line">                                 //   array and row element values (&quot;;&quot; by default)</div><div class="line">    .escapeCharacter(&apos;\\&apos;)       // optional: escape character for escaping values (disabled by default)</div><div class="line">    .nullLiteral(&quot;n/a&quot;)          // optional: null literal string that is interpreted as a</div><div class="line">                                 //   null value (disabled by default)</div><div class="line">)</div></pre></td></tr></table></figure><h3 id="SQL-CEP-函数支持-UDF"><a href="#SQL-CEP-函数支持-UDF" class="headerlink" title="SQL CEP 函数支持 UDF"></a>SQL CEP 函数支持 UDF</h3><p>Flink 1.7 支持了 SQL <code>MATCH_RECOGNIZE</code> 函数，用户可以在 SQL 中使用 CEP，但仍不能在 <code>MATCH_RECOGNIZE</code> 语句里使用 UDF，这会给使用场景带来比较大的局限。1.8 版本解决了这个问题。</p><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><h3 id="完全移除-legacy-部署模式"><a href="#完全移除-legacy-部署模式" class="headerlink" title="完全移除 legacy 部署模式"></a>完全移除 legacy 部署模式</h3><p>自 1.4 版本，Flink 社区一直在重构 Flink 的 runtime 架构，并在 1.5 版本后提供了 legacy 和 new 来种运行模式。在 1.8 版本，Flink 已经完全移除掉 legacy 的 runtime 架构。详情请见 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">[5]</a>。</p><h3 id="修复-on-YARN-模式请求过多-container-的问题"><a href="#修复-on-YARN-模式请求过多-container-的问题" class="headerlink" title="修复 on YARN 模式请求过多 container 的问题"></a>修复 on YARN 模式请求过多 container 的问题</h3><p>在 1.8 版本之前，Flink 存在的一个问题是当 JobManager 向 YARN RM 请求 container 成功，但是 TaskManager 无法利用分配到的 container 启动时（比如 Kerberos 权限错误或者无法读取用于恢复作业的 Savepoint），JobManager 没有移除相应 container request。这样的话 YARN RM 一直认为 Flink 的 AM 持有这些 container，导致 Flink 占用的 container 数会随着 JobManager 不断重试而增长，最终可能吃完整个队列。Flink 1.8 修复了这个问题。</p><h3 id="资源清理问题"><a href="#资源清理问题" class="headerlink" title="资源清理问题"></a>资源清理问题</h3><p>Flink on YARN 模式通常依赖于 Zookeeper 和 HDFS 来做元数据的持久化，但是在这些资源的管理上并不是很优雅，作业运行过后偶尔会留下一些痕迹需要手动异步清理，这在大规模部署的情况下可能会造成意想不到的问题。</p><p>Flink 1.8 修复了部分这样的问题，其中包括修复 Application 提交失败后 blob server 未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-10848" target="_blank" rel="external">[6]</a>和修复作业结束后 ZK 元数据未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-11383" target="_blank" rel="external">[7]</a>，但仍有 checkpoint 目录未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-11789" target="_blank" rel="external">[8]</a>。</p><h2 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h2><h3 id="默认不再提供包含-Hadoop-的发行包"><a href="#默认不再提供包含-Hadoop-的发行包" class="headerlink" title="默认不再提供包含 Hadoop 的发行包"></a>默认不再提供包含 Hadoop 的发行包</h3><p>在之前版本为方便用户部署，Flink 在发行包中默认包括了 Hadoop，但这导致发行包十分臃肿，并且在使用率上可能并不太高，造成资源的浪费。自 Flink 1.8 开始，Apache Flink 默认不再提供包含 Hadoop 的发行包。</p><h3 id="移除-flink-storm"><a href="#移除-flink-storm" class="headerlink" title="移除 flink-storm"></a>移除 flink-storm</h3><p>在 Flink 发展的初期，为了方便用户从 Storm 迁移过来，Flink 提供了 flink-storm 的兼容包来运行用 Storm 编写的作业。但时过境迁，flink-storm 无法跟上 Flink 本身的迭代，现在 flink-storm 只能利用 Flink 的一些低级功能，而且维护成本也逐渐增加，因此社区在调查之后决定废弃掉 flink-storm 模块。</p><h3 id="flink-python-变为可选"><a href="#flink-python-变为可选" class="headerlink" title="flink-python 变为可选"></a>flink-python 变为可选</h3><p>在之前的发行版，Flink lib 默认包含 flink-dist、flink-shaded-hadoop2-uber 和 flink-python 三个 Flink 模块，然而 flink-python 的使用率似乎并不高，因此后续会将其移到 opt 下。加上移除 Hadoop shaded 包，之后 lib 下应该只有 flink-dist 和日志相关的 lib。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://flink.apache.org/news/2019/04/09/release-1.8.0.html" target="_blank" rel="external">Apache Flink 1.8.0 Release Announcement</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-8354" target="_blank" rel="external">Add KafkaDeserializationSchema that directly uses ConsumerRecord</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="external">FLIP-32: Restructure flink-table for future contributions</a></li><li><a href="https://docs.google.com/document/d/1G0NUIaaNJvT6CMrNCP6dRXGv88xNhDQqZFrQEuJ0rVU/edit#heading=h.ob9i0lcn7ulz" target="_blank" rel="external">Unified Core API for Streaming and Batch</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10848" target="_blank" rel="external">FLINK-10848: Flink’s Yarn ResourceManager can allocate too many excess containers</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11383" target="_blank" rel="external">FLINK-11383: Dispatcher does not clean up blobs of failed submissions</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11789" target="_blank" rel="external">FLINK-11789: Flink HA didn’t remove ZK metadata</a></li><li><a href="http://www.whitewood.me/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/" target="_blank" rel="external">什么是 Flink State Evolution</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;center&gt;&lt;p&gt;&lt;img src=&quot;/img/Flink-1.8.png&quot; alt=&quot;Flink 1.8&quot;&gt;&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;在距离上个 feature 版本发布近四个月之后， 近日 Apache Flink 发布了 1.8 版本。该版本处理了 420 个 issue，其中新 feature 及改进主要集中在 State、Connector 和 Table API 三者上，并 fix 了一些在生产部署中较为常见的问题。下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes &lt;a href=&quot;https://flink.apache.org/news/2019/04/09/release-1.8.0.html&quot;&gt;[1]&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 网络传输优化技术</title>
    <link href="https://link3280.github.io/2019/04/03/Flink-%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
    <id>https://link3280.github.io/2019/04/03/Flink-网络传输优化技术/</id>
    <published>2019-04-03T12:01:59.000Z</published>
    <updated>2019-04-14T13:02:28.734Z</updated>
    
    <content type="html"><![CDATA[<p>作为工业级的流计算框架，Flink 被设计为可以每天处理 TB 甚至 PB 级别的数据，所以如何高吞吐低延迟并且可靠地在算子间传输数据是一个非常重要的课题。此外，Flink 的数据传输还需要支持框架本身的特性，例如反压和用于测量延迟的 latency marker。在社区不断的迭代中，Flink 逐渐积累了一套值得研究的网络栈（Network Stack），本文将详细介绍 Flink Network Stack 的实现细节以及关键的优化技术。</p><a id="more"></a><p>本文主要基于 Nico Kruber 在去年 9 月 Flink Forward Berlin 上的分享 <a href="https://www.ververica.com/flink-forward-berlin/resources/improving-throughput-and-latency-with-flinks-network-stack" target="_blank" rel="external">[1]</a>，涉及到的技术主要有 1.5 版本引入的 Credit-based 数据流控制以及在延迟和吞吐方面做的优化。在开始之前，我们首先来回顾下 Flink 计算模型里的核心概念，这写概念会在后续被频繁地提及。</p><h2 id="Flink-计算模型"><a href="#Flink-计算模型" class="headerlink" title="Flink 计算模型"></a>Flink 计算模型</h2><p>Flink 计算模型分为逻辑层和执行层，逻辑层主要用于描述业务逻辑，而执行层则负责作业具体的分布式执行。</p><p>用户提交一个作业以后，Flink 首先在 client 端执行用户 main 函数以生成描述作业逻辑的拓扑（StreaGraph），其中 StreamGraph 的每个节点是用户定义的一个算子（Operator）。随后 Flink 对 StreamGraph 进行优化，默认将不涉及 shuffle 并且并行度相同的相邻 Operator 串联起来成为 OperatorChain 形成 JobGraph，其中的每个节点称为 Vertice，是 OperatorChain 或独立的 Operator。</p><p><center><p><img src="/img/network-stack/distributed-runtime.png" alt="图1.分布式运行时" title="图1.分布式运行时"></p></center></p><p></p><p>每个 Vertice 在执行层会被视为一个 Task，而一个 Task 对应多个 Subtask，Subtask 的数目即是用户设置的并行度。Subtask 根据 Flink 的调度策略和具体的部署环境及配置，会被分发到相同或者不同的机器或者进程上，其中有上下游依赖关系的 Subtask 会有数据传输的需要，这是通过基于 Netty 的 Network Stack 来完成的。</p><p>Network Stack 主要包括三项内容，Subtask 的输出模式（数据集是否有界、阻塞或非阻塞）、调度类型（立即调度、等待上一阶段完成和等待上一阶段有输出）和数据传输的具体实现（buffer 和 buffer timeout）。</p><p><center><p><img src="/img/network-stack/network-stack-overview.png" alt="图2.网络栈概览" title="图2.网络栈概览"></p></center></p><p></p><p>下文的内容会主要围绕数据传输部分展开，逐一介绍其中的优化技术。</p><h2 id="Credit-based-数据流控制"><a href="#Credit-based-数据流控制" class="headerlink" title="Credit-based 数据流控制"></a>Credit-based 数据流控制</h2><p>在上文图二左半部分可以看到 Subtask 之间有一条独立的数据传输管道，其实这是逻辑视图，而在物理层 Flink 并不会为维护 Subtask 级别的 TCP 连接，Flink 的 TCP 连接是 TaskManager 级别的。对于每个 Subtask 来说，根据 key 的不同它可以输出数据到下游任意的 Subtask，因此 Subtask 在内部会维护下游 Subtask 数目的发送队列，相对地，下游 Subtask 也会维护上游 Subtask 数目的接收队列。相同两个 TaskManager 上不同的 Subtask 的数据传输会通过 Netty 实现复用和分用跑在同一条 TCP 连接上。</p><p><center><p><img src="/img/network-stack/physical-view.png" alt="图3.网络传输物理视图" title="图3.网络传输物理视图"></p></center></p><p></p><p>这种实现的问题在于当某个 Subtask 出现反压时，反压不仅会作用于该 Subtask 的 Channel，还会误伤到这个 TaskManager 上的其他 Subtask，因为整个 TCP 连接都被阻塞了。比如在图 3 中，因为 Subtask 4 一个 Channel 没有空闲 Buffer，使用同一连接的其他 3 个 Channel 也无法通信。为了解决这个问题，Flink 自 1.5 版本引入了 Credit-based 数据流控制为 TCP 连接提供更加细粒度的控制。</p><p><center><p><img src="/img/network-stack/initial-channel.png" alt="图4.Channel 初始状态" title="图4.Channel 初始状态"></p></center></p><p></p><p>具体来说，在接受端的 Buffer 被划分为 Exclusive Buffer 和 Floating Buffer 两种，前者是固定分配到每条接受队列里面的，后者是在 Subtask 级别的 Buffer Pool 里供动态分配。发送队列里的数据称为 Blacklog，而接收队列里的 Buffer 称为 Credit。Credit-Based 数据流控制的核心思想则是根据接收端的空闲 Buffer 数（即 Credit）来控制发送速率，这和 TCP 的速率控制十分类似，不过是作用在应用层。</p><p>假设当前发送队列有 5 个 Blacklog，而接收队列有 2 个空闲 Credit。首先接收端会通知发送端可以发送 2 个 Buffer，这个过程称为 Announce Credit。随后发送端接收到请求后将 Channel Credit 设为 2，并发送 1 个 Buffer（随后 Channel Credit 减为 1 ），并将剩余 4 个 Backlog 的信息随着数据一起发给接收端，这个两个过程分为称为 Send Buffer 和 Announce Blacklog Size。接收端收到 Backlog Size 之后会向 Buffer Pool 申请 Buffer 以将队列拓展至可以容纳 Backlog Size 的数据，但不一定能全部拿到。因为队列目前有一个空闲 Buffer，因此只需要向 Buffer Pool 申请 3 个 Buffer。假设 3 个 Buffer 都成功申请到，它们会成为 Unannounced Credit，并在下一轮请求中被 Announce。</p><p><center><p><img src="/img/network-stack/credit-based-flow-control.png" alt="图5.Credit-based 流控制" title="图5.Credit-based 流控制"></p></center></p><p></p><p>当一条 Channel 发送端的 Announced Credit 与 接收端的 Unannounced Credit 之和不小于 Blacklog Size 时，该 Channel 处于正常状态，否则处于反压状态。</p><p>从总体上讲，Credit-based 数据流控制避免了阻塞 TCP 连接，使得资源可以更加充分地被利用，另外通过动态分配 Buffer 和拓展队列长度，可以更好地适应生产环境中的不断变化的数据分布及其带来的 Channel 状态抖动，也有利于缓减部分 Subtask 遇到错误或者处理速率降低造成的木桶效应。然而这个新的机制也会引入额外的成本，即每传输一个 Buffer 要额外一轮 Announce Credit 请求来协商资源，不过从官方的测试来看，整体性能是有显著提升的。</p><p><center><p><img src="/img/network-stack/ccfc-performance.png" alt="图6.Credit-based 流控制性能提升" title="图6.Credit-based 流控制性能提升"></p></center></p><p></p><h2 id="重构-Task-Thread-和-IO-Thread-的协作模型"><a href="#重构-Task-Thread-和-IO-Thread-的协作模型" class="headerlink" title="重构 Task Thread 和 IO Thread 的协作模型"></a>重构 Task Thread 和 IO Thread 的协作模型</h2><p>熟悉网络传输的同学应该对高吞吐和低延迟两者的 trade-off 十分熟悉。网络是以 batch 的形式来传输数据的，而每个 batch 都会带来额外的空间开销（header 等元数据）和时间开销（发送延迟、序列化反序列化延等），因此 batch size 越大则传输的开销越小，但是这也会导致延时更高，因为数据需要在缓存中等待的时间越久。对于实时类应用来说，我们通常希望延迟可以被限定在一个合理的范围内，因此业界大多数的做法是设置一个 batch timeout 来强制发送低于 batch size 的数据 batch，这通常需要额外设置设置一个线程来实现。</p><p>Flink 也不例外。在上图的 TCP 连接发送端是 Netty Server，而接收端是 Netty Client，两者都会有 event loop 不断处理网络 IO。以实时作业为例子，与 Netty 组件直接交互的是 StreamRecordWriter 和 StreamRecordReader (现已被 StreamWriter 和 StreamInputProcessor 代替)，前者负责将 Subtask 最终输出的用 StreamRecord 包装的数据序列化为字节数组并交给 Netty Server，后者负责从 Netty Client 读取数据并反序列化为 StreamRecord。</p><p><center><p><img src="/img/network-stack/record-writer.png" alt="图7.StreamRecordWriter" title="图7.StreamRecordWriter"></p></center></p><p></p><p>当发送数据时，StreamRecordWriter 将记录反序列化为字节数组，并拷贝至 Netty Server 的 Channel 的一个 Buffer 中，如果 Buffer 满了它会提醒 Netty Server 将其发送。此后 StreamRecordWriter 会重新从 BufferPool 申请一个空的 Buffer 来重复上述过程，直至作业停止。为了实现 batch timeout，Flink 设置了一个 OutputFlusher 线程，它会定时 flush 在 Channel 中的 Buffer，也就是通知 Netty Server 有新的数据需要处理。Netty Server 会在额外分配线程来读取该 Buffer 到其已写的位置并将相关内容发送，其后该未写满 Buffer 会继续停留在 Channel 中等待后续写入。</p><p><center><p><img src="/img/network-stack/output-flusher.png" alt="图8.OutputFlusher" title="图8.OutputFlusher"></p></center></p><p></p><p>这种实现主要有两个问题: 一是 OutputFlusher 和 StreamRecordWriter 主线程在 Buffer 上会有竞争条件，因此需要同步操作，当 Channel 数量很多时这会带来性能上的损耗；二是当我们需要延迟尽可能小时，会将 timeout 设为 0 (实际上提供了 flushAlways 选项)，然后每写一条记录就 flush 一次，这样会带来很高的成本，最坏的情况下会造成 Netty Server 频繁触发线程来读取输入，相当于为每个 Buffer 设置一个 event loop。一个简单的优化想法是，既然 Netty Server 本来就有 event loop，为什么不让 Netty 线程自己去检测是否有新数据呢？因此 Flink 在 1.5 版本重构了这部分的架构，弃用了要求同步的 OutputFlusher 线程，改为使用 StreamRecordWriter 和 Netty 线程间的非线程安全交互方式来提高效率，其中核心设计是 BufferBuilder 和 BufferConsumer。</p><p><center><p><img src="/img/network-stack/buffer-builder-consumer.png" alt="图9.BufferBuilder &amp; BufferConsumer" title="图9.BufferBuilder &amp; BufferConsumer"></p></center></p><p></p><p>BufferBuilder 和 BufferConsumer 以生产者消费者的模式协作，前者是会被 StreamRecordWriter 调用来写入 Buffer，后者会被 Netty Server 线程调用，两者通过 volatile int 类型的位置信息来交换信息。通过这种方式，StreamRecordWriter 不会被 OutputFlusher 阻塞，资源利用率更高，网络传输的吞吐量和延迟均可受益。</p><p><center><p><img src="/img/network-stack/rework-performance.png" alt="图10.重构前后性能对比" title="图10.重构前后性能对比"></p></center></p><p></p><h2 id="避免不必要的序列化和反序列化"><a href="#避免不必要的序列化和反序列化" class="headerlink" title="避免不必要的序列化和反序列化"></a>避免不必要的序列化和反序列化</h2><p>众所周知，序列化和反序列化是成本很高的操作，尤其是对于实时计算来说，因此 Flink 在避免不必要的序列化和反序列化方面做了不少优化工作。</p><h3 id="Object-Reuse-模式（Stream-API）"><a href="#Object-Reuse-模式（Stream-API）" class="headerlink" title="Object Reuse 模式（Stream API）"></a>Object Reuse 模式（Stream API）</h3><p>在作业拓扑优化阶段，Flink 会尽可能将多个 Operator 合并为 Operator Chain 来减少 Task 数，因为 Subtask 内的 Operator 运行在同一个线程，不需要经过网络传输。尽管 Chained Operator 之间没有网络传输，但不同 Operator 直接共享对象实例并不安全，因为对象可能同时被多个算子并发访问造成意想不到的后果，并且按照函数式编程的理念，Operator 不应该对外界造成副作用，一个典型的正面例子就是 Scala 中的 Pure Function <a href="https://hello-scala.com/405-pure-functions.html" target="_blank" rel="external">[5]</a>，因此默认情况下两个 Chained Operator 的数据对象传递是通过深拷贝来完成的，而深拷贝则是通过一轮序列化和反序列实现。不过出于性能考虑，自 Flink 提供了 <code>Object Resue Mode</code> 来关闭 Chained Operator 间的数据拷贝。</p><p><center><p><img src="/img/network-stack/object-reuse.png" alt="图11.Object Reuse Mode" title="图11.Object Reuse Mode"></p></center></p><p></p><p><code>Object Resue Mode</code> 属于高级选项，当使用 Object Reuse 时用户函数必须符合 Flink 要求的规范 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/#operating-on-data-objects-in-functions" target="_blank" rel="external">[2]</a>，比如不能将输入的数据对象存到 State 中，再比如不能在输出对象之后仍对其进行修改。</p><p>要注意的是，<code>Object Resue Mode</code> 在 Stream API 中的行为和在 Batch API 中的行为并不完全一致，前者是避免了 Chained Operator 之间的深拷贝，但不同 Subtask 之间（即使在同一 JVM 内）仍然需要深拷贝，而后者是每一步都是复用之前的对象，是真正的意义上的 Object Reuse。为此了统一 Object Reuse 在两个 API 的语义，Flink 社区提出了 FLIP-21 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=71012982" target="_blank" rel="external">[3]</a>，但由于具体方案没有达成共识目前还没有实现的计划。</p><h3 id="输出到多个-Channel-时只序列化一次"><a href="#输出到多个-Channel-时只序列化一次" class="headerlink" title="输出到多个 Channel 时只序列化一次"></a>输出到多个 Channel 时只序列化一次</h3><p>由于 Flink 维护的 RecordWriter 是 Channel 级别的，当一条数据需要被输出到多个 Channel 时（比如 broadcast），同样的数据会被序列化多次，导致性能上的浪费。因此在 1.7 版本，Flink 将 RecordWriter 的写 Buffer 操作分为将数据反序列化为字节数组和将字节数组拷贝到 Channel 里两步，从而使得多个 Channel 可以复用同一个反序列化结果 <a href="https://issues.apache.org/jira/browse/FLINK-9913" target="_blank" rel="external">[4]</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在版本迭代中，Network Stack 一直在不断改进来适应新的特性或者提高性能。其中在 1.5 版本进行了比较多的改进，包括最重要的 Credit-based 流控制和重构 Task Thread 和 IO Thread 的协作模型。</p><p>作为底层基础架构，Network Stack 设计的好坏很大程度上决定了一个计算框架的性能上限，其重要性对于 Flink 开发者或者有意贡献代码的用户而言不必多说。而对于 Flink 用户而言，熟悉 Network Stack 也可以让你在开发阶段提前预计或者部署后及时发现应用的瓶颈，从而在应对生产环境的部署复杂性时更加游刃有余。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.ververica.com/flink-forward-berlin/resources/improving-throughput-and-latency-with-flinks-network-stack" target="_blank" rel="external">Improving throughput and latency with Flink’s network stack</a><br>2.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/#operating-on-data-objects-in-functions" target="_blank" rel="external">Operating on data objects in functions</a><br>3.<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=71012982" target="_blank" rel="external">FLIP-21 - Improve object Copying/Reuse Mode for Streaming Runtime</a><br>4.<a href="https://issues.apache.org/jira/browse/FLINK-9913" target="_blank" rel="external">FLINK-9913 - Improve output serialization only once in RecordWriter</a><br>4.<a href="https://hello-scala.com/405-pure-functions.html" target="_blank" rel="external">Pure Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为工业级的流计算框架，Flink 被设计为可以每天处理 TB 甚至 PB 级别的数据，所以如何高吞吐低延迟并且可靠地在算子间传输数据是一个非常重要的课题。此外，Flink 的数据传输还需要支持框架本身的特性，例如反压和用于测量延迟的 latency marker。在社区不断的迭代中，Flink 逐渐积累了一套值得研究的网络栈（Network Stack），本文将详细介绍 Flink Network Stack 的实现细节以及关键的优化技术。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>什么是 Flink State Evolution?</title>
    <link href="https://link3280.github.io/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/"/>
    <id>https://link3280.github.io/2019/03/17/什么是-Flink-State-Evolution/</id>
    <published>2019-03-17T06:55:51.000Z</published>
    <updated>2019-04-14T13:02:35.873Z</updated>
    
    <content type="html"><![CDATA[<p>State Evolution 是 Apache Flink （下简称 Flink）1.7 版本引入的新特性，目的是为用户提供迭代或修改 State 的方法，以适应长期运行的作业的版本迭代需求，比如迁移 State 到不同的序列化框架，或者对 State 的数据结构进行改变，甚至直接对 State 的内容进行修改。该特性对于企业级应用来说有着重大的意义。</p><p>作为 Statefull 计算框架，Flink 作业的状态通常用 State API 来保存而不是存在的数据库等外部存储，这样在获得更好的一致性和数据本地性的同时，也牺牲了使用数据库的灵活性和可访问性。State 以序列化的形式（Savepoint/Checkpoint）存在，对于很多不熟悉 Flink 序列化的用户来说相当于黑盒子，难以验证其中的数据正确性，更不用说修改其数据结构或者序列化格式，而 State Evolution 则是 Flink 在 State 管理上的一次很好的探索。</p><a id="more"></a><p>根据 Gordon Tai 在 2018 年 Flink Forward Berlin 的分享[1]，State Evolution 主要分为 State Schema Migration、Savepoint Management 和 Upgradability Dry-Runs 三个部分，其中 State Schema Migration 已经在 1.7 版本支持，其余的官方支持尚在开发中。</p><h2 id="State-Schema-Migration"><a href="#State-Schema-Migration" class="headerlink" title="State Schema Migration"></a>State Schema Migration</h2><p>State Schema Migration 的目的在于使得 State 可以随作业程序更新而更新，其工作原理和 Flink 流处理作业其他类型的更新一样，都是利用 Savepoint 来迁移作业状态。作业运行时，随着计算的进行 Flink 会持续地读写本地的 State，然后在 Savepoint 的时候将本地的 State 上传至分布式的文件系统，新版本的作业启动时只需要读取 Savepoint 即可恢复原先的作业状态。</p><p><center><p><img src="/img/state-evolution/flink-job-update-1.png" alt="旧版本作业 cancel with savepoint" title="旧版本作业 cancel with savepoint"></p></center></p><p></p><p><center><p><img src="/img/state-evolution/flink-job-update-2.png" alt="新版本作业 start with savepoint" title="新版本作业 start with savepoint"></p></center></p><p></p><p>不过由于 State 涉及到不同的序列化框架，比起像改变 JobGraph 或者作业并行度等其他类型的更新，它的更新方法更加复杂一些，主要问题在于在从 Savepoint 恢复时我们如何读取之前的版本的序列化对象。这需要将 State 更新分为 State Schema 更新和 State 序列化框架的更新这两种情况考虑。</p><p>State Schema 的更新主要是和业务逻辑的变更相关，比如新增一个字段或者移除一个字段，这种版本升级主要需要考虑序列化框架的兼容性，比如 Java 默认的对象序列化框架，可以兼容新增字段但不能兼容移除或重命名字段。如果变更是可兼容的，那么无需额外操作即可迁移，否则可能需要考虑更换序列号框架。然而 Flink 默认情况下会根据 State 的 POJO 类型来生成 Adhoc 的（反）序列化器（PojoSerializer），任何 POJO 的变更都会导致不同的序列化器，因此对于 Flink 来说 State Schema 的更新通常等同于 State 序列化框架的更新。</p><p>更新 State 序列化框架的实现里有很多有意思的细节，其中一个是不同的 StateBackend 对于序列化器的使用方式不同。基于内存的 StateBackend （Heap Based StateBackend），比如 FsStateBackend，属于 “Lazy serialization, eager deserialzition”，意思是仅当 Savepoint 时才会将内存的 State 序列化，而读取 Savepoint 时会全部反序列化到内存。</p><p><center><p><img src="/img/state-evolution/heap-based-backend-1.png" alt="Heap-based StateBackend snapshot" title="Heap-based StateBackend "></p></center></p><p></p><p>举个例子，假设我们从 v1 序列化器迁移至 v2 序列化器，在 Savepoint 的时候 Heap Based StateBackend 会将所有 key 的 State 全部用 v1 序列化器写出，在作业重启恢复时序列化器已经更新为 v2，但是只要我们还可以得到 v1 的（反）序列化器就可以顺利迁移，因此一个简单的方法是每次 Savepoint 时将当前使用的序列化器也一并使用 Java 序列化来写入 Savepoint，然后在读取 Savepoint 时我们首先提取（反）序列化器，再通过这个（反）序列化器来读取 State。</p><p><center><p><img src="/img/state-evolution/heap-based-backend-2.png" alt="Heap-based StateBackend restore" title="Heap-based StateBackend restore"></p></center></p><p></p><p>然而在使用 Off Heap StateBackend 的时候，情况则变得更加复杂。因为 Off Heap StateBackend，比如 RocksDBStateBackend，是 “Eager serialization, lazy deserialization” 的，即每当 State 有更新时就会将对应 key 的 State 写到 Savepoint，而读取 Savepoint 时仅会读取被访问到的 key 的 State。</p><p><center><p><img src="/img/state-evolution/off-heap-backend-1.png" alt="Off-heap StateBackend snapshot" title="Off-heap StateBackend snapshot"></p></center></p><p></p><p>这样造成的一个问题就是在一次作业运行中，可能同时存在使用不同版本的序列化器的 State。比如在前一次运行的 Savepoint，Flink 使用 v1 序列化器写了 5 个 key 的 State，而在第二次运行时我们只更新了其中 2 个 key，那么被更新的 key 是使用 v2 序列化器，而没有被更新的 key 仍然使用 v1 序列化器。因此在使用 Off Heap StateBackend 的情况下，我们不仅需要上一次运行使用的序列化器，还需要之前所有运行使用的序列化器。</p><p><center><p><img src="/img/state-evolution/off-heap-backend-2.png" alt="Off-heap StateBackend restore" title="Off-heap StateBackend restore"></p></center></p><p></p><p>这种情况下保存每次运行时使用的序列化器显然是不现实的，因此 Flink 提供了两种办法来更新序列化器: 一是序列化器保证向后兼容，二是 State Migration Process。对于序列化器来说，向后兼容性是十分重要的特性，大部分流行的序列化器，比如 ProtoBuf、Avro、Thrift，都提供了向后兼容的能力。如果用户使用定制化的序列化器，Flink 也提供了编程 API 的支持，让用户可以根据序列化器的版本号来维护向后兼容性。具体来说，每次 Savepoint snapshot 的时候，Flink 会将序列化器的配置信息也存储下来，在作业读取 Savepoint 时用户定制的序列化器可以根据相关信息来配置自己，再反序列化 State。</p><p><center><p><img src="/img/state-evolution/serializer-compatability.png" alt="serializer backward compatabilitye" title="serializer backward compatability"></p></center></p><p></p><p>但目前来说 Flink 默认的 PojoSerializer 并不能提供这样的向后兼容性，这就需要下面的 State Migration Process。State Migration Process 即在恢复作业状态时扫描所有 key 的 State 并强制用上次运行的序列化器反序列化，从而保证使用 Off Heap StateBackend 的情况下 State 不会存在多个序列化版本。但是在 State 比较大的情况下，在作业启动时进行 State Migration Process 可能会带来很长的恢复时间，因此该方法还是需要结合实际慎重使用。</p><h2 id="Savepoint-Management"><a href="#Savepoint-Management" class="headerlink" title="Savepoint Management"></a>Savepoint Management</h2><p>不同于 State Schema Migration 在作业恢复时执行，Savepoint Management 的目的是提供离线读写 State 的能力。这种对 State 的管理能力对于生产级应用是至关重要的，因为程序 bug 是不能完全避免的，而当 bug 导致作业 State 错误时我们需要有可以修复这种错误的能力。在之前我们大多数情况下的做法是修复程序 bug 并重流数据来重新计算正确的 State，但是这种做法成本太高，甚至有时是不可行的，比如 Kafka 的消息已经被清理或者作业外部依赖的状态已经改变。而通过 Savepoint Management，只要知道如何根据错误的 State 计算得出正确的 State，我们就可以离线地修复 State 的问题并应用到线上。此外，我们甚至可以用外部的数据来计算出一个之前不存在的 State，然后用于作业首次运行时的算子状态的初始化。</p><p>截止至 Flink 1.7 版本，官方还没有这方面的支持，不过社区有个可用的项目 Bravo [2]可以一定程度上满足这方面的需求，在和 Flink 社区达成共识之后 Bravo 也会被合并到 Flink 项目里。Bravo 是由荷兰的 King 银行，Flink 的重度用户之一，开发的一个用于读写 Savepoint 的工具。其原理是利用 Flink 序列化器提供 OperatorStateReader 和 OperatorStateWriter 两个主要 API，用户可以利用它们来将 Savepoint 读入转为 Flink DataSet 或者将 DataSet 写出为 Savepoint。</p><p>State 的读取是以算子为单位的，我们需要指定 uid、State 类型、POJO 类型来定位具体的 State。一个简单的使用 Demo 如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// First we start by taking a savepoint/checkpoint of our running job...</div><div class="line">// Now it&apos;s time to load the metadata</div><div class="line">Savepoint savepoint = StateMetadataUtils.loadSavepoint(savepointPath);</div><div class="line"></div><div class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</div><div class="line"></div><div class="line">// We create a KeyedStateReader for accessing the state of the operator with the UID &quot;CountPerKey&quot;</div><div class="line">OperatorStateReader reader = new OperatorStateReader(env, savepoint, &quot;CountPerKey&quot;);</div><div class="line"></div><div class="line">// The reader now has access to all keyed states of the &quot;CountPerKey&quot; operator</div><div class="line">// We are going to read one specific value state named &quot;Count&quot;</div><div class="line">// The DataSet contains the key-value tuples from our state</div><div class="line">DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; countState = reader.readKeyedStates(</div><div class="line">KeyedStateReader.forValueStateKVPairs(&quot;Count&quot;, new TypeHint&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;&#125;));</div></pre></td></tr></table></figure><p>Bravo 是基于 Flink 1.5 版本开发的，目前已经支持 1.6 版本引入的 State TTL，不过用户还是比较少，建议大家去尝试下（Bravo 维护者 Gyula Fora 非常 nice，提 issue 很快可以得到解决）。</p><h2 id="Upgradability-Dry-Runs"><a href="#Upgradability-Dry-Runs" class="headerlink" title="Upgradability Dry-Runs"></a>Upgradability Dry-Runs</h2><p>State Evolution 还提供的一项很方便的功能是 Upgradability Dry-Runs，这项功能用于离线检查作业的版本兼容性，以帮助用户提前发现兼容性问题。常见的兼容性问题主要有:</p><ul><li>作业拓扑的变更。这可能会导致算子 uid 的匹配失败，进而导致作业恢复后状态的不完整。目前来说 Flink 默认会在作业提交时进行安全性检查，用户可以通过 <code>-n,--allowNonRestoredState</code> 参数来允许不完整的状态恢复，但是这种检查应该测试阶段而不是在部署阶段完成。</li><li>State Schema 的变更。如上文所说，这会导致新版本的作业恢复失败并需要马上回滚。</li></ul><p>目前来说社区还没有这方面的讨论和开发计划，但 Gordon 分享了一些可行的办法。离线验证的关键点在于从用户作业程序提取出需要的信息，并对比找出其中不兼容的点。对于作业拓扑图来说，我们可以计算出两者的 StreamGraph 并比较它们的 uid。而对于 State Schema 来说则可能绕一点，因为按照 Flink 目前的设计注册 State 的过程是只对 StateBackend 可见，因此可能需要入侵 StateBackend 来支持兼容性检查。而另外一个办法则是引入新的 annotation 并在声明 State 的时候标记这个 State，这样我们就可以在解析用户代码时得出 State 的信息，并用于检测兼容性。这种功能暂时被命名为 Eager State Declaration，具体的一个 Demo 如下。</p><p><center><p><img src="/img/state-evolution/eager-state-declaration.png" alt="eager-state-declaration" title="eager state declaration"></p></center></p><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，State Evolution 是关注于 State 如何管理的一系列特性，用户可以利用这些特性来获得在 State 版本演进或者类似传统数据库的 CRUD 上的更强的控制能力和更大操作空间，以更好地维护长期运行并不断迭代的作业。另外，从 1.7 版本开始我们可以看到 Flink 社区越来越重视版本兼容和跨版本迁移，包括 State Evolution 和 REST 等 API 的版本化，这也是一个项目走向成熟和准备好企业级生产的标志之一。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.ververica.com/flink-forward-berlin/resources/upgrading-apache-flink-applications-state-of-the-union" target="_blank" rel="external">Upgrading Apache Flink Applications: State of the Union</a><br>2.<a href="https://github.com/king/bravo" target="_blank" rel="external">Bravo: Utilities for processing Flink checkpoints/savepoints</a><br>3.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/schema_evolution.html" target="_blank" rel="external">Apache Flink 官方文档: State Schema Evolution</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;State Evolution 是 Apache Flink （下简称 Flink）1.7 版本引入的新特性，目的是为用户提供迭代或修改 State 的方法，以适应长期运行的作业的版本迭代需求，比如迁移 State 到不同的序列化框架，或者对 State 的数据结构进行改变，甚至直接对 State 的内容进行修改。该特性对于企业级应用来说有着重大的意义。&lt;/p&gt;
&lt;p&gt;作为 Statefull 计算框架，Flink 作业的状态通常用 State API 来保存而不是存在的数据库等外部存储，这样在获得更好的一致性和数据本地性的同时，也牺牲了使用数据库的灵活性和可访问性。State 以序列化的形式（Savepoint/Checkpoint）存在，对于很多不熟悉 Flink 序列化的用户来说相当于黑盒子，难以验证其中的数据正确性，更不用说修改其数据结构或者序列化格式，而 State Evolution 则是 Flink 在 State 管理上的一次很好的探索。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入理解流计算中的 Watermark</title>
    <link href="https://link3280.github.io/2019/02/24/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84-Watermark/"/>
    <id>https://link3280.github.io/2019/02/24/深入理解流计算中的-Watermark/</id>
    <published>2019-02-24T08:47:39.000Z</published>
    <updated>2019-04-14T13:02:44.833Z</updated>
    
    <content type="html"><![CDATA[<p>近年来流计算技术发展迅猛，甚至有后来居上一统原本批处理主导的分布式计算之势，其中 Watermark 机制作为流计算结果准确性和延迟的桥梁扮演着不可或缺的角色。然而由于缺乏高质量的学习资源加上计算 Watermark 确实不是一件容易的事情，不少有着批处理计算背景的用户在流计算作业的开发中可能并不理解 Watermark 的重要意义，从而多走了很多弯路。为此，本文将基于笔者的学习积累和开发经验，谈谈个人对 Watermark 的理解，希望起到抛砖引玉的作用。</p><a id="more"></a><p>本文将首先说明 Watermark 提出的背景，然后详细解析 Watermark 的原理，最后结合工业案例说明 Watermark 在实践中如何被应用。</p><h1 id="Watermark-背景"><a href="#Watermark-背景" class="headerlink" title="Watermark 背景"></a>Watermark 背景</h1><p>自 Google 的三篇论文和 Hadoop 出现后，工业界的分布式计算技术进入了百花齐放的时期，然而相比于离线批处理计算的蓬勃发展，作为后来者的流计算却有点停滞不前。流计算和批处理在对于每条记录的单独处理上基本一致，不同之处在于聚合类的计算。批处理计算结果的输出依赖于输入数据集合的结束，而流计算的输入数据集通常是无边界的，不可能等待输入结束再输出结果。针对这个问题流处理引入了窗口的特性，简单来说就是将无限的数据流按照时间范围切分为一个个有限的数据集，所以我们依然能够沿用批处理的计算模型。来到这时，业界在流计算和批处理的关系上出现了两种截然不同的观点，一个观点认为流计算是批处理的特例，另一个观点则认为批处理是流处理的特例。</p><h2 id="实时计算与离线计算的分离"><a href="#实时计算与离线计算的分离" class="headerlink" title="实时计算与离线计算的分离"></a>实时计算与离线计算的分离</h2><p>流计算是批处理的特例的观点在早期占据了主导的地位，其中最为典型的便是以 Spark Streaming 为代表的 micro-batching 类型的实时处理框架的流行。Micro-batching 的主要思想是以分钟甚至秒级别的执行间隔来将批处理应用到数据流上，但不久后人们意识到这种计算模型依然不能完全满足低延迟高准确性的要求，主要问题除了批处理调度导致的延迟外，还有一点是窗口变小后，数据收集延迟对结果准确性的影响大大增强了。比如说计算一个游戏服务器每 5 min 的新登录玩家数，但因为网络或者客户端设备故障等因素，12:00 的玩家登录日志可能在 12:10 才被收集到服务器，如果实时计算在 12:05:00 就输出结果，必然会漏掉这条迟到的数据。在离线计算中这样的问题并不明显，因为一个批次的时间跨度较大且对延迟要求不高，因此计算的时间可以设置一个安全的延迟，比如 1 个小时，确保数据都已经收集完成后再开始计算，即使有大量数据是在 1 个小时后才收集到，只需要重算结果即可。然而这样的实践经验并不能应用于实时计算，一是引入额外的安全延迟对于很多对延迟敏感的场景不可接受，二是实时计算的重算要比批处理重算的成本高出很多。因此业界普遍是采用 结合离线和实时处理的 Lambda 架构来应对这个问题，其主要思想是同时运行实时和离线两个数据处理管道，实时管道提供最近小时内的临时结算结果，而离线管道提供小时以前的计算结果并覆盖掉对应时间段的实时计算结果，查询时将两者的结果再进行合并产生最终的结果<a href="">[1]</a>。</p><h2 id="实时计算与离线计算的融合"><a href="#实时计算与离线计算的融合" class="headerlink" title="实时计算与离线计算的融合"></a>实时计算与离线计算的融合</h2><p>实时计算与离线计算的分离说明了用批处理模型不足以表达流计算，于是人们开始探索批处理是流计算特例的模型。2015 年 Google 发表名为 The Dataflow Model 的论文，这篇论文较为详细地阐述了实时流计算和离线批计算的统一模型（出于篇幅原因不展开讲，详情请见<a href="">[2]</a>），而该模型基于批处理是流计算特例的观点。The Dataflow Model 将计算分为四个要素，即 what、where、when 和 how:</p><ul><li>what 表示要计算什么结果，即对数据的一系列转换操作；</li><li>where 表示结果计算上下文，即窗口如何定义；</li><li>when 表示何时输出和物化计算结果；</li><li>how 表示如何清理已经输出的结果。</li></ul><p>在 what 和 where 两点上流计算和批处理是相似的，而主要不同之处在于 when 和 how 两点，这两点在批处理里基本不会涉及，但在流计算里却影响着计算结果的准确性，实际上它们分别对应了上文所说的批处理经验不能应用于实时计算的两个问题。本文主要讨论的 watermark 就是属于 when 要素里的一种技术，因而下文将主要关注 when。</p><p>在批处理中 when 是输入数据集结束的时候，how 是以覆盖的形式来清理之前的输出结果，处理模式都是固定的，因此用户并不需要考虑。举个例子，假设要计算一个游戏每天的玩家充值金额，用离线计算时我们会考虑如何将充值金额从日志中提取出来并累加到一起，此为 what；再考虑批处理的运行时间，比如每天 00:30，所以每次计算是处理 24 小时采集到的数据，此为 where；而批处理的 when 是和 where 绑定的，即 00:30 计算开始，结束后马上输出结果；至于 how，不同批次的批处理运行的结果是互不相干的，同一批次的运行结果会覆盖前一次运行的结果。</p><p>然而如果游戏策划急于知道某个活动是否有带动玩家充值，希望看到每分钟更新的实时数据，那么上述题目改为用实时流计算去实现，此时要考虑的东西会复杂一点。首先，我们可以依旧可以复用批处理的 what 和 where，即定义一个时间范围为 24 小时的窗口，计算逻辑和之前一样；在 when 方面，为了可以实时地得到最新的计算结果，我们需要定义每分钟输出一次最新的计算结果，直到达到 24 小时后输出最终结果；而在 how 方面，我们每次的输出结果只需要覆盖之前的结果即可。然而 when 的问题并没有这么简单。还记得我们之前说过数据采集延迟吗？可能一个用户充值的时间在 16:00，但中间采集的延迟可能有 1 min，导致到达服务器却是 16:01 分，如果基于充值记录被处理的时间（即 processing time）来进行窗口划分，用户充值记录可能会被计入错误的窗口，所以我们应该以用户充值这个时间（即 event time）发生的时间为准。这里的难点在于我们计算时并不能判断所有 event time 窗口内的数据被收集完，因为数据的延迟是不可预知的，这被称为窗口完整性问题。针对窗口完整性问题，The Dataflow Model 提出了 Watermark 的解决方案。</p><h1 id="Watermark-原理解析"><a href="#Watermark-原理解析" class="headerlink" title="Watermark 原理解析"></a>Watermark 原理解析</h1><p>Watermark 并没有很正式的官方定义，最接近定义的是 Streaming 102<a href="">[3]</a> 里的一段描述。</p><blockquote><p>A watermark is a notion of input completeness with respect to event times. A watermark with a value of time X makes the statement: “all input data with event times less than X have been observed.” As such, watermarks act as a metric of progress when observing an unbounded data source with no known end.                                                                    </p></blockquote><p>简单来说 Watermark 是一个时间戳，表示已经收集完毕的数据的最大 event time，换句话说 event time 小于 Watermark 的数据不应该再出现，基于这个前提我们才有可能将 event time 窗口视为完整并输出结果。Watermark 设计的初衷是处理 event time 和 processing time 之间的延迟问题，三者的关系可以用下图展示:</p><center><p><img src="/img/streaming-watermark/streaming-system-watermark.png" alt="图 1. Event Time/Processing Time/Watermark 三者关系" title="图 1. Event Time/Processing Time/Watermark 三者关系"></p></center><p>理想的情况下数据没有延迟，因此 processing time 是等于 event time 的，理想的 Watermark 应该是斜率为 45 度的直线。然而在真实环境下，processing time 和 event time 之间总有不确定的延迟，表现出来的 Watermark 会类似图 1 中的红色的曲线。其中红色曲线与理想 Watermark 的纵坐标差值称为 processing-time lag，表示在真实世界中的数据延迟，而横坐标的差值表示 event-time skew，表示该延迟带来的 event-time 落后量。</p><p>Watermark 通常是基于已经观察到的数据的 event time 来判断（当然也可以引入 processing time 或者其他外部参数），具体需要用户根据数据流的 event time 特征来决定，比如最简单的算法就是取目前为止观察到的最大 event time。在数据流真实 event time 曲线是单调非减的情况下，比如 event time 是 Kafka producer timestamp 时，我们是可以计算出完美符合实际的 Watermark 的，然而绝大多数情况下数据流的 event time 都是乱序的，因此计算完美的 Watermark 是不现实的（实际上也是没有必要的），通常我们会以启发性的 Watermark 算法来代替。</p><p>启发性的 Watermark 算法目的在于在计算结果的延迟和准确性之间找到平衡点。如果采用激进的 Watermark 算法，那么 Watermark 会快于真实的 event time，导致在窗口数据还不完整的情况下过早输地出计算结果，影响数据的准确性；如果采用保守的 Watermark 算法，那么 Watermark 会落后于真实的 event time，导致窗口数据收集完整后不能及时输出计算结果，造成数据的延迟。实际上上文所说的 Watermark 取观察到的最大 event time 和批处理使用的设置一个足够大的安全延迟的办法分别就属于 Watermark 算法的两个极端。很多情况下用户偏向于牺牲一定的延时来换取准确性，不过在像金融行业的欺诈检测场景中，低延迟是首要的，否则准确性再高也没有意义。针对这种情况 The Dataflow Model 提供了 allow lateness 的机制，工作的原理是用户可以设置一个时间阈值，如果在计算结果输出后的这个阈值时间内发现迟到的数据，计算结果会被重新计算和输出，但如果超出这个阈值的迟到数据就会被丢弃。</p><p>这时你们可以看到要开发一个高质量的实时作业是多么不易了，这也是很多实时应用开发者最为头疼的地方，或许以后利用机器学习去计算 Watermark 是个不错的主意（然后我们的工作就可以愉快地从调 Watermark 算法参数变为调机器学习模型参数了 :) ）。</p><h1 id="Watermark-实践"><a href="#Watermark-实践" class="headerlink" title="Watermark 实践"></a>Watermark 实践</h1><p>接下来我们将结合工业生产的案例来说明实战中 Watermark 是如何影响流计算的。Watermark 在不同计算引擎的实现并不相同，本文将以笔者使用最多的 Apache Flink （下文简称 Flink）作为例子来说明。</p><p>对于游戏行业来说，游戏的日活跃玩家数是个很常见的指标，游戏策划或者运营通常可以根据日活跃玩家数的变动来实时地监控某个活动是否收到玩家欢迎的程度，但是游戏可能有海外服务器，数据收集的延迟可能差别较大，造成数据流 event time 乱序比较严重，在这种情况下设计 Watermark 算法是个比较大的挑战。</p><p>假设我们有 A、B、C 共 3 台服务器，其中 A、B 为国内服务器，延迟较低且稳定，而 C 为海外服务器，延迟较高且不稳定，而我们需要计算每分钟内的登录玩家数。</p><center><p><img src="/img/streaming-watermark/data-latency-of-servers.png" alt="图 2. 数据流延迟" title="图 2. 数据流延迟"></p></center><p>我们现在面临两种可能带来 event time 乱序的因素：一是不同服务器间的延迟不同，比如可能先收到服务器 A 在 t2 的数据，再收到服务 C 在 t1 的数据；二是同一服务器的不同数据的延迟不同，比如可能先收到服务器 C t2 的数据再收到 t1 的数据。针对第二种因素，我们可以对不同服务器的数据分别计算 Watermark，再取其中的最小值作为 Watermark，而针对第一种因素，我们则需要设计出针对单个服务器数据流的合理 Watermark 算法。</p><p>在算法实现上，Flink 提供两种触发 Watermark 更新的方法，即在收到特殊的消息时触发或者定时触发，我们这里将选用定时触发的方法。因为窗口是一分钟比较小，我们这里将定时的间隔设为 5 秒，也就是说 Watermark 大约落后真实 Watermark 5 秒，然后这 5 秒内 Watermark 是不会提升的，所以可以容忍局部的 processing lag。</p><p>我们试着取目前为止观察到的最大时间戳作为 Watermark，那么 Watermark 的效果如下(为了在消费端更加直观，我们将坐标系调转，现在 x 轴表示 processing time）。</p><center><p><img src="/img/streaming-watermark/watermark-implemented.png" alt="图 3. Watermark 算法实现" title="图 3. Watermark 算法实现"></p></center><p>其中 t0-t3 分别表示 Watermark 提升的时间点，黄虚线表示在一个 Watermark 周期内的最大 event time，红线表示 Watermark。可以看到在 t0-t1 的 Watermark 周期内出现了轻微的 event time 乱序，但是并不影响计算的准确性。接下来在 t1-t2 和 t2-t3 两个周期间也发生了相似的乱序，但是这个乱序并不在同一个 Watermark 周期，因此导致正常延迟的数据被误认为是迟到数据。解决方法是引入一定可容忍的 event time skew，比如说最简单的设置一个 skew 阈值，即每次计算 Watermark 的结果都减去这个值。根据数据流延迟的不同，我们还可以给不同服务器设置不同的 skew 阈值。</p><p>上述 Watermark 算法代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">public class WatermarkProcessor implements AssignerWithPeriodicWatermarks&lt;UserLogin&gt; &#123;</div><div class="line"></div><div class="line">    private static final long ALLOWED_EVENT_TIME_SKEW = 1000L;</div><div class="line">    private static final Map&lt;String, Long&gt; maxTimestampPerServer = new HashMap&lt;&gt;(3);</div><div class="line"></div><div class="line">    @Nullable</div><div class="line">    public Watermark getCurrentWatermark() &#123;</div><div class="line">        Optional&lt;Long&gt; maxTimestamp  = maxTimestampPerServer.values().stream()</div><div class="line">                .min(Comparator.comparingLong(Long::valueOf));</div><div class="line">        if (maxTimestamp.isPresent()) &#123;</div><div class="line">            return new Watermark(maxTimestamp.get() - ALLOWED_EVENT_TIME_SKEW);</div><div class="line">        &#125; else&#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public long extractTimestamp(UserLogin userLogin, long previousElementTimestamp) &#123;</div><div class="line">        String server = userLogin.getServer();</div><div class="line">        long eventTime = userLogin.getEventTime();</div><div class="line">        if (!maxTimestampPerServer.containsKey(server) || </div><div class="line">                userLogin.getEventTime() &gt; maxTimestampPerServer.get(server)) &#123;</div><div class="line">            maxTimestampPerServer.put(server, eventTime);</div><div class="line">        &#125;</div><div class="line">        return eventTime;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>流计算和批处理谁是表达能力更强的计算模式，这个问题或许还将继续被争论下去，不过根据 The Dataflow Model 我们已经有足够的理论支撑来开发低延迟高准确并且可容错的流计算应用。其中流计算的准确性很大程度上决定于数据流时间的乱序程度，因此我们在开发实时流计算应用时，比起开发离线批处理应用，很大的一个不同是要考虑数据是以什么顺序到达，并针对性地设计 Watermark 算法来处理数据流时间的乱序。Watermark 算法需要平衡低延迟和高准确性两者，在引入最低延迟成本的情况下准确判断窗口的计算和输出结果的时机，通常可以从 processing lag 和 event time skew 两者的容忍阈值入手。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html" target="_blank" rel="external">How to beat the CAP theorem</a><br>2.<a href="https://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf" target="_blank" rel="external">The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive Scale, Unbounded, Out of Order Data Processing</a><br>3.<a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102" target="_blank" rel="external">Streaming 102: The world beyond batch</a><br>4.Tyler AkidauSlava, Chernyak, Reuven Lax. (2018). Streaming Systems.  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近年来流计算技术发展迅猛，甚至有后来居上一统原本批处理主导的分布式计算之势，其中 Watermark 机制作为流计算结果准确性和延迟的桥梁扮演着不可或缺的角色。然而由于缺乏高质量的学习资源加上计算 Watermark 确实不是一件容易的事情，不少有着批处理计算背景的用户在流计算作业的开发中可能并不理解 Watermark 的重要意义，从而多走了很多弯路。为此，本文将基于笔者的学习积累和开发经验，谈谈个人对 Watermark 的理解，希望起到抛砖引玉的作用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="实时计算" scheme="https://link3280.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://link3280.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink on YARN Security 浅析(Flink Part)</title>
    <link href="https://link3280.github.io/2019/01/06/Flink-on-YARN-Security-%E6%B5%85%E6%9E%90-Flink-Part/"/>
    <id>https://link3280.github.io/2019/01/06/Flink-on-YARN-Security-浅析-Flink-Part/</id>
    <published>2019-01-06T13:05:28.000Z</published>
    <updated>2019-04-14T13:03:26.497Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇关于 YARN 系统 Security 的博客，我们解析了通过 YARN 提供的 Security API，Application 已经在 RM 注册并且可以顺利地申请到 container，但 YARN 对 container 后续的凭证刷新（reacquire）并不能作用到已经在运行的 Application 进程，因此对于长期运行的 Application 而言要开发者自己实现认证和后续凭证刷新的逻辑。本文将接着分析 Flink 如何在申请到的 container 启动 jobmanger 和 taskmanager 并完成认证，也就是 Flink Application 自身的认证，以及后续的凭证刷新方法，最后再讲述最近社区对于 Flink on YARN Security 改进提案。</p><a id="more"></a><h2 id="长时间运行的-YARN-Application-的四种认证方法"><a href="#长时间运行的-YARN-Application-的四种认证方法" class="headerlink" title="长时间运行的 YARN Application 的四种认证方法"></a>长时间运行的 YARN Application 的四种认证方法</h2><h3 id="在-YARN-集群机器上预安装-keytab"><a href="#在-YARN-集群机器上预安装-keytab" class="headerlink" title="在 YARN 集群机器上预安装 keytab"></a>在 YARN 集群机器上预安装 keytab</h3><p>将 Application 可能会用到的 keytab 预先安装到 YARN 集群的所有机器的本地文件系统并设置好目录权限，然后将相关路径作为作为 Application 的配置提供给 AM 和其他普通 container。用户进程启动时通过 <code>UserGroupInformation.loginUserFromKeytab()</code> 来加载凭证和认证，并且后续通过 keytab 来刷新 kerberos 凭证。具体的安全凭证分布如图一所示，其中实线表示永久凭证 keytab 的传递，虚线表示临时凭证 token 的传递。</p><center><p><img src="/img/flink-on-yarn-security/Approach1-pre-installed-keytab.png" alt="图一. pre-installed-keytab" title="图一. pre-installed-keytab"></p></center><p>这种方式直接使用 Kerberos keytab ，绕开了 Hadoop delegation token，相当于 后者只用于 container 的申请。因此这样的优点是避开了 token 最大生命周期的问题，而缺点在于没有了 delegation token 的优点，即每次 TGT 刷新需要请求 KDC，而且 keytab 也需要比较高的运维成本。</p><h3 id="通过-YARN-分发-keytab-给-AM-和其他-container"><a href="#通过-YARN-分发-keytab-给-AM-和其他-container" class="headerlink" title="通过 YARN 分发 keytab 给 AM 和其他 container"></a>通过 YARN 分发 keytab 给 AM 和其他 container</h3><p>首先将 Application 客户端将 keytab 上传至 HDFS，并在提交 Application 时将其作为AM 需要本地化的资源。AM container 初始化时 NodeManager 会负责将 keytab 拷贝至 container 的资源目录，AM 启动时通过 <code>UserGroupInformation.loginUserFromKeytab()</code> 来重新认证。当 AM 需要申请 container 时，也将 HDFS 上的 keytab 列为需要本地化的资源，因此 container 也可以仿照 AM 进行认证。此外 AM 和 container 都必须额外实现一个线程来定时刷新 Kerberos TGT。</p><center><p><img src="/img/flink-on-yarn-security/Approach2-distribute-via-YARN.png" alt="图二. distribute-via-yarn" title="图二. distribute-via-yarn"></p></center><p>Apache Flink 目前使用的正是这种方法。比起第一种方式，优点在于 keytab 只需要被安装在 client 端，YARN 集群上的机器只在有一个用户的作业在运行时才会有该用户的 keytab，作业完成后 keytab 也会随 container 被清理掉。</p><h3 id="通过-YARN-分发-keytab-给-AM；AM-为-container-生成-delegation-token"><a href="#通过-YARN-分发-keytab-给-AM；AM-为-container-生成-delegation-token" class="headerlink" title="通过 YARN 分发 keytab 给 AM；AM 为 container 生成 delegation token"></a>通过 YARN 分发 keytab 给 AM；AM 为 container 生成 delegation token</h3><p>从 client 上传 keytab 到 AM 获得 keytab 的流程都与第二种方法相同，区别在于后续 AM 申请 container 时并不是将 keytab 列本地化资源，而是请求 container 需要的 delegation token（比如最基础的 HDFS_DELEGATION_TOKEN）并将这些 token 作为 ContainerLaunchContext 的安全凭证。由于 token 的最大生命周期问题，container 后续需要继续从 AM 获取新的 token，实现方式通常是 AM 和 container 通过 IPC 来定时更新 token。</p><center><p><img src="/img/flink-on-yarn-security/Approach3-am-generates-token.png" alt="图三. am-generates-token" title="图三. am-generates-token"></p></center><p>这种方式比起前两种方式的好处在于 keytab 只存在于 AM 机器上似乎更加安全，不过由于 container 和 AM 相同的 HDFS 访问权限，实际上它们还是可以访问到 keytab，除非 AM 启动后将 keytab 从 HDFS 删除，不过这样在 AM 崩溃的情况下 YARN 就没有办法进行重试了。目前 Apache Spark 是使用这种方式认证。</p><h3 id="Client-端推送-token-给-AM；AM-推送-token-给-container"><a href="#Client-端推送-token-给-AM；AM-推送-token-给-container" class="headerlink" title="Client 端推送 token 给 AM；AM 推送 token 给 container"></a>Client 端推送 token 给 AM；AM 推送 token 给 container</h3><p>如果将 YARN 集群视为不受信任的环境，严格限制将 keytab 分发到集群上，在 client 端推送 token 会是唯一的方式，即通过 Hadoop delegation token 启动 AM 和 container，随后 client 定时用 keytab 认证并重新获取 AM 的 token 并通过 IPC 的方式传递给它，AM 再通过 IPC 将 token 传递给 container。</p><center><p><img src="/img/flink-on-yarn-security/Approach4-client-generates-token.png" alt="图四. client-generates-token" title="图四. client-generates-token"></p></center><p>这种方式把 keytab 限制在 client 机器上最为安全，但是 client 端的实现比较重，对于作业数成千上万的部署规模来说并不合适，而且 client 会成为一个故障单点。如果要一条路走到黑，部署多个 client 和高可用，那就相当于又一个安全的小分布式集群。</p><h2 id="Flink-Application-认证源码解析"><a href="#Flink-Application-认证源码解析" class="headerlink" title="Flink Application 认证源码解析"></a>Flink Application 认证源码解析</h2><p>如上文所说，Flink 使用通过 YARN 分发 keytab 给 AM 和其他 container 的方式来认证，下面从源码角度来解析具体的实现，这会分为 Flink client 、AM（YarnClusterEntrypoint） 和 container（YarnTaskExecutorRunner）三个部分，源码以 1.6.2 版本为例。</p><h3 id="Flink-Client"><a href="#Flink-Client" class="headerlink" title="Flink Client"></a>Flink Client</h3><p>篇幅起见这里只给出核心代码和以注释形式说明，完整代码见 AbstractYarnClusterDescriptor#startAppMaster。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">// 从配置读取 keytab 路径，若不为空则注册为 container 的 LocalResource</div><div class="line">Path remotePathKeytab = null;</div><div class="line">String keytab = configuration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);</div><div class="line">if (keytab != null) &#123;</div><div class="line">LOG.info(&quot;Adding keytab &#123;&#125; to the AM container local resource bucket&quot;, keytab);</div><div class="line">remotePathKeytab = setupSingleLocalResource(</div><div class="line">Utils.KEYTAB_FILE_NAME,</div><div class="line">fs,</div><div class="line">appId,</div><div class="line">new Path(keytab),</div><div class="line">localResources,</div><div class="line">homeDir,</div><div class="line">&quot;&quot;);</div><div class="line">&#125;</div><div class="line">// 根据配置和环境变量生成 AM 的启动命令，并设置到 AM ContainerLaunchContext 里</div><div class="line">final ContainerLaunchContext amContainer = setupApplicationMasterContainer(</div><div class="line">yarnClusterEntrypoint,</div><div class="line">hasLogback,</div><div class="line">hasLog4j,</div><div class="line">hasKrb5,</div><div class="line">clusterSpecification.getMasterMemoryMB());</div><div class="line"></div><div class="line">// 如果是 Kerberized 集群，向 NameNode 申请 HDFS_DELEGATION_TOKEN，并设置到 AM ContainerLaunchContext 里</div><div class="line">if (UserGroupInformation.isSecurityEnabled()) &#123;</div><div class="line">// set HDFS delegation tokens when security is enabled</div><div class="line">LOG.info(&quot;Adding delegation token to the AM container..&quot;);</div><div class="line">Utils.setTokensFor(amContainer, paths, yarnConfiguration);</div><div class="line">&#125;</div><div class="line"></div><div class="line">amContainer.setLocalResources(localResources);</div></pre></td></tr></table></figure><p>其中 Utils 类包含了许多 helper method，获取 token 的方法 #setTokensFor 源码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">public static void setTokensFor(ContainerLaunchContext amContainer, List&lt;Path&gt; paths, Configuration conf) throws IOException &#123;</div><div class="line">Credentials credentials = new Credentials();</div><div class="line">// for HDFS</div><div class="line">TokenCache.obtainTokensForNamenodes(credentials, paths.toArray(new Path[0]), conf);</div><div class="line">// for HBase</div><div class="line">obtainTokenForHBase(credentials, conf);</div><div class="line">// for user</div><div class="line">UserGroupInformation currUsr = UserGroupInformation.getCurrentUser();</div><div class="line"></div><div class="line">Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt; usrTok = currUsr.getTokens();</div><div class="line">for (Token&lt;? extends TokenIdentifier&gt; token : usrTok) &#123;</div><div class="line">final Text id = new Text(token.getIdentifier());</div><div class="line">LOG.info(&quot;Adding user token &quot; + id + &quot; with &quot; + token);</div><div class="line">credentials.addToken(id, token);</div><div class="line">&#125;</div><div class="line">try (DataOutputBuffer dob = new DataOutputBuffer()) &#123;</div><div class="line">credentials.writeTokenStorageToStream(dob);</div><div class="line"></div><div class="line">if (LOG.isDebugEnabled()) &#123;</div><div class="line">LOG.debug(&quot;Wrote tokens. Credentials buffer length: &quot; + dob.getLength());</div><div class="line">&#125;</div><div class="line"></div><div class="line">ByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());</div><div class="line">amContainer.setTokens(securityTokens);</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="YarnClusterEntrypoint"><a href="#YarnClusterEntrypoint" class="headerlink" title="YarnClusterEntrypoint"></a>YarnClusterEntrypoint</h3><p>YarnClusterEntrypoint 是 Flink 的 AM 主类，它在启动时会先初始化 SecurityContext。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">protected void startCluster() &#123;</div><div class="line">LOG.info(&quot;Starting &#123;&#125;.&quot;, getClass().getSimpleName());</div><div class="line"></div><div class="line">try &#123;</div><div class="line">configureFileSystems(configuration);</div><div class="line"></div><div class="line">SecurityContext securityContext = installSecurityContext(configuration);</div><div class="line"></div><div class="line">securityContext.runSecured((Callable&lt;Void&gt;) () -&gt; &#123;</div><div class="line">runCluster(configuration);</div><div class="line"></div><div class="line">return null;</div><div class="line">&#125;);</div><div class="line">&#125; catch (Throwable t) &#123;</div><div class="line">LOG.error(&quot;Cluster initialization failed.&quot;, t);</div><div class="line"></div><div class="line">shutDownAndTerminate(</div><div class="line">STARTUP_FAILURE_RETURN_CODE,</div><div class="line">ApplicationStatus.FAILED,</div><div class="line">t.getMessage(),</div><div class="line">false);</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>而 #installSecurityContext 方法通过 SecurityUtils 逐个调用了安全模块的认证，其中最重要的一个就是 HadoopModule 。HadoopModule 的核心代码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">String keytabPath = (new File(securityConfig.getKeytab())).getAbsolutePath();</div><div class="line"></div><div class="line">// 用 keytab 认证</div><div class="line">UserGroupInformation.loginUserFromKeytab(securityConfig.getPrincipal(), keytabPath);</div><div class="line"></div><div class="line">loginUser = UserGroupInformation.getLoginUser();</div><div class="line"></div><div class="line">// supplement with any available tokens</div><div class="line">String fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);</div><div class="line">if (fileLocation != null) &#123;</div><div class="line"></div><div class="line">try &#123;</div><div class="line">Method readTokenStorageFileMethod = Credentials.class.getMethod(&quot;readTokenStorageFile&quot;,</div><div class="line">File.class, org.apache.hadoop.conf.Configuration.class);</div><div class="line">Credentials cred =</div><div class="line">(Credentials) readTokenStorageFileMethod.invoke(</div><div class="line">null,</div><div class="line">new File(fileLocation),</div><div class="line">hadoopConfiguration);</div><div class="line"></div><div class="line">// 由于 Hadoop 认证机制更偏好 delegation token，使用 kerberos keytab 认证时需要过滤掉 delegation token</div><div class="line">// 以下代码从 container 的本地 token 文件读取 token ，过滤掉 HDFS_DELEGATION_TOKEN 后覆盖掉原本的 container credentail</div><div class="line">Method getAllTokensMethod = Credentials.class.getMethod(&quot;getAllTokens&quot;);</div><div class="line">Credentials credentials = new Credentials();</div><div class="line">final Text hdfsDelegationTokenKind = new Text(&quot;HDFS_DELEGATION_TOKEN&quot;);</div><div class="line">Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt; usrTok = (Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt;) getAllTokensMethod.invoke(cred);</div><div class="line">//If UGI use keytab for login, do not load HDFS delegation token.</div><div class="line">for (Token&lt;? extends TokenIdentifier&gt; token : usrTok) &#123;</div><div class="line">if (!token.getKind().equals(hdfsDelegationTokenKind)) &#123;</div><div class="line">final Text id = new Text(token.getIdentifier());</div><div class="line">credentials.addToken(id, token);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Method addCredentialsMethod = UserGroupInformation.class.getMethod(&quot;addCredentials&quot;,</div><div class="line">Credentials.class);</div><div class="line">addCredentialsMethod.invoke(loginUser, credentials);</div><div class="line">&#125; catch (NoSuchMethodException e) &#123;</div><div class="line">LOG.warn(&quot;Could not find method implementations in the shaded jar. Exception: &#123;&#125;&quot;, e);</div><div class="line">&#125; catch (InvocationTargetException e) &#123;</div><div class="line">throw e.getTargetException();</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>此外 AM 还要负责申请 container 时设置好 container 的安全凭证，具体可见 Utils#createTaskExecutorContext，以下是核心代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">// 从 AM container 的本地 token 文件读取 token 并设置给 taskmanager container </div><div class="line">// 其中这里有个问题是连同 AMRMToken 一齐传递给了 taskmanager，而 AMRMToken 顾名思义应该只有 AM 有权限使用，详见 [FLINK-11126](https://issues.apache.org/jira/browse/FLINK-11126)</div><div class="line">final String fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);</div><div class="line"></div><div class="line">if (fileLocation != null) &#123;</div><div class="line">log.debug(&quot;Adding security tokens to TaskExecutor&apos;s container launch context.&quot;);</div><div class="line"></div><div class="line">try (DataOutputBuffer dob = new DataOutputBuffer()) &#123;</div><div class="line">Method readTokenStorageFileMethod = Credentials.class.getMethod(</div><div class="line">&quot;readTokenStorageFile&quot;, File.class, org.apache.hadoop.conf.Configuration.class);</div><div class="line"></div><div class="line">Credentials cred =</div><div class="line">(Credentials) readTokenStorageFileMethod.invoke(</div><div class="line">null,</div><div class="line">new File(fileLocation),</div><div class="line">HadoopUtils.getHadoopConfiguration(flinkConfig));</div><div class="line"></div><div class="line">cred.writeTokenStorageToStream(dob);</div><div class="line">ByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());</div><div class="line">ctx.setTokens(securityTokens);</div><div class="line">&#125; catch (Throwable t) &#123;</div><div class="line">log.error(&quot;Failed to add Hadoop&apos;s security tokens.&quot;, t);</div><div class="line">&#125;</div><div class="line">&#125; else &#123;</div><div class="line">log.info(&quot;Could not set security tokens because Hadoop&apos;s token file location is unknown.&quot;);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="YarnTaskExecutorRunner"><a href="#YarnTaskExecutorRunner" class="headerlink" title="YarnTaskExecutorRunner"></a>YarnTaskExecutorRunner</h3><p>YarnTaskExecutorRunner 是 Flink TaskManager 的主类，它初始化时的认证过程和 YarnClusterEntrypoint 相似，都是调用各个 SecurityModule 来认证。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">String keytabPath = null;</div><div class="line">if (remoteKeytabPath != null) &#123;</div><div class="line">File f = new File(currDir, Utils.KEYTAB_FILE_NAME);</div><div class="line">keytabPath = f.getAbsolutePath();</div><div class="line">LOG.info(&quot;keytab path: &#123;&#125;&quot;, keytabPath);</div><div class="line">&#125;</div><div class="line"></div><div class="line">UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();</div><div class="line"></div><div class="line">LOG.info(&quot;YARN daemon is running as: &#123;&#125; Yarn client user obtainer: &#123;&#125;&quot;,</div><div class="line">currentUser.getShortUserName(), yarnClientUsername);</div><div class="line"></div><div class="line">if (keytabPath != null &amp;&amp; remoteKeytabPrincipal != null) &#123;</div><div class="line">configuration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);</div><div class="line">configuration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);</div><div class="line">&#125;</div><div class="line"></div><div class="line">SecurityConfiguration sc = new SecurityConfiguration(configuration);</div><div class="line"></div><div class="line">final String containerId = ENV.get(YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID);</div><div class="line">Preconditions.checkArgument(containerId != null,</div><div class="line">&quot;ContainerId variable %s not set&quot;, YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID);</div><div class="line"></div><div class="line">// use the hostname passed by job manager</div><div class="line">final String taskExecutorHostname = ENV.get(YarnResourceManager.ENV_FLINK_NODE_ID);</div><div class="line">if (taskExecutorHostname != null) &#123;</div><div class="line">configuration.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, taskExecutorHostname);</div><div class="line">&#125;</div><div class="line"></div><div class="line">SecurityUtils.install(sc);</div><div class="line"></div><div class="line">SecurityUtils.getInstalledContext().runSecured(new Callable&lt;Void&gt;() &#123;</div><div class="line">@Override</div><div class="line">public Void call() throws Exception &#123;</div><div class="line">TaskManagerRunner.runTaskManager(configuration, new ResourceID(containerId));</div><div class="line">return null;</div><div class="line">&#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure><h2 id="社区的-Flink-on-YARN-Security-改进提案"><a href="#社区的-Flink-on-YARN-Security-改进提案" class="headerlink" title="社区的 Flink on YARN Security 改进提案"></a>社区的 Flink on YARN Security 改进提案</h2><p>Uber 是 Apache Flink 的重度用户，而且由于技术栈和架构的关系 Uber 对于 Security 这块的积累比较多，最近（2108 年年底） Uber 的工程师向社区提出了 Security 的新改进方案<a href="https://docs.google.com/document/d/1rBLCpyQKg6Ld2P0DEgv4VIOMTwv4sitd7h7P5r202IE/edit#heading=h.vcblrwijpe4n" target="_blank" rel="external">[3]</a>。虽然这个改进方案目前还没有得到广泛的关注（毕竟没有很多人熟悉安全模块）并且部分列举的场景比较小众，但其中的一些设计还是很有参考意义。</p><p>这个提案包含两个部分: 1.支持多种 YARN Application 认证方式；2.支持超级用户伪装为普通用户（impersonation）。下面将分点讲述。</p><h3 id="支持多种-YARN-Application-认证方式"><a href="#支持多种-YARN-Application-认证方式" class="headerlink" title="支持多种 YARN Application 认证方式"></a>支持多种 YARN Application 认证方式</h3><p>该提案建议将 client 端、AM 端和普通 container 端的认证方法通过 CredentialFactory 来解耦，因此用户可以灵活定制三者的认证方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">/** </div><div class="line"> * Kerberos Credential Factory</div><div class="line"> */</div><div class="line">public interface CredentialFactory &#123;</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on client side. Setup environment properties which will be shipped</div><div class="line">   * to the cluster side.</div><div class="line">   */</div><div class="line">  Map&lt;String, String&gt; createEnvCredentialProperties(Configuration conf);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on client side. Setup environment credential by creating file cache or</div><div class="line">   * validate provided credentials are accessible on cluster side.</div><div class="line">   */</div><div class="line">  void prepareEnvCrediential(Configuration conf, FileSystem fs);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on application master. Derive &#123;@link SecurityConfiguration&#125;, used by</div><div class="line">   * &#123;@link SecureUtils&#125; and &#123;@link SecureModule&#125; to install credentials.</div><div class="line">   */</div><div class="line">  SecurityConfiguration prepareApplicationMasterCredentials(Map&lt;String, String&gt; envProperties);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on task executor. Derive &#123;@link SecurityConfiguration&#125;, used by</div><div class="line">   * &#123;@link SecureUtils&#125; and &#123;@link SecureModule&#125; to install credentials.</div><div class="line">   */</div><div class="line">  SecurityConfiguration prepareTaskExecutorCredentials(Map&lt;String, String&gt; envProperties);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Checker function to ensure proper credentials are configured within the </div><div class="line">   * current cluster environment.</div><div class="line">   */</div><div class="line">  boolean isCredentialEnvConfigured(Map&lt;String, String&gt; envProperties);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>默认情况下提供除了 client 生成 token 外的三种常见认证策略。</p><h3 id="支持超级用户-Impersonation"><a href="#支持超级用户-Impersonation" class="headerlink" title="支持超级用户 Impersonation"></a>支持超级用户 Impersonation</h3><p>目前来说其实 Flink 的 Impersonation 需要通过 client 操作系统来实现（主要是因为 keytab 的权限是 600），比如 root 用户 su 为 joe 用户再使用 joe 用户的名义提交作业，这种方法会造成潜在的权限滥用。该提案提出我们可以使用 Hadoop 的 proxy user API<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html" target="_blank" rel="external">[4]</a>来实现 Flink 层面的 Impersonation 以避免这个问题。</p><p>要实现超级用户 Impersonation 首先要设置一个用户，比如 flink，为 Hadoop 的超级用户，这样他就可以申请到其他用户的 delagtion token，因此 flink 用户不需要访问一般用户joe 的 keytab 就可以通过 delegation token 来提交作业。后续 Flink 需要使用上述 Application 认证方式的第三或者第四种来持续刷新 delegation tokem，所以提案第一点支持多种 YARN Application 认证方式也是第二点 Impersonation 的前置条件。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YARN 原本不是为长时间运行的实时应用设计的，尤其是在安全认证这块尤为明显，因此各种实时计算框架的 on YARN 认证都是神仙过海各显神通，没有统一的方式。Flink on YARN Security 目前已经比较稳定但是仍然不够灵活，导致在其上构建平台的企业级用户多少有些束手束脚的感觉。Uber 的提案是目前看上去是比较合理的，期待 Uber 这个提案可以顺利实现并被社区接受。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">Hadoop 官方文档: YARN Security</a><br>2.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/security-kerberos.html" target="_blank" rel="external">Flink 官方文档: Kerberos Authentication Setup and Configuration</a><br>3.<a href="https://docs.google.com/document/d/1rBLCpyQKg6Ld2P0DEgv4VIOMTwv4sitd7h7P5r202IE/edit#heading=h.vcblrwijpe4n" target="_blank" rel="external">Flink Kerberos Improvement Design</a><br>4.<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html" target="_blank" rel="external">Hadoop 官方文档: Proxy User</a><br>5.<a href="https://docs.google.com/document/d/10V7LiNlUJKeKZ58mkR7oVv1t6BrC6TZi3FGf2Dm6-i8/edit#heading=h.jxspj25an0dn" target="_blank" rel="external">Flink Security Improvements</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇关于 YARN 系统 Security 的博客，我们解析了通过 YARN 提供的 Security API，Application 已经在 RM 注册并且可以顺利地申请到 container，但 YARN 对 container 后续的凭证刷新（reacquire）并不能作用到已经在运行的 Application 进程，因此对于长期运行的 Application 而言要开发者自己实现认证和后续凭证刷新的逻辑。本文将接着分析 Flink 如何在申请到的 container 启动 jobmanger 和 taskmanager 并完成认证，也就是 Flink Application 自身的认证，以及后续的凭证刷新方法，最后再讲述最近社区对于 Flink on YARN Security 改进提案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink on YARN Security 浅析 (YARN Part)</title>
    <link href="https://link3280.github.io/2018/12/23/Flink-on-YARN-Security-%E6%B5%85%E6%9E%90-YARN-Part/"/>
    <id>https://link3280.github.io/2018/12/23/Flink-on-YARN-Security-浅析-YARN-Part/</id>
    <published>2018-12-23T02:32:43.000Z</published>
    <updated>2019-04-14T13:03:37.015Z</updated>
    
    <content type="html"><![CDATA[<p>自我们将作业迁移到 Kerberized 的 YARN 新集群后，运行中的作业有一定几率出现出现认证失败，其中失败有 AMRMToken 过期导致 AM 无法与 RM 通信而被 kill 掉和 AM 申请的新 container 启动时报缺少 credential 两种表现。出现了这个问题后只能重启作业解决，这严重影响了平台的稳定性，因此笔者前后投入了接近两周的时间去研究这个问题，最后定位到问题在于 RM 刷新 viewfs token 时只会刷新第一个挂载的 FS 的 token。另一方面，笔者对于 YARN Security 和 Flink 与 YARN 在安全机制上的集成已经有了一定的积累，所以先记录并分享出来，以便后续有需要时检索（retrive）和与其他同学讨论交流。</p><a id="more"></a><p>下文将先解释 Hadoop delegation token 的作用和种类，然后介绍 YARN Application 是如何应用这些 token 的，最后再解析 Flink 如何使用 YARN 的 API 来完成安全认证。值得注意的是，YARN 框架的系统级认证和 Application 的用户级认证是有所不同的。前者是 YARN 如何代表用户完成系统级的工作（下载所需资源、以用户身份启动进程等），更侧重于 YARN，后者是用户应用自身的安全操作（以什么身份访问 Hive、Hbase 等)，更侧重于 Application（Flink）。本文将主要关注前者，而关于 Flink on YARN 应用安全机制的内容将留到下一篇博客。</p><h2 id="Hadoop-Delegation-Token"><a href="#Hadoop-Delegation-Token" class="headerlink" title="Hadoop Delegation Token"></a>Hadoop Delegation Token</h2><p>关于 Hadoop delegation token 的背景和实现，Cloudera 有一篇很棒博客<a href="https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/" target="_blank" rel="external">[1]</a>，这里简单总结下。</p><p>Delegatin token 是 Hadoop 生态圈广泛使用的一种认证机制，包括 HDFS、YARN、HBase 等都支持 delegation token。Delegation token 机制基于 Kerberos，类似于 Kerberos TGT （Ticket Granting Ticket）。区别在于 TGT 的 renewal 需要请求 KDC 完成，是个三方协议，而 delegation token 的 renewal 只涉及到用户和服务，是个两方协议，这样的好处是避免了集群上所有机器初始化作业或后续 renew TGT 对 KDC 造成的巨大请求量。另外由于 delegation token 是服务级别并且由相应的服务管理，delegation token 的泄漏只会造成对应服务在 token 有效期（3-7天）内的不安全，相比分发 keytab 或者 TGT 到机器上更加安全。</p><center><p><img src="/img/hadoop-authentication.png" alt="Hadoop 安全认证" title="Hadoop 安全认证"></p></center><p>Delegation token 的工作方式是（见图一）: 用户先通过 Kerberos 认证并向需要访问的服务（比如 NameNode、ResourceManager）申请 delegation token，在提交作业时将这些 token 一并附上，然后作业被分发到集群机器上后自动通过附带的 token 来认证。</p><p>在 Hadoop 内部 delegation token 有很多种，HDFS 使用的 token 主要是 HDFS delegation token，YARN 使用的 token 可参见董西成的一篇博客<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">[2]</a>（版本是较老的 Hadoop 2.0 ，但 2.x 系列变动不大）。下文会主要围绕开发 YARN application 涉及到的最基础的两个 token，即 HDFS delegation token 和 AMRMToken 展开。</p><h2 id="YARN-Application-的认证方式"><a href="#YARN-Application-的认证方式" class="headerlink" title="YARN Application 的认证方式"></a>YARN Application 的认证方式</h2><p>YARN Security 在官网<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">[3]</a>有详细的描述，这里结合我们的应用场景简单总结下。</p><h3 id="Application-首次运行的认证"><a href="#Application-首次运行的认证" class="headerlink" title="Application 首次运行的认证"></a>Application 首次运行的认证</h3><p>在提交作业时，YARN client 需要获取作业运行时用到的所有 delegation token 并设置到 ApplicationSubmissionContext 里面。其中的 token 可以分为两类，一是系统 token，包括 HDFS delegation token 和 AMRMToken，二是用户 token，包括访问 HBase 或其他服务的 token。两者的区别在于系统 token 是 YARN 内部使用并负责管理的，而用户 token 则是可选的。</p><p>HDFS delegation token 是 NameNode 赋予的访问 HDFS 的凭证。YARN Client 提交作业前需要先向 NameNode 申请 HDFS delegation token，将作业所需资源上传到 HDFS，然后将 token 设置到 ApplicationSubmissionContext 里。对于 YARN 来说 HDFS delegation token 主要用在两个地方: log aggregation 和 container localization。log aggregation 是指收集用户作业日志上传到 HDFS；container localization 是指在 container 启动前将用户指定的资源从 HDFS 上下载到本地。两者都发生在 container 启动前，但 log aggregation 的失败不会影响 container 的启动，而 localization 则是必须的。</p><p>AMRMToken 是 ApplicationMaster 与 ResourceManager 通信的凭证。YARN Client 提交作业后会收到 ResourceManager 生成的 AMRMToken，这个 token 会被设置到 ApplicationSubmissionContext 里面传递给 AM 用于后续的通信。此后每次 AM 和 RM 的心跳或者其他通信，AM 都要附上 AMRMToken。</p><h3 id="Application-后续运行的凭证刷新"><a href="#Application-后续运行的凭证刷新" class="headerlink" title="Application 后续运行的凭证刷新"></a>Application 后续运行的凭证刷新</h3><p>虽然首次运行没有问题，但随着作业运行时间变长 token 会失效，因此需要相应的 token 刷新机制来保证作业认证凭证的有效性。HDFS delegation token 和 AMRMToken 均属于系统 token，因此都由 YARN 来刷新。</p><p>对于 HDFS delegation token，YARN 在 [YARN-2704]<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">[2]</a> 引入了 proxy-user 的功能，即 YARN RM 可以代替用户刷新 HDFS delegation token。这里的刷新有两种方式，一是 renew，即在 HDFS delegation token 失效前向 RM 续订（默认 1 天），延长 token 的有效期；二是当 token 达到最大生命周期（默认 7 天）后不能再被续订，此时需要申请一个新的 token 来代替旧 token。</p><p>对于 AMRMToken，RM 会每天刷新一次，刷新的方式是更新用于生成 AMRMToken 的 master key。在准备启用新 master key 的一个窗口内（默认 15 分钟），当 RM 发现 AM 心跳使用的是旧 master key，就会返回新的 master key 给 AM 通知它更新。过了这个更新窗口，AM 再使用旧 master key 生成的 token 通信，就会得到 Invalid Token 的报错。</p><h2 id="Flink-on-YARN-作业提交时认证"><a href="#Flink-on-YARN-作业提交时认证" class="headerlink" title="Flink on YARN 作业提交时认证"></a>Flink on YARN 作业提交时认证</h2><center><p><img src="/img/flink-job-submission.png" alt="Flink on YARN 作业提交流程" title="Flink on YARN 作业提交流程"></p></center><p>Flink on YARN 的 Application 提交流程如下<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="external">[4]</a>:</p><ol><li>Flink client（同时也是 YARN client）会先通过 Kerberos 认证，向 Namenode 申请 HDFS delegation token 并上传作业依赖到 HDFS。  </li><li>Flink client 将相应的资源和 token 设置到 ApplicationSubmissionContext 里面，提交 Application。</li><li>YARN RM 收到提交作业请求后记录下 Application 对应的 token ，并分配一个 container 启动 ApplicationMaster，其中包括了 Flink JobManager。  </li><li>JobManager 启动后会将自己的 HDFS delegation token 和 TaskManager 启动的资源设置到 ContainerLaunchContext 里面，然后提交给 YARN RM。RM 收到请求后分配 container token 给 AM，AM 再凭借这个 token 向 NodeManager 要求并启动 container。</li></ol><p>其中每个 container （包括 AM container 和一般 container）启动前 RM 会将这个 Application 的 token 发送给 NodeManager，因此 NodeManager 能以用户的身份完成 container 启动的前置工作以及最终启动 container。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YARN Security 的设计思想在于用户在 Client 端先通过 Kerberos 认证获取基本服务(YARN、HDFS)的 delegation token，并作为 Application Submission Context 的凭证提交给 RM。如果 Application 可以成功运行则证明了用户凭证的有效性，token 后续的刷新和重新获取都由 RM 来负责。不过新 token 仅限 YARN 内部使用，用户应用本身的 token 失效问题仍需要用户自己处理。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/" target="_blank" rel="external">Hadoop Delegation Tokens Explained</a><br>2.<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">Hadoop 2.0（YARN）中的安全机制概述</a><br>3.<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">Apache Hadoop 官方文档: YARN Security</a><br>4.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="external">Apache Flink 官方文档: on YARN 模式</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自我们将作业迁移到 Kerberized 的 YARN 新集群后，运行中的作业有一定几率出现出现认证失败，其中失败有 AMRMToken 过期导致 AM 无法与 RM 通信而被 kill 掉和 AM 申请的新 container 启动时报缺少 credential 两种表现。出现了这个问题后只能重启作业解决，这严重影响了平台的稳定性，因此笔者前后投入了接近两周的时间去研究这个问题，最后定位到问题在于 RM 刷新 viewfs token 时只会刷新第一个挂载的 FS 的 token。另一方面，笔者对于 YARN Security 和 Flink 与 YARN 在安全机制上的集成已经有了一定的积累，所以先记录并分享出来，以便后续有需要时检索（retrive）和与其他同学讨论交流。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="YARN" scheme="https://link3280.github.io/tags/YARN/"/>
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.7 Release 解读</title>
    <link href="https://link3280.github.io/2018/12/02/Flink-1-7-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2018/12/02/Flink-1-7-Release-解读/</id>
    <published>2018-12-02T05:04:43.000Z</published>
    <updated>2019-04-14T13:03:47.216Z</updated>
    
    <content type="html"><![CDATA[<center><p><img src="/img/Flink-1.7-release-announcement-blog-1-768x591.png" alt="Flink 1.7 Release"></p></center><p>自 feature freezed 以后经过近一个月的努力，Flink 社区在十一月的最后一天终于发布 Flink 1.7.0 版本。该版本处理了 420 个 issue，其中新特性或者改进主要集中在 SQL 和 State 两个模块上，另外从 1.7 开始更多的 API （包括 REST、State 还有正在讨论的 runtime）会考虑版本兼容性，以便用户更重度地依赖 Flink 做上层的开发。</p><a id="more"></a><p>下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12343585" target="_blank" rel="external">1</a>。</p><h2 id="SQL-模块"><a href="#SQL-模块" class="headerlink" title="SQL 模块"></a>SQL 模块</h2><h3 id="CEP-与-SQL-的集成"><a href="#CEP-与-SQL-的集成" class="headerlink" title="CEP 与 SQL 的集成"></a>CEP 与 SQL 的集成</h3><p>CEP (Complex Event Processing) 是 Flink 常见的使用场景，其 DSL 语法与 SQL 类似但仍需要以编程 API 的方式调用。因此在 SQL 里支持 CEP 将大大降低使用门槛，对于用户来说仅需学习一个新的 SQL 函数。在 2016 年年底国际标准组织(IOS)发布了 <a href="https://standards.iso.org/ittf/PubliclyAvailableStandards/c065143_ISO_IEC_TR_19075-5_2016.zip" target="_blank" rel="external">Row Pattern Recognition in SQL [3]</a>，这为在 SQL 中实现 CEP 奠定了基础。 </p><p>Flink CEP 的 SQL 支持是基于 Apache Calcite 的新函数 <code>MATCH_RECOGNIZE</code> 实现的，具体语法见下面的案例。场景是检测某个商品最近一次连续降价的开始时间和结束时间，给定输入为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// 输入</div><div class="line">symbol         rowtime         price    tax</div><div class="line">======  ====================  ======= =======</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:00&apos;   12      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:01&apos;   17      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:02&apos;   19      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:03&apos;   21      3</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:04&apos;   25      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:05&apos;   18      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:06&apos;   15      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:07&apos;   14      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:08&apos;   24      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:09&apos;   25      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:10&apos;   19      1</div></pre></td></tr></table></figure><p>CEP SQL 语句如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">SELECT *</div><div class="line">FROM Ticker</div><div class="line">MATCH_RECOGNIZE (</div><div class="line">    PARTITION BY symbol</div><div class="line">    ORDER BY rowtime</div><div class="line">    MEASURES</div><div class="line">        START_ROW.rowtime AS start_tstamp,</div><div class="line">        LAST(PRICE_DOWN.rowtime) AS bottom_tstamp,</div><div class="line">        LAST(PRICE_UP.rowtime) AS end_tstamp</div><div class="line">    ONE ROW PER MATCH</div><div class="line">    AFTER MATCH SKIP TO LAST PRICE_UP</div><div class="line">    PATTERN (START_ROW PRICE_DOWN+ PRICE_UP)</div><div class="line">    DEFINE</div><div class="line">        PRICE_DOWN AS</div><div class="line">            (LAST(PRICE_DOWN.price, 1) IS NULL AND PRICE_DOWN.price &lt; START_ROW.price) OR</div><div class="line">                PRICE_DOWN.price &lt; LAST(PRICE_DOWN.price, 1),</div><div class="line">        PRICE_UP AS</div><div class="line">            PRICE_UP.price &gt; LAST(PRICE_DOWN.price, 1)</div><div class="line">    ) MR;</div></pre></td></tr></table></figure><p>SQL 语句具体的执行顺序如下: </p><ol><li>依据 <code>PARTITION BY</code> 和 <code>ORDER BY</code> 语句将 <code>MATCH_RECOGNIZE</code> 语句的输入数据按 Key 逻辑切分和排序。</li><li>用 <code>PATTERN</code> 语句定义模式序列，语法与正则表达式一致。</li><li>根据 <code>DEFINE</code> 语句的条件将一行数据映射为模式变量，供后续使用。</li><li>根据 <code>MEASURES</code> 语句将模式变量进行表达式计算转换为最终数据结果。</li></ol><p>上述案例的对应输出如下，分别是(商品标识, 降价开始时间, 最低价时间, 价格回升时间):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// 输出</div><div class="line"> symbol       start_tstamp       bottom_tstamp         end_tstamp</div><div class="line">=========  ==================  ==================  ==================</div><div class="line">ACME       01-APR-11 10:00:04  01-APR-11 10:00:07  01-APR-11 10:00:08</div></pre></td></tr></table></figure><h3 id="Streaming-SQL-支持-Temporal-Tables-和-Temporal-Joins"><a href="#Streaming-SQL-支持-Temporal-Tables-和-Temporal-Joins" class="headerlink" title="Streaming SQL 支持 Temporal Tables 和 Temporal Joins"></a>Streaming SQL 支持 Temporal Tables 和 Temporal Joins</h3><p>Temporal Table 是 Flink 1.7 版本引入的新特性，简单来说它是有时间版本的 Streaming Table，可以用于基于时间版本的 Join（Temporal Joins）。</p><p>在之前的版本，Streaming SQL 中任何对 Table 的更新都会触发 Join 结果的更新，这意味着我们无法反映历史版本数据。举个例子，我们需要将订单中的价格按实时汇率转换为本地货币，其中订单数据来自是一个只会追加（append-only）的交易订单数据流，货币汇率是依据实时更新事件的数据流。此前如果直接将两个数据流 join 起来，当汇率进行变化时，之前订单的本地货币金额也会随之更新，这显然是不符合需求的。</p><p>为了实现支持时间版本的 Join，我们需要保存其中一个（维度）表的全部时间版本，然后根据事件数据流的时间决定具体使用哪个一个版本的数值。这体现在 SQL 上新增了一个 UDF API 来定义 Temporal Table，这需要配合 <code>LATERAL TABLE</code> 使用。</p><p>首先需要在 Table API 注册目标表并调用 API 创建针对该表的 UDF:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">// 创建汇率历史表</div><div class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; ratesHistoryStream = env.fromCollection(ratesHistoryData);</div><div class="line">Table ratesHistory = tEnv.fromDataStream(ratesHistoryStream, &quot;r_currency, r_rate, r_proctime.proctime&quot;);</div><div class="line"></div><div class="line">tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory);</div><div class="line"></div><div class="line">// 创建并注册该表的 temporal table function</div><div class="line">// 将 `r_proctime` 作为时间字段，`r_currency` 作为主键</div><div class="line">TemporalTableFunction rates = ratesHistory.createTemporalTableFunction(&quot;r_proctime&quot;, &quot;r_currency&quot;); // &lt;==== (1)</div><div class="line">tEnv.registerFunction(&quot;Rates&quot;, rates);</div></pre></td></tr></table></figure><p>然后可以在 SQL 里使用 <code>Rates</code> UDF：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">SELECT</div><div class="line">  SUM(o.amount * r.rate) AS amount</div><div class="line">FROM Orders AS o</div><div class="line">LATERAL JOIN (TABLE Rates(o.rowtime)) AS r</div><div class="line">ON r.currency = o.currency;</div></pre></td></tr></table></figure><h3 id="新增-SQL-内置函数"><a href="#新增-SQL-内置函数" class="headerlink" title="新增 SQL 内置函数"></a>新增 SQL 内置函数</h3><table><thead><tr><th>函数名</th><th>描述</th></tr></thead><tbody><tr><td>TO_BASE64(string)</td><td>Returns the base64-encoded result from string; returns NULL if string is NULL.</td></tr><tr><td>LOG2(numeric)</td><td>Returns the base 2 logarithm of numeric.</td></tr><tr><td>LTRIM(string)</td><td>Returns a string that removes the left whitespaces from string.</td></tr><tr><td>RTRIM(string)</td><td>Returns a string that removes the right whitespaces from string.</td></tr><tr><td>REPEAT(string, integer)</td><td>Returns a string that repeats the base string integer times.</td></tr><tr><td>REPLACE(string1, string2, string3)</td><td>Returns a new string which replaces all the occurrences of string2 with string3 (non-overlapping) from string1.</td></tr><tr><td>COSH(numeric)</td><td>Returns the hyperbolic cosine of NUMERIC.</td></tr><tr><td>SINH(numeric)</td><td>Returns the hyperbolic sine of numeric.</td></tr><tr><td>TANH(numeric)</td><td>Returns the hyperbolic tangent of numeric.</td></tr></tbody></table><h3 id="SQL-Client-优化"><a href="#SQL-Client-优化" class="headerlink" title="SQL Client 优化"></a>SQL Client 优化</h3><p>SQL Client 是 Flink 1.5.0 引入的实验性特性，目的是为用户提供执行探索性查询的 SQL shell，而在 Flink 1.7.0 版本中 SQL Client 得到进一步的改进。Flink SQL 一直缺乏 DDL 的实现（目前正在开发中），定义表是通过启动 SQL Client 时指定的配置文件来完成，因此创建数据源一直是个比较繁琐的操作，尤其是在探索性的场景中，用户一般需要经常变换数据源或者表格式。不过现在虽然仍不支持在 SQL Client Session 中创建表，但可以用视图(View)来缓解这个问题。SQL Client 可以在配置文件或者 Session 中创建虚拟视图。视图会立即被解析和校验，但是”懒执行”的，即知道被调用输出结果的时候才会真正被计算。</p><h2 id="State-模块"><a href="#State-模块" class="headerlink" title="State 模块"></a>State 模块</h2><h3 id="State-Evolution"><a href="#State-Evolution" class="headerlink" title="State Evolution"></a>State Evolution</h3><p>对于 Stateful 作业来说 State 是极为重要的信息，用户一般会期望可以在程序升级、State Schema 变化的情况下，原有的 State 仍可以向后兼容。原先的 State Schema 变化的兼容性需要依赖序列化器本身提供，换句话说我们是不能直接将 State 迁移到一个不兼容的序列号器上的，而是需要经过以下的步骤:</p><ol><li>用旧反序列化器读取 State。</li><li>实现一个“迁移 map 函数”将每个 State 对象转换为新序列化器格式。</li><li>用新序列化器将 State 写回外部存储。</li></ol><p>State Evolution 的目的在于封装整个流程，只暴露简单接口给用户。因为步骤 2 根据不同的序列号器/反序列化器不同而不同，所以目前限制了 State Evolution 只对方便灵活改变 Schema 的 Avro 类型的 State 有效，不过之后会拓展到更多的 State 类型。</p><h3 id="Local-Recovery"><a href="#Local-Recovery" class="headerlink" title="Local Recovery"></a>Local Recovery</h3><p>Flink 1.7.0 加入了 State 的 Local Recovery 特性，即每次 checkpoint 时 TaskManager 都会本地保存一份本地的 State，当作业重启恢复时 Flink 调度器会优先考虑将 Task State 的本地性，这样可以减少网络流量提高恢复的效率，对于并行度较高的作业来说是十分重要的。</p><p>值得注意的是在 1.7.0 版本 Flink 的 failover strategy 因为重大的 bug 被暂时移除了<a href="RestartPipelinedRegionStrategy does not restore state">[6]</a>。因此基本所有的 Flink 都是全局恢复模式，一个 task 的失败会导致全部 task 的重新调度，Local Recovery 在这种场景下可以大大减少恢复时长和减缓网络压力。</p><h3 id="修复-Cancel-with-Savepoint-连接过早关闭的问题"><a href="#修复-Cancel-with-Savepoint-连接过早关闭的问题" class="headerlink" title="修复 Cancel with Savepoint 连接过早关闭的问题"></a>修复 Cancel with Savepoint 连接过早关闭的问题</h3><p>这是由于在 per-job cluster 模式下，cancel with savepoint 会导致 cluster 在取消作业后马上关闭，但这是 Flink Client 可能还处在轮询 savepoint 路径的过程中。这会导致 <code>java.net.ConnectException</code>，并且 Client 端无法得到 savepoint 的路径。这也是我们在做 Flink 作业平台时遇到的一个问题，并在内部分支的 1.5.3 版本和 1.6.0 版本用 <code>closeTimeWait</code> 的方式来简单修复。Flink 1.7 的实现会更加优雅: 用 CompletableFuture 确保 REST Server 在处理完所有的 pending 请求后再关闭，以保证 Client 在 Cluster 关闭前可以轮询到 Savepoint 路径。详情可见 [FLINK-10309]<a href="https://issues.apache.org/jira/browse/FLINK-10309" target="_blank" rel="external">[8]</a></p><h2 id="Connector-模块"><a href="#Connector-模块" class="headerlink" title="Connector 模块"></a>Connector 模块</h2><h3 id="Exactly-once-S3-StreamingFileSink"><a href="#Exactly-once-S3-StreamingFileSink" class="headerlink" title="Exactly-once S3 StreamingFileSink"></a>Exactly-once S3 StreamingFileSink</h3><p>自 Flink 1.6 引入的 StreamingFileSink 现在新增了 S3 的 exactly-once 支持。此前用户需要使用 BucketingFileSink 来写 S3，而 BucketingFileSink 并不支持 Flink FileSystem，因此许多 Flink 层面的支持都没有办法使用，而现在用户可以安全地迁移到 StreamingFileSink 上。</p><h3 id="Kafka-2-0-Connector"><a href="#Kafka-2-0-Connector" class="headerlink" title="Kafka 2.0 Connector"></a>Kafka 2.0 Connector</h3><p>Flink 1.7 将抛弃掉针对每个版本 Kafka 定制 connector 的模式，转而提供新的 Kafka connector (modern Kafka connector) 来支持 1.0+ 版本的 Kafka <a href="https://blog.csdn.net/yanghua_kobe/article/details/83210897" target="_blank" rel="external">[7]</a>。新 connector 由腾讯的工程师团队贡献，对不同版本的 Kafka Client 实现了一个统一的 Facade，可以自动适配不同版本的 Kafka broker。不过由于兼容性问题，1.0 以下版本的 Kafka client 并不在支持列表内，而对应的 connector 也将一直保留。</p><h2 id="Scala-2-12-支持"><a href="#Scala-2-12-支持" class="headerlink" title="Scala 2.12 支持"></a>Scala 2.12 支持</h2><p>Flink 1.7 全面支持了 Scala 2.12，但由于 Scala 2.12 与 Scala 2.11 的语法有不兼容，所以部分 Public API 的一致性会被破坏，迁移到新版本的 Scala 用户要注意。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12343585" target="_blank" rel="external">Flink 1.7.0 Release Notes</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-20%3A+Integration+of+SQL+and+CEP" target="_blank" rel="external">FLIP-20: Integration of SQL and CEP</a></li><li><a href="https://standards.iso.org/ittf/PubliclyAvailableStandards/c065143_ISO_IEC_TR_19075-5_2016.zip" target="_blank" rel="external">Row Pattern Recognition in SQL</a></li><li><a href="https://docs.google.com/document/d/1KaAkPZjWFeu-ffrC9FhYuxE6CIKsatHTTxyrxSBR8Sk/edit#heading=h.96en6mr8aklf" target="_blank" rel="external">Enrichment joins with Table Version Functions in Flink</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/schema_evolution.html" target="_blank" rel="external">State Schema Evolution</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10712" target="_blank" rel="external">FLINK-10712 RestartPipelinedRegionStrategy does not restore state</a></li><li><a href="https://blog.csdn.net/yanghua_kobe/article/details/83210897" target="_blank" rel="external">Flink即将在1.7版本发布全新的Kafka连接器</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10309" target="_blank" rel="external">FLINK-10309 Cancel with savepoint fails with java.net.ConnectException when using the per job-mode</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;p&gt;&lt;img src=&quot;/img/Flink-1.7-release-announcement-blog-1-768x591.png&quot; alt=&quot;Flink 1.7 Release&quot;&gt;&lt;/p&gt;&lt;/center&gt;

&lt;p&gt;自 feature freezed 以后经过近一个月的努力，Flink 社区在十一月的最后一天终于发布 Flink 1.7.0 版本。该版本处理了 420 个 issue，其中新特性或者改进主要集中在 SQL 和 State 两个模块上，另外从 1.7 开始更多的 API （包括 REST、State 还有正在讨论的 runtime）会考虑版本兼容性，以便用户更重度地依赖 Flink 做上层的开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
</feed>
